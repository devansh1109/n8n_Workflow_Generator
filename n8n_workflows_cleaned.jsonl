{"prompt": "Create an n8n workflow for: AI-Powered Social Media Content Generator & Publisher AI-Powered Social Media Content Generator & Publisher \ud83d\ude80 This AI-driven n8n workflow automates social media content creation and publishing across LinkedIn, Instagram, Facebook, and Twitter (X). It generates engaging, platform-optimized posts using Google Gemini AI, based on user inputs such as a post title, keywords, and an uploaded image. The workflow ensures seamless content generation and publishing, making it a perfect tool for marketers, business owners, influencers, and content creators. \ud83c\udf1f Features & Benefits \u2705 AI-Generated Social Media Posts \u2013 Uses Google Gemini AI to create high-quality, optimized content. \u2705 Multi-Platform Support \u2013 Automatically generates posts for LinkedIn, Instagram, Facebook, and Twitter (X). \u2705 Hashtag & SEO Optimization \u2013 Includes trending hashtags to enhance visibility and engagement. \u2705 Image Upload & Processing \u2013 Allows image uploads for Instagram and Facebook using imgbb and Facebook Graph API. \u2705 Automated Publishing \u2013 Posts are automatically published on all selected platforms. \u2705 Custom Call-to-Action \u2013 Each platform's post is optimized with CTAs for better engagement. \u2705 User-Friendly Form Submission \u2013 Easy-to-use form where users can enter post titles, keywords, links, and images. \u2705 Performance Tracking \u2013 Provides confirmation and tracking links for published posts. \ud83d\udccc How It Works 1\ufe0f\u20e3 User Submission Form Fill out the form with Post Title, Keywords, and an Optional Link. Upload an image for Instagram & Facebook posts. 2\ufe0f\u20e3 AI Content Generation Google Gemini AI generates optimized content for each platform. The AI ensures professional, engaging, and audience-specific content. 3\ufe0f\u20e3 Content Review Users review and approve the AI-generated content before publishing. 4\ufe0f\u20e3 Automated Publishing Posts are automatically published on LinkedIn, Facebook, Instagram, and Twitter (X). Uses Facebook Graph API, LinkedIn API, Twitter API, and Instagram API. 5\ufe0f\u20e3 Post Confirmation & Tracking Get links to track published posts on each platform. \ud83d\udee0\ufe0f Prerequisites Before using this workflow, ensure you have: \u2705 n8n Instance (Cloud or Self-Hosted) \u2705 Social Media API Credentials (Facebook, Instagram, LinkedIn, Twitter API) \u2705 Google Gemini AI API Key \u2705 imgbb API Key (for image hosting) \ud83d\udcfa YouTube Video Tutorial \ud83c\udfa5 Watch the step-by-step tutorial on how to set up and use this n8n workflow template: \ud83d\udd17 YouTube Tutorial - AI-Powered Social Media Posting in n8n \ud83c\udfaf Use Cases \ud83d\udccc Marketing Agencies \u2013 Automate client content scheduling. \ud83d\udccc Businesses & Brands \u2013 Maintain a consistent brand presence on social media. \ud83d\udccc Content Creators & Influencers \u2013 Generate high-quality posts quickly. \ud83d\udccc E-commerce & Startups \u2013 Promote products and services effortlessly. \ud83d\udccc Corporate & Enterprise Teams \u2013 Streamline internal and external communications. \ud83d\udc68\u200d\ud83d\udcbb Creator Information \ud83d\udc64 Developed by: Amjid Ali \ud83c\udf10 Website: SyncBricks \ud83d\udce7 Email: info@syncbricks.com \ud83d\udcbc LinkedIn: Amjid Ali \ud83d\udcfa YouTube: SyncBricks \ud83d\udca1 Support & Contributions If you find this workflow helpful, consider supporting my work: \ud83d\udc49 Donate via PayPal For full courses on * AI Automation*, visit: \ud83d\udcda SyncBricks LMS \ud83d\udcda AI and Auotmation Course \ud83d\udc49 Get Started with N8N Amjid Ali | 12 days ago | Free +3", "completion": "{\"id\":\"LzDLmNc63zyqQBXz\",\"meta\":{\"instanceId\":\"942ff96108ceedc60ba777eabc4e8ea2ae87415579996cd64d20cc647c4d3523\"},\"name\":\"Social Media Tutorial\",\"tags\":[],\"nodes\":[{\"id\":\"8034d082-f50e-4f6b-8a79-ce7f86012d72\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[920,20],\"webhookId\":\"syncbricks-social-posting-ai-agent\",\"parameters\":{\"options\":{\"buttonLabel\":\"Continue to Image Upload\"},\"formTitle\":\"\ud83d\udcf1\ud83e\udd16 Syncbricks - Social Media Content AI Agent\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Post Title/Topic\",\"placeholder\":\"Write a brief and clear title or main topic for the post (e.g., \\\"New Product Launch,\\\" \\\"Exciting Company Update\\\"). This will help the AI understand the core subject of your post.\",\"requiredField\":true},{\"fieldLabel\":\"Optional (Keywords or Hashtags)\",\"placeholder\":\"Add any specific keywords or hashtags you want the AI to include in the post. This will ensure the generated content aligns with your branding or campaign needs.\"},{\"fieldLabel\":\"Optional Link\",\"placeholder\":\"http:// link of any product, service, formt, support etc\"}]},\"responseMode\":\"lastNode\",\"authentication\":\"basicAuth\",\"formDescription\":\"=This is syncbricks AI-driven assistant designed to simplify and accelerate your social media content creation process. This intelligent agent generates engaging and platform-optimized post descriptions based on your inputs, such as a brief topic or title and an uploaded image. Whether you're a marketer, business owner, or influencer, the AI streamlines content creation, saving time and ensuring professional-quality posts tailored to your audience.\"},\"typeVersion\":2.2},{\"id\":\"e38a74d5-a733-4f5c-a97b-2c761b46919b\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[1180,520],\"parameters\":{\"text\":\"=You are a content creation AI for Syncbricks, a leading IT-focused platform that delivers insightful articles, tech news,"}
{"prompt": "Create an n8n workflow for: \ud83e\udde0 Empower Your AI Chatbot with Long-Term Memory and Dynamic Tool Routing Empower Your AI Chatbot with Long-Term Memory and Dynamic Tool Routing This n8n workflow equips your AI agent with long-term memory and a dynamic tools router, enabling it to provide intelligent, context-aware responses while managing tasks across multiple tools. By combining persistent memory and modular task routing, this workflow makes your AI smarter, more efficient, and highly adaptable. \ud83d\udc65 Who Is This For? AI Developers & Automation Enthusiasts: Integrate advanced AI features like long-term memory and task routing without coding expertise. Businesses & Teams: Automate tasks while maintaining personalized, context-aware interactions. Customer Support Teams: Improve user experience with chatbots that remember past interactions. Marketers & Content Creators: Streamline communication across platforms like Gmail and Telegram. AI Researchers: Experiment with persistent memory and multi-tool integration. \ud83d\ude80 What Problem Does This Solve? This workflow simplifies the creation of intelligent AI systems that retain memory, manage tasks dynamically, and automate notifications across tools like Gmail and Telegram\u2014saving time and improving efficiency. \ud83d\udee0\ufe0f What This Workflow Does Save & Retrieve Memories**: Uses Google Docs for long-term storage to recall past interactions or user preferences. Dynamic Task Routing**: Routes tasks to the right tools (e.g., saving/retrieving memories or sending notifications). AI-Powered Context Understanding**: Combines OpenAI GPT-based short-term memory with long-term memory for smarter responses. Multi-Channel Notifications**: Sends updates via Gmail or Telegram. \ud83d\udd27 Setup API Credentials: Connect to OpenAI (AI processing), Google Docs (memory storage), Gmail/Telegram (notifications). Customize Parameters: Adjust the AI agent's system message for your use case. Define task-routing rules in the tools router node. Test & Deploy: Verify memory saving/retrieval, task routing, and notification delivery. \ud83d\udca1 How to Customize Modify the system message in the OpenAI node to tailor your agent\u2019s behavior. Add or adjust routing rules for additional tools. Update notification settings to match your communication preferences. Joseph LePage | 5 days ago | Free +3", "completion": "{\"id\":\"p7xESnT1xMZD2hRk\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83e\udde0 Give Your AI Agent Chatbot Long Term Memory Tools Router\",\"tags\":[],\"nodes\":[{\"id\":\"d5947a9d-bad4-4db7-8cdb-dc277928565a\",\"name\":\"When Executed by Another Workflow\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[320,-80],\"parameters\":{\"inputSource\":\"passthrough\"},\"typeVersion\":1.1},{\"id\":\"c9e9715d-c691-4516-9e0c-cf393a18d814\",\"name\":\"Save Long Term Memories\",\"type\":\"n8n-nodes-base.googleDocs\",\"position\":[1020,-80],\"parameters\":{\"actionsUi\":{\"actionFields\":[{\"text\":\"={\\n \\\"date\\\": \\\"{{ $now }}\\\",\\n \\\"memory\\\": \\\"{{ $json.query }}\\\"\\n},\\n\",\"action\":\"insert\"}]},\"operation\":\"update\",\"documentURL\":\"1YFTbVGlnpdYF5FuA6grfyvzmcdjcPviClAqbJ5HJIlc\"},\"credentials\":{\"googleDocsOAuth2Api\":{\"id\":\"YWEHuG28zOt532MQ\",\"name\":\"Google Docs account\"}},\"typeVersion\":2},{\"id\":\"306272bf-e264-4cfb-b8b6-8c0af1de3f14\",\"name\":\"Saved response\",\"type\":\"n8n-nodes-base.set\",\"position\":[1200,-80],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"6b7e85d8-379b-4a0f-9799-9193a29f0447\",\"name\":\"response\",\"type\":\"string\",\"value\":\"Memory saved\"}]}},\"typeVersion\":3.4},{\"id\":\"152b6922-8600-4fdb-8372-3b0102a7b73a\",\"name\":\"Respond with long term"}
{"prompt": "Create an n8n workflow for: Automated Content Generation & Publishing - Wordpress Workflow Description: Automated Content Publishing for WordPress This n8n workflow automates the entire process of content generation, image selection, and scheduled publishing to a self-hosted WordPress website. It is designed for bloggers, marketers, and businesses who want to streamline their content creation and posting workflow. \ud83c\udf1f Features \u2705 AI-Powered Content Generation Uses ChatGPT to generate engaging, market-ready blog articles Dynamically incorporates high-search volume keywords \u2705 Automated Image Selection Searches for relevant stock images from Pexels Embeds images directly into posts (Optional)* Supports *Featured Image from URL (FIFU) plugin** for WordPress \u2705 Scheduled & Randomized Posting Automatically schedules posts at predefined intervals Supports randomized delay (0-6 hours) for natural publishing \u2705 WordPress API Integration Uses WordPress REST API to directly publish posts Configures featured images, categories, and metadata Supports SEO-friendly meta fields \u2705 Flexible & Customizable Works with any WordPress website (self-hosted) Can be modified for other CMS platforms \ud83d\udd27 How It Works 1\ufe0f\u20e3 Trigger & Scheduling Automatically runs at preset times or on-demand Supports cron-like scheduling 2\ufe0f\u20e3 AI Content Generation Uses a well-crafted prompt to generate high-quality blog posts Extracts relevant keywords for both SEO and image selection 3\ufe0f\u20e3 Image Fetching from Pexels Searches and retrieves high-quality images Embeds image credits and ensures proper formatting 4\ufe0f\u20e3 WordPress API Integration Sends post title, content, image, and metadata via HTTP Request Can include custom fields, categories, and tags 5\ufe0f\u20e3 Randomized Delay Before Publishing Ensures natural posting behavior Avoids bulk publishing issues \ud83d\udccc Requirements Self-hosted WordPress website* with *REST API enabled** FIFU Plugin* (optional) for *external featured images** n8n Self-Hosted or Cloud Instance** \ud83d\ude80 Who Is This For? \u2705 Bloggers who want to automate content publishing \u2705 Marketing teams looking to scale content production \u2705 Business owners who want to boost online presence \u2705 SEO professionals who need consistent, optimized content \ud83d\udca1 Ready to Automate? \ud83d\udc49 Click here to get this workflow! (Replace with Purchase URL) Sunny | 6 days ago | Free", "completion": "{\"id\":\"YOUR_WORKFLOW_ID\",\"meta\":{\"instanceId\":\"YOUR_INSTANCE_ID\",\"templateCredsSetupCompleted\":true},\"name\":\"Automated Content Generation & Publishing - Wordpress\",\"tags\":[],\"nodes\":[{\"id\":\"9cd63357-19dc-4420-baa9-1e1389c7120f\",\"name\":\"Create posts on Wordpress\",\"type\":\"n8n-nodes-base.wordpress\",\"position\":[1180,280],\"parameters\":{\"title\":\"={{ $('Save to Sheet').item.json['title'] }}\",\"additionalFields\":{\"status\":\"publish\",\"content\":\"=<img src=\\\"{{ $('Automated Image Retrieval from Pexels').item.json.photos[0].src.landscape }}\\\" alt=\\\"image text\\\" style=\\\"width:100%; height:auto;\\\"><br><br>\\n<br><br>\\n{{ $node['Save to Sheet'].json['content'] }}\"}},\"credentials\":{\"wordpressApi\":{\"id\":\"YOUR_WORDPRESS_CREDENTIAL_ID\",\"name\":\"Wordpress account 2\"}},\"typeVersion\":1,\"alwaysOutputData\":false},{\"id\":\"65f62f19-d10f-4ca1-a853-9cedb3506743\",\"name\":\"Processing Delay\",\"type\":\"n8n-nodes-base.code\",\"position\":[180,580],\"parameters\":{\"jsCode\":\"const delay = Math.floor(Math.random() * (6 * 60 * 60 * 1000)); // random delay 0-6 hour\\nreturn {\\n json: {\\n delay: delay,\\n delay_minutes: Math.round(delay / 60000), // to minutes\\n delay_hours: (delay / 3600000).toFixed(2) // to hours\\n }\\n};\\n\"},\"typeVersion\":2},{\"id\":\"193d2876-c50e-4b9e-8856-9fd11baa025e\",\"name\":\"Random Wait\",\"type\":\"n8n-nodes-base.wait\",\"position\":[180,760],\"webhookId\":\"61377399-ce9f-497a-80b1-aab29fc9fb69\",\"parameters\":{\"amount\":\"={{$json[\\\"delay\\\"] / 1000}}\"},\"typeVersion"}
{"prompt": "Create an n8n workflow for: All-in-One Telegram/Baserow AI Assistant \ud83e\udd16\ud83e\udde0 Voice/Photo/Save Notes/Long Term Mem Telegram Personal Assistant with Long-Term Memory & Note-Taking This n8n workflow transforms your Telegram bot into a powerful personal assistant that handles voice, photo, and text messages. The assistant uses AI to interpret messages, save important details as long-term memories or notes in a Baserow database, and recall information for future interactions. \ud83c\udf1f How It Works Message Reception & Routing Telegram Integration: The workflow is triggered by incoming messages on your Telegram bot. Dynamic Routing: A switch node inspects the message to determine whether it's voice, text, or photo (with captions) and routes it for the appropriate processing. Content Processing Voice Messages: Audio files are retrieved and sent to an AI transcription node to convert spoken words into text. Text Messages: Text is directly captured and prepared for analysis. Photos: If an image is received, the bot fetches the file (and caption, if provided) and uses an AI-powered image analysis node to extract relevant details. AI-Powered Agent & Memory Management The core AI agent (powered by GPT-4o-mini) processes the incoming message along with any previous conversation history stored in PostgreSQL memory buffers. Long-Term Memory: When a message contains personal or noteworthy information, the assistant uses a dedicated tool to save this data as a long-term memory in Baserow. Note-Taking: For specific instructions or reminders, the assistant saves concise notes in a separate Baserow table. The AI agent follows defined rules to decide which details are saved as memories and which are saved as notes. Response Generation After processing the message and updating memory/notes as needed, the AI agent crafts a contextual and personalized response. The response is sent back to the user via Telegram, ensuring smooth and natural conversation flow. \ud83d\ude80 Key Features Multimodal Input:** Seamlessly handles voice, photo (with captions), and text messages. Long-Term Memory & Note-Taking:** Uses a Baserow database to store personal details and notes, enhancing conversational context over time. AI-Driven Contextual Responses:** Leverages an AI agent to generate personalized, context-aware replies based on current input and past interactions. User Security & Validation:** Incorporates validation steps to verify the user's Telegram ID before processing, ensuring secure and personalized interactions. Easy Baserow Setup:** Comes with a clear setup guide and sample configurations to quickly integrate Baserow for managing memories and notes. \ud83d\udd27 Setup Guide Telegram Bot Setup: Create your bot via BotFather and obtain the Bot Token. Configure the Telegram webhook in n8n with your bot's token and URL. Baserow Database Configuration: Memory Table: Create a workspace titled \"Memories and Notes\". Set up a table (e.g., \"Memory Table\") with at least two fields: Memory (long text) Date Added (US date format with time) Notes Table: Duplicate the Memory Table and rename it to \"Notes Table\". Change the first field's name from \"Memory\" to \"Notes\". n8n Workflow Import & Configuration: Import the workflow JSON into your n8n instance. Update credentials for Telegram, Baserow, OpenAI, and PostgreSQL (for memory buffering) as needed. Adjust node settings if you need to customize AI agent prompts or memory management rules. Testing & Deployment: Test your bot by sending various message types (text, voice, photo) to confirm that the workflow processes them correctly, updates Baserow, and returns the appropriate response. Monitor logs to ensure that memory and note entries are correctly stored and retrieved. \u2728 Example Interactions Voice Message Processing:** User sends a voice note requesting a reminder. Bot Response: \"Thanks for your message! I've noted your reminder and saved it for future reference.\" Photo with Caption:** User sends a photo with the caption \"Save this recipe for dinner ideas.\" Bot Response: \"Got it! I've saved this recipe along with the caption for you.\" Text Message for Memory Saving:** User: \"I love hiking on weekends.\" Bot Response: \"Noted! I\u2019ll remember your interest in hiking.\" Retrieving Information:** User asks: \"What notes do I have?\" Bot Response: \"Here are your latest notes: [list of saved notes].\" \ud83d\udee0\ufe0f Resources & Next Steps Telegram Bot Configuration:** Telegram BotFather Guide n8n Documentation:** n8n Docs Community Forums:** Join discussions and share your customizations! This workflow not only streamlines message processing but also empowers users with a personal AI assistant that remembers details over time. Customize the rules and responses further to fit your unique requirements and enjoy a more engaging, intelligent conversation experience on Telegram! Rod | 9 days ago | Free +2", "completion": "{\"id\":\"Dctc6QKyRXK17oEq\",\"meta\":{\"instanceId\":\"ca87034fd2d5cf9c52373260a6e37ca15a4a059ecc2971ac41c079ce4aa21138\",\"templateCredsSetupCompleted\":true},\"name\":\"All-in-One Telegram/Baserow AI Assistant \ud83e\udd16\ud83e\udde0 Voice/Photo/Save Notes/Long Term Mem\",\"tags\":[],\"nodes\":[{\"id\":\"bbcaac28-f07c-421b-91c6-e3b2d024a71f\",\"name\":\"Sticky Note7\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-900,80],\"parameters\":{\"color\":4,\"width\":340,\"height\":380,\"content\":\"## Retrieve Long Term Memories\\nBaserow\"},\"typeVersion\":1},{\"id\":\"399c0a44-72b5-4e71-b578-3e4eb8087b3c\",\"name\":\"Sticky Note8\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-180,420],\"parameters\":{\"width\":280,\"height\":380,\"content\":\"## Save To Current Chat Memory\"},\"typeVersion\":1},{\"id\":\"d0438c1d-42e4-47d0-b23b-78f9cee5a28c\",\"name\":\"Sticky Note9\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[140,420],\"parameters\":{\"color\":4,\"width\":280,\"height\":380,\"content\":\"## Save Long Term Memories\\nBaserow\"},\"typeVersion\":1},{\"id\":\"d8cbf733-2a69-45d5-9767-43e8c67a43ee\",\"name\":\"gpt-4o-mini\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-400,580],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"lpt7A1AOIMshaaE7\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"3cd6b923-b369-45e6-a3c1-5"}
{"prompt": "Create an n8n workflow for: \ud83e\udd16 AI Powered RAG Chatbot for Your Docs + Google Drive + Gemini + Qdrant \ud83e\udd16 AI-Powered RAG Chatbot with Google Drive Integration This workflow creates a powerful RAG (Retrieval-Augmented Generation) chatbot that can process, store, and interact with documents from Google Drive using Qdrant vector storage and Google's Gemini AI. How It Works Document Processing & Storage \ud83d\udcda Retrieves documents from a specified Google Drive folder Processes and splits documents into manageable chunks Extracts metadata using AI for enhanced search capabilities Stores document vectors in Qdrant for efficient retrieval Intelligent Chat Interface \ud83d\udcac Provides a conversational interface powered by Google Gemini Uses RAG to retrieve relevant context from stored documents Maintains chat history in Google Docs for reference Delivers accurate, context-aware responses Vector Store Management \ud83d\uddc4\ufe0f Features secure delete operations with human verification Includes Telegram notifications for important operations Maintains data integrity with proper version control Supports batch processing of documents Setup Steps Configure API Credentials: Set up Google Drive & Docs access Configure Gemini AI API Set up Qdrant vector store connection Add Telegram bot for notifications Add OpenAI Api Key to the 'Delete Qdrant Points by File ID' node Configure Document Sources: Set Google Drive folder ID Define Qdrant collection name Set up document processing parameters Test and Deploy: Verify document processing Test chat functionality Confirm vector store operations Check notification system This workflow is ideal for organizations needing to create intelligent chatbots that can access and understand large document repositories while maintaining context and providing accurate responses through RAG technology. Joseph LePage | 9 days ago | Free +8", "completion": "{\"id\":\"8tusZTTtcyaiznEG\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83e\udd16 AI Powered RAG Chatbot for Your Docs + Google Drive + Gemini + Qdrant\",\"tags\":[],\"nodes\":[{\"id\":\"7ad5796b-d1a0-4cc1-bed6-105ff499beeb\",\"name\":\"Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[620,720],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"file_id\",\"value\":\"={{ $json.id }}\"},{\"name\":\"pubkey\",\"value\":\"={{ $json.name }}\"},{\"name\":\"=overarching_theme\",\"value\":\"={{ $('Extract Meta Data').item.json.output.overarching_theme }}\"},{\"name\":\"recurring_topics\",\"value\":\"={{ $('Extract Meta Data').item.json.output.recurring_topics }}\"},{\"name\":\"pain_points\",\"value\":\"={{ $('Extract Meta Data').item.json.output.pain_points }}\"},{\"name\":\"analytical_insights\",\"value\":\"={{ $('Extract Meta Data').item.json.output.analytical_insights }}\"},{\"name\":\"conclusion\",\"value\":\"={{ $('Extract Meta Data').item.json.output.conclusion }}\"},{\"name\":\"keywords\",\"value\":\"={{ $('Extract Meta Data').item.json.output.keywords }}\"}]}},\"dataType\":\"binary\",\"binaryMode\":\"specificField\"},\"typeVersion\":1},{\"id\":\"84986ee2-4d79-49e8-8778-b8a955cf2174\",\"name\":\"Token Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter\",\"position\":[720,860],\"parameters\":{\"chunkSize\":3000},\"typeVersion\":1},{\"id\":\"82aa7016-f3af-4de8-a410-b1c802041213\",\"name\":\"Qdrant Vector Store\",\"type\":\"@n8n/n"}
{"prompt": "Create an n8n workflow for: Invoices from Gmail to Drive and Google Sheets Attachments Gmail to Drive and Google Sheets Description Automatically process invoice emails by saving attachments to Google Drive and extracting key invoice data to Google Sheets using AI. This workflow monitors your Gmail for unread emails with attachments, saves PDFs to a specified Google Drive folder, and uses OpenAI's GPT-4o to extract invoice details (date, description, amount) into a structured spreadsheet. Use cases Invoice Management**: Automatically organize and track invoices received via email Financial Record Keeping**: Maintain a structured database of all invoice information Document Organization**: Keep digital copies of invoices organized in Google Drive Automated Data Entry**: Eliminate manual data entry for invoice processing Resources Gmail account Google Drive account Google Sheets account OpenAI API key Setup instructions Prerequisites Active Gmail, Google Drive, and Google Sheets accounts OpenAI API key (GPT-4o model access) n8n instance with credentials manager Steps Gmail and Google Drive Setup: Connect your Gmail account in n8n credentials Connect your Google Drive account with appropriate permissions Create a destination folder in Google Drive for invoice storage Google Sheets Setup: Connect your Google Sheets account Create a spreadsheet with columns: Invoice date, Invoice Description, Total price, and Fichero Copy your spreadsheet ID for configuration OpenAI Setup: Add your OpenAI API key to n8n credentials Configure Email Filter: Update the email filter node to match your specific sender requirements Benefits Time Saving**: Eliminates manual downloading, filing, and data entry Accuracy**: AI-powered data extraction reduces human error Organization**: Consistent file naming and storage structure Searchability**: Creates a searchable database of all invoice information Automation**: Runs every minute to process new emails as they arrive Related templates Email Parser to CRM Document Processing Workflow Financial Data Automation Juan Carlos Cavero Gracia | 6 days ago | Free +2", "completion": "{\"id\":\"XnGZZfT5u0Cw1X3p\",\"meta\":{\"instanceId\":\"3378b0d68c3b7ebfc71b79896d94e1a044dec38e99a1160aed4e9c323910fbe2\",\"templateCredsSetupCompleted\":true},\"name\":\"Attachments Gmail to drive and google sheets\",\"tags\":[],\"nodes\":[{\"id\":\"0404ef0a-9750-495a-8798-98d4b059a083\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-580,-420],\"parameters\":{\"height\":440,\"content\":\"## Setup\\n1. Setup your **Gmail** and **Google Drive** credentials\\n2. Setup your **Google Sheets** credentials\\n3. Setup your **Openai** api key\"},\"typeVersion\":1},{\"id\":\"8751a7f1-aae4-4746-aae7-3d8563845b8c\",\"name\":\"Gmail Trigger1\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-640,120],\"parameters\":{\"simple\":false,\"filters\":{\"readStatus\":\"unread\"},\"options\":{\"downloadAttachments\":true},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"v8YJP3VfeGtRk5la\",\"name\":\"Gmail account\"}},\"typeVersion\":1.1},{\"id\":\"40f62192-5acb-4915-aa07-e5a0dfeb7581\",\"name\":\"Setup1\",\"type\":\"n8n-nodes-base.set\",\"position\":[-300,120],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"4cca07a2-6a70-4011-a025-65246e652fb9\",\"name\":\"url_to_drive_folder\",\"type\":\"string\",\"value\":\"1fCWCdqrFP3WrjjLc-gJtxMaiaF5lh8Ko\"}]},\"includeOtherFields\":true},\"typeVersion\":3.4},{\"id\":\"d928e797-8851-4ab4-9199-cd555a40eae9\",\"name\":\"Upload"}
{"prompt": "Create an n8n workflow for: \u2728\ud83d\udd2a Advanced AI Powered Document Parsing & Text Extraction with Llama Parse Description This workflow automates document processing using LlamaParse to extract and analyze text from various file formats. It intelligently processes documents, extracts structured data, and delivers actionable insights through multiple channels. How It Works Document Ingestion & Processing \ud83d\udcc4 Monitors Gmail for incoming attachments or accepts documents via webhook Validates file formats against supported LlamaParse extensions Uploads documents to LlamaParse for advanced text extraction Stores original documents in Google Drive for reference Intelligent Document Analysis \ud83e\udde0 Automatically classifies document types (invoices, reports, etc.) Extracts structured data using customized AI prompts Generates comprehensive document summaries with key insights Converts unstructured text into organized JSON data Invoice Processing Automation \ud83d\udcbc Extracts critical invoice details (dates, amounts, line items) Organizes financial data into structured formats Calculates tax breakdowns, subtotals, and payment information Maintains detailed records for accounting purposes Multi-Channel Delivery \ud83d\udcf1 Saves extracted data to Google Sheets for tracking and analysis Sends concise summaries via Telegram for immediate review Creates searchable document archives in Google Drive Updates spreadsheets with structured financial information Setup Steps Configure API Credentials \ud83d\udd11 Set up LlamaParse API connection Configure Gmail OAuth for email monitoring Set up Google Drive and Sheets integrations Add Telegram bot credentials for notifications Customize AI Processing \u2699\ufe0f Adjust document classification parameters Modify extraction templates for specific document types Fine-tune summary generation prompts Customize invoice data extraction schema Test and Deploy \ud83d\ude80 Test with sample documents of various formats Verify data extraction accuracy Confirm notification delivery Monitor processing pipeline performance Joseph LePage | 7 days ago | Free +3", "completion": "{\"id\":\"kjyWJWfDlyXkKL3m\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\u2728\ud83d\udd2a Advanced AI Powered Document Parsing & Text Extraction with Llama Parse\",\"tags\":[],\"nodes\":[{\"id\":\"ea7670da-896e-4b9c-b0c2-b3a3dbb6f88f\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[-2320,80],\"webhookId\":\"a9668054-5bd3-427d-8f18-932436441e42\",\"parameters\":{\"path\":\"parse\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":2},{\"id\":\"2c445d40-5d8b-469e-811e-7423f57ba054\",\"name\":\"Gmail\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[-2040,-1260],\"webhookId\":\"344de9dc-4062-4552-ae29-1e9150322cdb\",\"parameters\":{\"limit\":28,\"filters\":{\"q\":\"has:attachment\",\"sender\":\" joe@example.com\"},\"operation\":\"getAll\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"1xpVDEQ1yx8gV022\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"f321e1d3-24ba-4623-bb31-93c7f6389aa9\",\"name\":\"Gmail Trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-2360,-1260],\"parameters\":{\"filters\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"},{}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"1xpVDEQ1yx8gV022\",\"name\":\"Gmail account\"}},\"typeVersion\":1.2},{\"id\":\"ad2701f8-be77-465e-bd58-0e964ba412c0\",\"name\":\"Limit\",\"type\":\"n8n-nodes-base"}
{"prompt": "Create an n8n workflow for: \u26a1\ud83d\udcfd\ufe0f Ultimate AI-Powered Chatbot for YouTube Summarization & Analysis \ud83c\udfa5 YouTube Video AI Agent Workflow This n8n workflow template allows you to interact with an AI agent that extracts details and the transcript of a YouTube video using a provided video ID. Once the details and transcript are retrieved, you can chat with the AI agent to explore or analyze the video's content in a conversational and insightful manner. \ud83c\udf1f How the Workflow Works \ud83d\udd17 Input Video ID: The user provides a YouTube video ID as input to the workflow. \ud83d\udcc4 Data Retrieval: The workflow fetches essential details about the video (e.g., title, description, upload date) and retrieves its transcript using YouTube's Data API and additional tools for transcript extraction. \ud83e\udd16 AI Agent Interaction: The extracted details and transcript are processed by an AI-powered agent. Users can then ask questions or engage in a conversation with the agent about the video's content, such as: Summarizing the transcript. Analyzing key points. Clarifying specific sections. \ud83d\udcac Dynamic Responses: The AI agent uses natural language processing (NLP) to generate contextual and accurate responses based on the video data, ensuring a smooth and intuitive interaction. \ud83d\ude80 Use Cases \ud83d\udcca Content Analysis**: Quickly analyze long YouTube videos by querying specific sections or extracting summaries. \ud83d\udcda Research and Learning**: Gain insights from educational videos or tutorials without watching them entirely. \u270d\ufe0f Content Creation**: Repurpose transcripts into blogs, social media posts, or other formats efficiently. \u267f Accessibility**: Provide an alternative, text-based way to interact with video content for users who prefer reading over watching. \ud83d\udee0\ufe0f Resources for Getting Started Google Cloud Console** (for API setup): Visit Google Cloud's Get Started Guide to configure your API access. YouTube Data API Key Setup**: Follow this guide to create and manage your YouTube Data API key. Install n8n Locally**: Refer to this installation guide for setting up n8n on your local machine. \u2728 Sample Prompts \"Tell me about this YouTube video with id: JWfNLF_g_V0\" \"Can you provide a list of key takeaways from this video with id: [youtube-video-id]?\" Joseph LePage | 11 days ago | Free", "completion": "{\"id\":\"GM9Qxzul4NPQpJkn\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\u26a1\ud83d\udcfd\ufe0f Ultimate AI-Powered Chatbot for YouTube Summarization & Analysis\",\"tags\":[],\"nodes\":[{\"id\":\"10cfb27f-ef93-41cd-972e-37dfdcab97ad\",\"name\":\"Get YouTube Transcript\",\"type\":\"n8n-nodes-base.code\",\"position\":[20,360],\"parameters\":{\"jsCode\":\"// Get all input items\\nconst items = $input.all();\\nconst results = [];\\n\\n// Import the YoutubeTranscript module from the youtube-transcript package\\n// npm i -g youtube-transcript\\nconst { YoutubeTranscript } = require('youtube-transcript');\\n\\nfor (let i = 0; i < items.length; i++) {\\n // Extract the VIDEO_ID from the input JSON\\n const VIDEO_ID = items[i].json.VIDEO_ID;\\n \\n if (!VIDEO_ID) {\\n throw new Error('The video ID parameter is empty.');\\n }\\n \\n try {\\n // Fetch the transcript for the provided video ID\\n const transcript = await YoutubeTranscript.fetchTranscript(VIDEO_ID);\\n \\n // Append the fetched transcript data to the results\\n results.push({\\n json: {\\n youtubeId: VIDEO_ID,\\n transcript,\\n },\\n });\\n } catch (error) {\\n // In case of an error, add an error message to the output for this item\\n results.push({\\n json: {\\n youtubeId: VIDEO_ID,\\n error: error.message,\\n },\\n });\\n }\\n}\\n\\n// Return the results to be used by the next node in the workflow\\nreturn results;\\n\"},\"typeVersion\":2},{\"id\":\"a7b7740e-7470-4ce0-a698-6043559eb781\",\"name\":\"When Executed by Another Workflow\",\"type\":\"n8n-n"}
{"prompt": "Create an n8n workflow for: \ud83e\udd16\ud83e\udde0 AI Agent Chatbot + LONG TERM Memory + Note Storage + Telegram This workflow template creates an AI agent chatbot with long-term memory and note storage using Google Docs and Telegram integration. Google Docs Integration \ud83d\udcc4 n8n Google Docs Node Setup Google Credentials Telegram Integration \ud83d\udcac Telegram Setup Core Features \ud83c\udf1f AI Agent Integration \ud83e\udd16 Implements a sophisticated AI agent with memory management capabilities Uses GPT-4o-mini and DeepSeek models for intelligent conversation handling Maintains context awareness through session management Memory System \ud83e\udde0 Long-term memory storage using Google Docs Separate note storage system for specific information Window buffer memory for maintaining conversation context Intelligent memory retrieval and storage mechanisms Communication Interface \ud83d\udcac Telegram integration for message handling Real-time message processing and response generation Technical Components \ud83d\udd27 Memory Architecture \ud83d\udcda Dual storage system separating memories from notes Automated memory retrieval before each interaction Structured memory saving with timestamps AI Models \ud83e\udd16 Primary GPT-4o-mini mini model for general interactions DeepSeek-V3 Chat for specialized processing Custom agent system with tool integration Storage Integration \ud83d\udcbe Google Docs integration for persistent storage Separate document management for memories and notes Automated document updates and retrievals Joseph LePage | 22 days ago | Free", "completion": "{\"id\":\"QJZLBn9L6NbmjmLK\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83e\udd16\ud83e\udde0 AI Agent Chatbot + LONG TERM Memory + Note Storage + Telegram\",\"tags\":[],\"nodes\":[{\"id\":\"20a2d959-5412-447b-a2c4-7736b6b758b3\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-320,1600],\"webhookId\":\"8ba8fa53-2c24-47a8-b4dd-67b88c106e3d\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"de79c268-bac5-48ff-be4d-18f522861c22\",\"name\":\"Sticky Note7\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-100,1280],\"parameters\":{\"color\":4,\"width\":340,\"height\":380,\"content\":\"## Retrieve Long Term Memories\\nGoogle Docs\"},\"typeVersion\":1},{\"id\":\"000a94d1-57ce-4eec-a021-9123685d22bf\",\"name\":\"Sticky Note8\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1040,1840],\"parameters\":{\"width\":280,\"height\":380,\"content\":\"## Save To Current Chat Memory (Optional)\"},\"typeVersion\":1},{\"id\":\"1bf1cade-bb3e-450a-a531-9add259069df\",\"name\":\"Sticky Note9\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1360,1840],\"parameters\":{\"color\":4,\"width\":280,\"height\":380,\"content\":\"## Save Long Term Memories\\nGoogle Docs\"},\"typeVersion\":1},{\"id\":\"8b30f207-8204-4548-8f51-38c387d98ae9\",\"name\":\"gpt-4o-mini\",\"type\":\"@n8n/n8n-nodes-langchain"}
{"prompt": "Create an n8n workflow for: Effortless Task Management: Create Todoist Tasks Directly from Telegram with AI Effortless Task Management: Create Todoist Tasks Directly from Telegram with AI This n8n workflow empowers you to seamlessly manage your tasks by creating Todoist entries directly from Telegram, using the power of AI. Simply send a voice or text message to your Telegram bot, and this workflow will transform it into actionable tasks in your Todoist account. Who is this for? Busy professionals** who need a quick and easy way to capture tasks on the go. Students** looking to streamline their assignments and project management. Anyone** who wants to leverage AI for effortless task management. What Problem Does it Solve? This workflow eliminates the need to manually enter tasks into Todoist. It automates the process of capturing, organizing, and prioritizing tasks, saving you time and effort. What are the Benefits? Seamless Integration:** Connect your Telegram and Todoist accounts for a frictionless workflow. AI-Powered Task Breakdown:** LLM AI intelligently analyzes your messages and breaks them down into manageable sub-tasks. Voice-to-Task:** Create tasks with voice messages for hands-free convenience. Increased Productivity:** Capture and organize tasks quickly, keeping you focused and productive. Accessibility:** Access your tasks from anywhere with Todoist's mobile app and Google extension. How it Works Send a message: Send a voice or text message describing your task to your Telegram bot. AI analysis: The workflow uses an LLM (OpenAI Chat Model) to analyze your message and break it down into sub-tasks. Task creation: The workflow creates tasks in your Todoist account based on the AI's analysis. Notification: You receive a Telegram notification with a link to your newly created tasks in Todoist. Nodes in the Workflow Telegram Trigger:** Listens for incoming messages on Telegram. Switch:** Routes messages based on their type (voice or text). Telegram:** Fetches voice messages from Telegram. OpenAI:** Transcribes voice messages to text using OpenAI's Whisper API. Edit Fields:** Prepares the text for the LLM. Basic LLM Chain:** Analyzes messages and generates sub-tasks using OpenAI's GPT model. Structured Output Parser:** Extracts sub-tasks from the LLM's response. Todoist:** Creates tasks in your Todoist account. Telegram:** Sends a notification with a link to your Todoist tasks. Requirements Active n8n instance. Telegram account with a bot. Todoist account. OpenAI API key. Setup Information Import the workflow JSON into your n8n instance. Configure the Telegram Trigger node with your bot token. Set up the OpenAI credentials with your API key. Connect your Todoist account in the Todoist node. Customize the LLM prompt (optional) to fine-tune task creation. Additional Tips Explore Todoist's features to further organize and manage your tasks. Experiment with different LLM prompts to optimize task breakdown. Use n8n's features to automate other aspects of your workflow. This workflow combines the convenience of Telegram with the power of AI and Todoist to provide a seamless task management experience. Start managing your tasks effortlessly today! Onur | 3 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"b41148c809c7896d124743d940fc0964703e540af66564ef95e25a4ceea61c77\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"e87d3723-7e7a-4ff3-bffb-b2bd2096bd34\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1080,260],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"uFPD9I4pWJ4xUVf7\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"d25bf3ea-0de4-4317-9205-651f8a1a6ba8\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1060,40],\"parameters\":{\"text\":\"={{ $json.text }}\",\"messages\":{\"messageValues\":[{\"message\":\"=Okay, I've further refined the system prompt to include only the \\\"content\\\" and \\\"priority\\\" fields in the JSON output for the Todoist tool. Here's the updated prompt:\\n\\n**System Prompt:**\\n\\n```\\nYou are an AI agent acting as a project management assistant. The user will provide you with a task or project description. Your job is to break down this task or project into smaller, manageable sub-tasks. You will then format each sub-task into a JSON object suitable for input to the \\\"Todoist\\\" tool and provide these JSON objects in a list.\\n\\n**Requirements:**\\n\\n1. **Sub-Task Decomposition:** Break down the task or project provided by the user into logical and actionable sub-tasks. Each sub-task should be self-contained, completable, and measurable.\\n2. **JSON Format for Todoist:** Format each sub-task as a JSON object with the following structure:\\n\\n ```json\\n {\\n \\\"content\\\": \\\"[Task Description]\\\",\\n"}
{"prompt": "Create an n8n workflow for: Copy Viral Reels with Gemini AI Video Guide I prepared a detailed guide that shows the whole process of building an AI tool to analyze Instagram Reels using n8n. Youtube Link Who is this for? This workflow is ideal for social media analysts, digital marketers, and content creators who want to leverage data-driven insights from their Instagram Reels. It's particularly useful for those looking to automate the analysis of video performance to inform strategy and content creation. What problem does this workflow solve? Analyzing video performance on Instagram can be tedious and time-consuming, requiring multiple steps and data extraction. This workflow automates the process of fetching, analyzing, and recording insights from Instagram Reels, making it simpler for users to track engagement metrics without manual intervention. What this workflow does This workflow integrates several services to analyze Instagram Reels, allowing users to: Automatically fetch recent Reels from specified creators. Analyze the most-watched videos for insights. Store and manage data in Airtable for easy access and reporting. Initial Trigger: The process begins with a manual trigger that can later be modified for scheduled automation. Data Retrieval: It connects to Airtable to fetch a list of creators and their respective Instagram Reels. Video Analysis: It handles the fetching, downloading, and uploading of videos for analysis using an external service, simplifying performance tracking through a structured query process. Record Management: It saves relevant metrics and insights into Airtable, ensuring that users can access and organize their video analytics effectively. Setup Create accounts: Set up Airtable, Edify, n8n, and Gemini accounts. Prepare triggers and modules: Replace credentials in each node accordingly. Configure data flow: Ensure modules are set to fetch and analyze the correct data fields as outlined in the guide. Test the workflow: Run the scenario manually to confirm that data is fetched and analyzed correctly. Mark Shcherbakov | 8 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6a2a7715680b8313f7cb4676321c5baa46680adfb913072f089f2766f42e43bd\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"7594b4b8-506e-4f87-b921-ad376c9b85f2\",\"name\":\"Wait\",\"type\":\"n8n-nodes-base.wait\",\"position\":[1260,0],\"webhookId\":\"2126ffb5-dcff-4328-85f7-aff6470bffd1\",\"parameters\":{\"amount\":60},\"typeVersion\":1.1},{\"id\":\"0a9e1ed8-2fbe-421a-89a1-663709cbcaaf\",\"name\":\"Set Prompt\",\"type\":\"n8n-nodes-base.set\",\"position\":[1420,0],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"a21fe2d1-533b-4f09-94aa-9312c3dd05b8\",\"name\":\"prompt\",\"type\":\"string\",\"value\":\"=Analyze reels: 1. How it starts 2. How it ends 3. What is going on video. For us important to understand main params of the video to be able reproduct it. Params: 1. Background 2. Pose 3. Text 4. Clothes 5. Context 6. People/participants/roles Example of result: Example 1: - Girl in the frame in evening dress - Party - Text: \u201cLet's already admit that the most important ponte of a man is a gorgeous wife by his side\u201d - Formatting: White text in the center on the background of a dynamic frame. Example 2: - Behind-the-scenes moments from the shooting of the video - A girl with a beautiful figure, large breasts in a swimsuit and bathing skirt - Text below \u201cWhen at 35 you explain to your man that you need a hand in the frame for the strongest part\u201d.\"}]}},\"typeVersion\":3.4},{\"id\":\"78f145dc-cfd4-41b6-86d0-f1036adb6ef5\",\"name\":\"Gemini - Generate Upload URL\",\"type\":\"n8n-nodes-base.httpRequest\",\""}
{"prompt": "Create an n8n workflow for: \u270d\ufe0f\ud83c\udf04 Your First Wordpress + AI Content Creator - Quick Start \u270d\ufe0f\ud83c\udf04 WordPress + AI Content Creator This workflow automates the creation and publishing of multi-reading-level content for WordPress blogs. It leverages AI to generate optimized articles, automatically creates featured images, and provides versions of the content at different reading levels (Grade 2, 5, and 9). How It Works Content Generation & Processing \ud83c\udfaf Starts with a manual trigger and a user-defined blog topic Uses AI to create a structured blog post with proper HTML formatting Separates and validates the title and content components Saves a draft version to Google Drive for backup Multi-Reading Level Versions \ud83d\udcda Automatically rewrites the content for different reading levels: Grade 9: Sophisticated language with appropriate metaphors Grade 5: Simplified with light humor and age-appropriate examples Grade 2: Basic language with simple metaphors and child-friendly explanations WordPress Integration \ud83c\udf10 Creates a draft post in WordPress with the Grade 9 version Generates a relevant featured image using Pollinations.ai Automatically uploads and sets the featured image Sends success/error notifications via Telegram Setup Steps Configure API Credentials \ud83d\udd11 Set up WordPress API connection Configure OpenAI API access Set up Google Drive integration Add Telegram bot credentials for notifications Customize Content Parameters \u2699\ufe0f Adjust reading level prompts as needed Modify image generation settings Set WordPress post parameters Test and Deploy \ud83d\ude80 Run a test with a sample topic Verify all reading level versions Check WordPress draft creation Confirm notification system This workflow is perfect for content creators who need to maintain a consistent blog presence while catering to different audience reading levels. It's especially useful for educational content, news sites, or any platform that needs to communicate complex topics to diverse audiences. Joseph LePage | 9 days ago | Free +3", "completion": "{\"id\":\"ALg2eFzN4AsHIf3R\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\u270d\ufe0f\ud83c\udf04 Your First Wordpress Content Creator - Quick Start\",\"tags\":[],\"nodes\":[{\"id\":\"19673371-10cb-419f-b86b-63155aeb4a55\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[180,-240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"9df33245-102c-45e3-a99b-86be656d12e4\",\"name\":\"gpt-4o-mini\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[820,-40],\"parameters\":{\"options\":{\"responseFormat\":\"json_object\"}},\"credentials\":{\"openAiApi\":{\"id\":\"jEMSvKmtYfzAkhe6\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"5c0cacb0-82de-42b0-903d-0b3718085ad8\",\"name\":\"Structured Output - JSON\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1040,-40],\"parameters\":{\"jsonSchemaExample\":\"{\\n \\\"title\\\": \\\"title\\\",\\n \\\"content\\\": \\\"content\\\"\\n}\"},\"typeVersion\":1.2},{\"id\":\"1ec2e58e-c775-47ab-9544-7c21521741a1\",\"name\":\"Separate Title & Content\",\"type\":\"n8n-nodes-base.code\",\"position\":[1300,-340],\"parameters\":{\"jsCode\":\"try {\\n // Check if input exists and has the expected structure\\n const input = $input.all();\\n if (!input || !input.length) {\\n throw new Error('No input data received');\\n }\\n\\n const firstItem = input[0];\\n if (!firstItem || !firstItem.json || !"}
{"prompt": "Create an n8n workflow for: \ud83c\udf10 Confluence Page AI Chatbot Workflow \ud83c\udf10 Confluence Page AI Chatbot Workflow This n8n workflow template enables users to interact with an AI-powered chatbot designed to retrieve, process, and analyze content from Confluence pages. By leveraging Confluence's REST API and an AI agent, the workflow facilitates seamless communication and contextual insights based on Confluence page data. \ud83c\udf1f How the Workflow Works \ud83d\udd17 Input Chat Message The workflow begins when a user sends a chat message containing a query or request for information about a specific Confluence page. \ud83d\udcc4 Data Retrieval The workflow uses the Confluence REST API to fetch page details by ID, including its body in the desired format (e.g., storage, view). The retrieved HTML content is converted into Markdown for easier processing. \ud83e\udd16 AI Agent Interaction An AI-powered agent processes the Markdown content and provides dynamic responses to user queries. The agent is context-aware, ensuring accurate and relevant answers based on the Confluence page's content. \ud83d\udcac Dynamic Responses Users can interact with the chatbot to: Summarize the page's content. Extract specific details or sections. Clarify complex information. Analyze key points or insights. \ud83d\ude80 Use Cases \ud83d\udcda Knowledge Management**: Quickly access and analyze information stored in Confluence without manually searching through pages. \ud83d\udcca Team Collaboration**: Facilitate discussions by summarizing or explaining page content during team chats. \ud83d\udd0d Research and Documentation**: Extract critical insights from large documentation repositories for efficient decision-making. \u267f Accessibility**: Provide an alternative way to interact with Confluence content for users who prefer conversational interfaces. \ud83d\udee0\ufe0f Resources for Getting Started Confluence API Setup: Generate an API token for authentication via Atlassian's account management portal. Refer to Confluence's REST API documentation for endpoint details and usage instructions. n8n Installation: Install n8n locally or on a server using the official installation guide. AI Agent Configuration: Set up OpenAI or other supported language models for natural language processing. Joseph LePage | 6 days ago | Free", "completion": "{\"id\":\"mOcaSIUAvpt3QjQ1\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83c\udf10 Confluence Page AI Powered Chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"f4761e1a-6430-4b3d-97f9-b91743e02ea6\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[80,-340],\"parameters\":{\"color\":7,\"width\":633,\"height\":974,\"content\":\"## Confluence\\nhttps://developer.atlassian.com/cloud/confluence/basic-auth-for-rest-apis/\\nhttps://id.atlassian.com/manage-profile/security/api-tokens\\nhttps://developer.atlassian.com/cloud/confluence/rest/v2/intro/#about\\n\\nSupplying basic auth headers\\nYou can construct and send basic auth headers yourself, including a base64-encoded string that contains your Atlassian account email and API token.\\n\\nTo use basic auth headers, perform the following steps:\\n\\nGenerate an API Token for your Atlassian Account: https://id.atlassian.com/manage/api-tokens\\nBuild a string of the form your_email@domain.com:your_user_api_token.\\nYou'll need to encode your authorization credentials to Base64 encoded. You can do this locally:\\nLinux/Unix/MacOS:\\n\\nCopy\\n```\\necho -n your_email@domain.com:your_user_api_token | base64\\n```\\nWindows 7 and later, using Microsoft Powershell:\\n\\nCopy\\n```\\n$Text = \u2018your_email@domain.com:your_user_api_token\u2019\\n$Bytes = [System.Text.Encoding]::UTF8.GetBytes($Text)\\n$EncodedText = [Convert]::ToBase64String($Bytes)\\n$EncodedText\\n```\\n"}
{"prompt": "Create an n8n workflow for: \ud83d\udca1\ud83c\udf10 Essential Multipage Website Scraper with Jina.ai \ud83d\udca1\ud83c\udf10 Essential Multipage Website Scraper with Jina.ai Use responsibly and follow local rules and regulations This N8N workflow enables automated multi-page website scraping using Jina.ai's powerful web scraping capabilities, with seamless integration to Google Drive for content storage. Here's how it works: Main Features The workflow automatically scrapes multiple pages from a website's sitemap and saves each page's content as a separate Google Drive document. Key Components Input Configuration Starts with a sitemap URL (default: https://ai.pydantic.dev/sitemap.xml)** Processes the sitemap to extract individual page URLs Includes filtering options to target specific topics or pages Scraping Process Uses Jina.ai's web scraper to extract content from each URL Converts webpage content into clean markdown format Extracts page titles automatically for document naming Storage Integration Creates individual Google Drive documents for each scraped page Names documents using the format \"URL - Page Title\" Saves content in markdown format for better readability Usage Instructions Set your target website's sitemap URL in the \"Set Website URL\" node Configure the \"Filter By Topics or Pages\" node to select specific content Adjust the \"Limit\" node (default: 20 pages) to control batch size Connect your Google Drive account Run the workflow to begin automated scraping Additional Features Built-in rate limiting through the Wait node to prevent overloading servers Batch processing capability for handling large sitemaps The workflow requires no API key for Jina.ai, making it accessible for immediate use while maintaining responsible scraping practices. Joseph LePage | 11 days ago | Free", "completion": "{\"id\":\"xEij0kj2I1DHbL3I\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83d\udca1\ud83c\udf10 Essential Multipage Website Scraper with Jina.ai\",\"tags\":[],\"nodes\":[{\"id\":\"3a503859-ef0a-492d-81c6-37e4f0c4c25e\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-840,0],\"parameters\":{\"color\":3,\"width\":340,\"height\":320,\"content\":\"## Jina.ai Web Scraper\\n### No API Key Required\\n\"},\"typeVersion\":1},{\"id\":\"c5217a1a-f074-409b-8340-72afdc5fc8b5\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-1500,-300],\"parameters\":{},\"typeVersion\":1},{\"id\":\"72af3b00-2632-4877-a0b6-7477e2f468f7\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[-1080,20],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"11f0fa02-51f8-41cc-b789-5c452b6899aa\",\"name\":\"Wait\",\"type\":\"n8n-nodes-base.wait\",\"position\":[80,220],\"webhookId\":\"081ce124-0cbf-4a21-a1e7-2c465f460448\",\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"cf3b5887-8ff2-46e0-ab33-384ab0987cbb\",\"name\":\"Limit\",\"type\":\"n8n-nodes-base.limit\",\"position\":[80,-300],\"parameters\":{\"maxItems\":20},\"typeVersion\":1},{\"id\":\"c4f04d82-aa33-46cf-a8e2-0b4e7"}
{"prompt": "Create an n8n workflow for: Chat with Postgresql Database Who is this template for? This workflow template is designed for any professionals seeking relevent data from database using natural language. How it works Each time user ask's question using the n8n chat interface, the workflow runs. Then the message is processed by AI Agent using relevent tools - Execute SQL Query, Get DB Schema and Tables List and Get Table Definition, if required. Agent uses these tool to form and run sql query which are necessary to answer the questions. Once AI Agent has the data, it uses it to form answer and returns it to the user. Set up instructions Complete the Set up credentials step when you first open the workflow. You'll need a Postgresql Credentials, and OpenAI api key. Template was created in n8n v1.77.0 KumoHQ | 25 days ago | Free", "completion": "{\"id\":\"eOUewYsEzJmQixI6\",\"meta\":{\"instanceId\":\"77c4feba8f41570ef06dc76ece9a6ded0f0d44f7f1477a64c2d71a8508c11faa\",\"templateCredsSetupCompleted\":true},\"name\":\"Chat with Postgresql Database\",\"tags\":[],\"nodes\":[{\"id\":\"6501a54f-a68c-452d-b353-d7e871ca3780\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-300,-80],\"webhookId\":\"cf1de04f-3e38-426c-89f0-3bdb110a5dcf\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"cd32221b-2a36-408d-b57e-8115fcd810c9\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[0,-80],\"parameters\":{\"agent\":\"openAiFunctionsAgent\",\"options\":{\"systemMessage\":\"You are DB assistant. You need to run queries in DB aligned with user requests.\\n\\nRun custom SQL query to aggregate data and response to user. Make sure every table has schema prefix to it in sql query which you can get from `Get DB Schema and Tables List` tool.\\n\\nFetch all data to analyse it for response if needed.\\n\\n## Tools\\n\\n- Execute SQL query - Executes any sql query generated by AI\\n- Get DB Schema and Tables List - Lists all the tables in database with its schema name\\n- Get Table Definition - Gets the table definition from db using table name and schema name\"}},\"typeVersion\":1.7},{\"id\":\"8accbeeb-7eaf-4e9e-aabc-de8ab3a0459b\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-60,160],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{"}
{"prompt": "Create an n8n workflow for: Analyze Reddit Posts with AI to Identify Business Opportunities Use case Manually monitoring Reddit for viable business ideas is time-consuming and inconsistent. This workflow automatically analyzes trending Reddit discussions using AI to surface high-potential opportunities, filter irrelevant content, and generate actionable insights - saving entrepreneurs 10+ hours weekly in market research. What this workflow does This AI-powered workflow automatically collects trending Reddit discussions, analyzes posts for viable business opportunities using GPT-4, applies smart filters to exclude low-value content, and generates scored opportunity reports with market insights. It identifies unmet customer needs through sentiment analysis, prioritizes high-potential ideas using custom criteria, and outputs structured data to Google Sheets for actionable decision-making. Setup Add Reddit,Google and OpenAI credentials Configure target subreddits in Subreddit node Test workflow by testing workflow Review generated opportunity report in Google Sheets How to adjust this template Change data sources**: Replace Reddit trigger with Twitter/X or Hacker News API Modify criteria**: Adjust scoring thresholds in Opportunity Calculator node Add integrations**: Create automatic Slack alerts for urgent opportunities Generate draft business plans using AI Document Writer Alex Huang | 9 days ago | Free +3", "completion": "{\"id\":\"Xx4zOjRFLI8W9PiC\",\"meta\":{\"instanceId\":\"481a48d2941aac0cf9462ce6b93b63097e0c030779c473519ff7c167c8bed8f7\",\"templateCredsSetupCompleted\":true},\"name\":\"Analyze Reddit Posts with AI to Identify Business Opportunities\",\"tags\":[],\"nodes\":[{\"id\":\"52bdf7eb-ee1a-43c5-a0ad-199283003892\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-1400,-640],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e9a000b6-2f35-4928-a8d8-aa2d8cc27513\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-360,-760],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\",\"cachedResultName\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"SOgg2BJ10kvhpBbS\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"cd38a8b6-1369-4209-a80e-9e9949df49c0\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[680,-1080],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"SOgg2BJ10kvhpBbS\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"4749ca62-6061-4dc0-8f1a-b0e995bb3d0f\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[640,-220"}
{"prompt": "Create an n8n workflow for: Open Deep Research - AI-Powered Autonomous Research Workflow Open Deep Research - AI-Powered Autonomous Research Workflow Description This workflow automates deep research by leveraging AI-driven search queries, web scraping, content analysis, and structured reporting. It enables autonomous research with iterative refinement, allowing users to collect, analyze, and summarize high-quality information efficiently. How it works \ud83d\udd39 User Input The user submits a research topic via a chat message. \ud83e\udde0 AI Query Generation A Basic LLM generates up to four refined search queries to retrieve relevant information. \ud83d\udd0e SERPAPI Google Search The workflow loops through each generated query and retrieves top search results using the SerpAPI API. \ud83d\udcc4 Jina AI Web Scraping Extracts and summarizes webpage content from the URLs obtained via SerpAPI. \ud83d\udcca AI-Powered Content Evaluation An AI Agent evaluates the relevance and credibility of the extracted content. \ud83d\udd01 Iterative Search Refinement If the AI finds insufficient or low-quality information, it generates new search queries to improve results. \ud83d\udcdc Final Report Generation The AI compiles a structured markdown report, including sources with citations. Set Up Instructions \ud83d\ude80 Estimated setup time: ~10-15 minutes \u2705 Required API Keys:** SerpAPI \u2192 For Google Search results Jina AI \u2192 For text extraction OpenRouter \u2192 For AI-driven query generation and summarization \u2699\ufe0f n8n Components Used:** AI Agents with memory buffering for iterative research Loops to process multiple search queries efficiently HTTP Requests for direct API interactions with SerpAPI and Jina AI \ud83d\udcdd Recommended Enhancements:** Add sticky notes in n8n to explain each step for new users Implement Google Drive or Notion Integration to save reports automatically \ud83c\udfaf Ideal for: \u2714\ufe0f Researchers & Analysts - Automate background research \u2714\ufe0f Journalists - Quickly gather reliable sources \u2714\ufe0f Developers - Learn how to integrate multiple AI APIs into n8n \u2714\ufe0f Students - Speed up literature reviews \ud83d\udd17 Completely free and open-source! \ud83d\ude80 Leonard | 20 days ago | Free", "completion": "{\"id\":\"WLSqXECfQF7rOj2A\",\"meta\":{\"instanceId\":\"cba4a4a2eb5d7683330e2944837278938831ed3c042e20da6f5049c07ad14798\"},\"name\":\"Open Deep Research - AI-Powered Autonomous Research Workflow\",\"tags\":[],\"nodes\":[{\"id\":\"b7b70ba1-0267-4d2b-91f4-5cc4fd22fd03\",\"name\":\"Chat Message Trigger\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-1940,160],\"webhookId\":\"cb0b9dbe-1f35-441a-b062-29624b0ebc6a\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"55a8a512-f2d4-4aed-93e5-dd9bfa2dcaad\",\"name\":\"Generate Search Queries using LLM\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[-1760,160],\"parameters\":{\"text\":\"=User Query: {{ $('Chat Message Trigger').item.json.chatInput }}\",\"messages\":{\"messageValues\":[{\"message\":\"=You are an expert research assistant. Given a user's query, generate up to four distinct, precise search queries that would help gather comprehensive information on the topic. Return only a JSON list of strings, for example: ['query1', 'query2', 'query3'].\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"5f92361a-b490-479d-8360-c87a100b470e\",\"name\":\"LLM Response Provider (OpenRouter)\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\"position\":[-1760,700],\"parameters\":{\"model\":\"google/gemini-2.0-flash-001\",\"options\":{}},\"credentials\":{\"openRouterApi\":{\"id\":\"WZWYWCfluxuKxZzV\",\"name\":\"OpenRouter account\"}},\"typeVersion\":1},{\"id\":\"4ab360eb-858f-48"}
{"prompt": "Create an n8n workflow for: Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3 This template attempts to replicate OpenAI's DeepResearch feature which, at time of writing, is only available to their pro subscribers. &gt; An agent that uses reasoning to synthesize large amount of online information and complete multi-step research tasks for you. Source Though the inner workings of DeepResearch have not been made public, it is presumed the feature relies on the ability to deep search the web, scrape web content and invoking reasoning models to generate reports. All of which n8n is really good at! Using this workflow, n8n users can enjoy a variation of the Deep Research experience for themselves and their teams at a fraction of the cost. Better yet, learn and customise this Deep Research template for their businesses and/or organisations. Check out the generated reports here: https://jimleuk.notion.site/19486dd60c0c80da9cb7eb1468ea9afd?v=19486dd60c0c805c8e0c000ce8c87acf How it works A form is used to first capture the user's research query and how deep they'd like the researcher to go. Once submitted, a blank Notion page is created which will later hold the final report and the researcher gets to work. The user's query goes through a recursive series of web serches and web scraping to collect data on the research topic to generate partial learnings. Once complete, all learnings are combined and given to a reasoning LLM to generate the final report. The report is then written to the placeholder Notion page created earlier. How to use Duplicate this Notion database template and make sure all Notion related nodes point to it. Sign-up for APIFY.com API Key for web search and scraping services. Ensure you have access to OpenAI's o3-mini model. Alternatively, switch this out for o1 series. You must publish this workflow and ensure the form url is publically accessible. On depth & breadth configuration For more detailed reports, increase depth and breadth but be warned the workflow will take exponentially longer and cost more to complete. The recommended defaults are usually good enough. Depth=1 & Breadth=2 - will take about 5 - 10mins. Depth=1 & Breadth=3 - will take about 15 - 20mins. Dpeth=3 & Breadth=5 - will take about 2+ hours! Customising this workflow I deliberately chose not to use AI-powered scrapers like Firecrawl as I felt these were quite costly and quotas would be quickly exhausted. However, feel free to switch web search and scraping services which suit your environment. Maybe you don't decide to source the web and instead, data collection comes from internal documents instead. This template gives you freedom to change this. Experiment with different Reasoning/Thinking models such as Deepseek and Google's Gemini 2.0. Finally, the LLM prompts could definitely be improved. Refine them to fit your use-case. Credits This template is largely based off the work by David Zhang (dzhng) and his open source implementation of Deep Research: https://github.com/dzhng/deep-research Jimleuk | 21 days ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"a342005e-a88e-419b-b929-56ecbba4a936\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1140,1180],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"learnings\\\": {\\n \\\"type\\\": \\\"array\\\",\\n \\\"description\\\": \\\"List of learnings, max of 3.\\\",\\n \\\"items\\\": { \\\"type\\\": \\\"string\\\" }\\n }\\n }\\n}\"},\"typeVersion\":1.2},{\"id\":\"126b8151-6d20-43b8-8028-8163112c4c5b\",\"name\":\"Set Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[-1360,-460],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"df28b12e-7c20-4ff5-b5b8-dc773aa14d4b\",\"name\":\"request_id\",\"type\":\"string\",\"value\":\"={{ $execution.id }}\"},{\"id\":\"9362c1e7-717d-444a-8ea2-6b5f958c9f3f\",\"name\":\"prompt\",\"type\":\"string\",\"value\":\"={{ $json['What would you like to research?'] }}\"},{\"id\":\"09094be4-7844-4a9e-af82-cc8e39322398\",\"name\":\"depth\",\"type\":\"number\",\"value\":\"={{ $json.depth }}\"},{\"id\":\"3fc30a30-7806-4013-835d-97e27ddd7ae1\",\"name\":\"breadth\",\"type\":\"number\",\"value\":\"={{ $json.breadth }}\"}]}},\"typeVersion\":3.4},{\"id\":\"1d0fb87b-263d-46c2-b016-a29ba1d407"}
{"prompt": "Create an n8n workflow for: Allow Users to Send a Sequence of Messages to an AI Agent in Telegram Use Case When creating chatbots that interface through applications such as Telegram and WhatsApp, users can often sends multiple shorter messages in quick succession, in place of a single, longer message. This workflow accounts for this behaviour. What it Does This workflow allows users to send several messages in quick succession, treating them as one coherent conversation instead of separate messages requiring individual responses. How it Works When messages arrive, they are stored in a Supabase PostgreSQL table The system waits briefly to see if additional messages arrive If no new messages arrive within the waiting period, all queued messages are: Combined and processed as a single conversation Responded to with one unified reply Deleted from the queue Setup Create a table in Supabase called message_queue. It needs to have the following columns: user_id (uint8), message (text), and message_id (uint8) Add your Telegram, Supabase, OpenAI, and PostgreSQL credentials Activate the workflow and test by sending multiple messages the Telegram bot in one go Wait ten seconds after which you will receive a single reply to all of your messages How to Modify it to Your Needs Change the value of Wait Amount in the Wait 10 Seconds node in order to to modify the buffering window Add a System Message to the AI Agent to tailor it to your specific use case Replace the OpenAI sub-node to use a different language model Chris Carr | 16 days ago | Free", "completion": "{\"id\":\"DnHvQ3KL8v8r5L5Z\",\"meta\":{\"instanceId\":\"ac63467607103d9c95dd644384984672b90b1cb03e07edbaf18fe72b2a6c45bb\",\"templateCredsSetupCompleted\":true},\"name\":\"Telegram Chat with Buffering\",\"tags\":[],\"nodes\":[{\"id\":\"a3cc74e9-c696-48de-a04e-d48555641897\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1640,-800],\"parameters\":{\"color\":7,\"width\":220,\"height\":280,\"content\":\"## 1. Receive Message\\n\\n\"},\"typeVersion\":1},{\"id\":\"ff18667d-0a31-4768-acf8-ed0d53b2f382\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[160,-840],\"parameters\":{\"color\":7,\"width\":600,\"height\":520,\"content\":\"## 3. AI Assistant\\n\"},\"typeVersion\":1},{\"id\":\"ce90f954-19b6-4224-ae88-b20c4da639e6\",\"name\":\"Reply\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[920,-700],\"webhookId\":\"e3313c88-0d56-4d06-81cf-b48870dfe2fe\",\"parameters\":{\"text\":\"={{ $json.output }}\",\"chatId\":\"={{ $('Receive Message').item.json.message.chat.id }}\",\"additionalFields\":{\"appendAttribution\":false}},\"credentials\":{\"telegramApi\":{\"id\":\"lvrGkOs0ywXp5agp\",\"name\":\"Telegram bsde.ai\"}},\"typeVersion\":1.2},{\"id\":\"6f46d89b-034c-47ea-a217-8d007bec1531\",\"name\":\"Receive Message\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[-1580,-680],\"webhookId\":\"5047a673-ca1d-4e87-b51b-893108de0a59\",\""}
{"prompt": "Create an n8n workflow for: \ud83d\udc0b\ud83e\udd16 DeepSeek AI Agent + Telegram + LONG TERM Memory \ud83e\udde0 This n8n workflow template is designed to integrate a DeepSeek AI agent with Telegram, incorporating long-term memory capabilities for personalized and context-aware responses. Here's a detailed breakdown: Core Features Telegram Integration Uses a webhook to receive messages from Telegram users. Validates user identity and message content before processing. AI-Powered Responses Employs DeepSeek's AI models for conversational interactions. Includes memory capabilities to personalize responses based on past interactions. Error Handling Sends an error message if the input cannot be processed. Model Options \ud83e\udde0 DeepSeek-V3 Chat**: Handles general conversational tasks. DeepSeek-R1 Reasoning**: Provides advanced reasoning capabilities for complex queries. Memory Buffer Window**: Maintains session context for ongoing conversations. Quick Setup \ud83d\udee0\ufe0f Telegram Webhook Configuration Set up a webhook using the Telegram Bot API: https://api.telegram.org/bot{my_bot_token}/setWebhook?url={url_to_send_updates_to} Replace {my_bot_token} with your bot's token and {url_to_send_updates_to} with your n8n webhook URL. Verify the webhook setup using: https://api.telegram.org/bot{my_bot_token}/getWebhookInfo DeepSeek API Configuration Base URL: https://api.deepseek.com Obtain your API key from the DeepSeek platform. Implementation Details \ud83d\udd27 User Validation The workflow validates the user's first name, last name, and ID using data from incoming Telegram messages. Only authorized users proceed to the next steps. Message Routing Routes messages based on their type (text, audio, or image) using a switch node. Ensures appropriate handling for each message format. AI Agent Interaction Processes text input using DeepSeek-V3 or DeepSeek-R1 models. Customizable system prompts define the AI's behavior and rules, ensuring user-centric and context-aware responses. Memory Management Retrieves long-term memories stored in Google Docs to enhance personalization. Saves new memories based on user interactions, ensuring continuity across sessions. Joseph LePage | 24 days ago | Free", "completion": "{\"id\":\"rtsvydad1MOCryia\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83d\udc0b\ud83e\udd16 DeepSeek AI Agent + Telegram + LONG TERM Memory \ud83e\udde0\",\"tags\":[],\"nodes\":[{\"id\":\"23b50c07-39a8-4166-ab13-9683b3ee25e6\",\"name\":\"Check User & Chat ID\",\"type\":\"n8n-nodes-base.if\",\"position\":[-80,160],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"5fe3c0d8-bd61-4943-b152-9e6315134520\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.first_name }}\",\"rightValue\":\"={{ $json.first_name }}\"},{\"id\":\"98a0ea91-0567-459c-bbce-06abc14a49ce\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.last_name }}\",\"rightValue\":\"={{ $json.last_name }}\"},{\"id\":\"18a96c1f-f2a0-4a2a-b789-606763df4423\",\"operator\":{\"type\":\"number\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.id }}\",\"rightValue\":\"={{ $json.id }}\"}]},\"looseTypeValidation\":\"=\"},\"typeVersion\":2.2},{\"id\":\"ecbc13fe-305d-4cdd-b35c-3e119e8e8b5d\",\"name\":\"Error message\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[160,440],\"parameters\":{\"text\":\"=Unable to process your"}
{"prompt": "Create an n8n workflow for: Build Your Own Counseling Chatbot on LINE to Support Mental Health Conversations Are you looking to create a counseling chatbot that provides emotional support and mental health guidance through the LINE messaging platform ? This guide will walk you through connecting LINE with powerful AI language models like GPT-4 to build a chatbot that supports users in navigating their emotions, offering 24/7 conversational therapy and accessible mental health resources . By leveraging LINE's webhook integration and Azure OpenAI , this template allows you to design a chatbot that is both empathetic and efficient, ensuring users receive timely and professional responses. Whether you're a developer, counselor, or business owner, this guide will help you create a customizable counseling chatbot tailored to your audience's needs. Who Is This Template For? Developers who want to integrate AI-powered chatbots into the LINE platform for mental health applications. Counselors & Therapists looking to expand their reach and provide automated emotional support to clients outside of traditional sessions. Businesses & Organizations focused on improving mental health accessibility and offering innovative solutions to their users. Educators & Nonprofits seeking tools to provide free or low-cost counseling services to underserved communities. How this work? Line Webhook to receive new message Send loading animation in Line Check if the input is text or not Send the text as prompt in chat model (GPT 4o) Reply the message to user (you'll need 'edit field' to format it before reply) Pre-Requisites You have access to the LINE Developers Console. An Azure OpenAI account with necessary credentials. Set-up To receive messages from LINE, configure your webhook: Set up a webhook in LINE Developer Console. Copy the Webhook URL from the Line Chatbot node and paste it into the LINE Console. Ensure to remove any 'test' part when moving to production. The loading animation reassures users that the system is processing their request. Authorize using header authorization Message Handling Use the Check Message Type IsText? node to verify if the incoming message is text. If the message type is text, proceed with ChatGPT processing; otherwise, send a reply indicating non-text inputs are not supported. AI Agent Configuration Define the system message within the AI Agent node to guide the conversation based on desired interaction principles. Connect the Azure OpenAI Chat Model to the AI Agent. Formatting Responses Ensure responses are properly formatted before sending them back to the user. Reply Message Use the ReplyMessage - Line node to send the formatted response. Ensure proper header authorization using Bearer tokens. lin@davoy.tech | 10 days ago | Free", "completion": "{\"id\":\"ghfbOYrOSiQVAbl5\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\"},\"name\":\"Chatbot AI\",\"tags\":[],\"nodes\":[{\"id\":\"6eec6665-eea6-4aaa-8ae5-2fc7bf0c4746\",\"name\":\"Loading Animation\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-520,340],\"parameters\":{\"url\":\"https://api.line.me/v2/bot/chat/loading/start\",\"body\":\"={\\n \\\"chatId\\\": \\\"{{ $json.body.events[0].source.userId }}\\\",\\n \\\"loadingSeconds\\\": 60\\n}\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"contentType\":\"raw\",\"authentication\":\"genericCredentialType\",\"rawContentType\":\"application/json\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"PQI3F0ibV3txKWfv\",\"name\":\"Talking Therapy Line@\"}},\"typeVersion\":4.2},{\"id\":\"72ff06e5-e1d8-47e7-be15-888ec9171c72\",\"name\":\"ReplyMessage - Not supported\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[100,760],\"parameters\":{\"url\":\"https://api.line.me/v2/bot/message/reply\",\"body\":\"={\\n \\\"replyToken\\\":\\\"{{ $('Line Chatbot').item.json.body.events[0].replyToken }}\\\",\\n \\\"messages\\\":[\\n {\\n \\\"type\\\":\\\"text\\\",\\n \\\"text\\\":\\\"Currently, the input of image or other type are not supported.\\\"\\n }\\n ]\\n}\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"contentType\":\"raw\",\"sendHeaders\":true,\"rawContentType\":\"application/json\",\"headerParameters\":{\"parameters\":[{\"name\":\"Authorization\",\"value\":\"Bearer /lQWKI4dp71pOMWZu2q18mL8P+zwf9iIOBzUDQOPMqLGMMIg88J"}
{"prompt": "Create an n8n workflow for: Upload to Instagram and Tiktok from Google Drive Description This automation template is designed for content creators, digital marketers, and social media managers looking to simplify their video posting workflow. It automates the process of generating engaging video descriptions and uploading content to both Instagram and TikTok, making your social media management more efficient and error-free. Who Is This For? Content Creators & Influencers:** Streamline your video uploads and focus more on creating content. Digital Marketers:** Ensure consistent posting across multiple platforms with minimal manual intervention. Social Media Managers:** Automate repetitive tasks and maintain a steady online presence. What Problem Does This Workflow Solve? Manually creating descriptions and uploading videos to different platforms can be time-consuming and error-prone. This workflow addresses these challenges by: Automating Video Uploads:** Monitors a designated Google Drive folder for new videos. Generating Descriptions:** Uses OpenAI to transcribe video audio and generate engaging, customized social media descriptions. Ensuring Multi-Platform Consistency:** Simultaneously posts your video with the generated description to Instagram and TikTok. Error Notifications:** Optional Telegram integration sends alerts in case of issues, ensuring smooth operations. How It Works Video Upload: Place your video in the designated Google Drive folder. Description Generation: The automation triggers OpenAI to transcribe your video\u2019s audio and generate a captivating description. Content Distribution: Automatically uploads the video and description to both Instagram and TikTok. Error Handling: Sends Telegram notifications if any issues arise during the process. Setup Generate an API token at upload-post.com and configure it in both the Upload to TikTok and Upload to Instagram nodes. Google Cloud Project: Create a project in Google Cloud Platform, enable the Google Drive API, and generate the necessary OAuth credentials to connect to your Google Drive account. Set up your Google Drive folder in the Google Drive Trigger node. Customize the OpenAI prompt in the Generate Social Description node to match your brand\u2019s tone. (Optional) Configure Telegram credentials for error notifications. Requirements Accounts:** upload-post.com, Google Drive, and (optionally) Telegram. API Keys & Credentials:** Upload-post.com API token, OpenAI API key, and (optional) Telegram bot token. Google Cloud:** A project with the Google Drive API enabled and valid OAuth credentials. Use this template to enhance your productivity, maintain consistency across your social media channels, and engage your audience with high-quality video content. Juan Carlos Cavero Gracia | 19 days ago | Free", "completion": "{\"id\":\"cZPEH5aMMZNy61xs\",\"meta\":{\"instanceId\":\"3378b0d68c3b7ebfc71b79896d94e1a044dec38e99a1160aed4e9c323910fbe2\",\"templateCredsSetupCompleted\":true},\"name\":\"template in store\",\"tags\":[],\"nodes\":[{\"id\":\"14f93cdb-72cb-419a-b8d7-a68ae9383290\",\"name\":\"Google Drive Trigger\",\"type\":\"n8n-nodes-base.googleDriveTrigger\",\"position\":[440,320],\"parameters\":{\"event\":\"fileCreated\",\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]},\"triggerOn\":\"specificFolder\",\"folderToWatch\":{}},\"credentials\":{\"googleDriveOAuth2Api\":{}},\"typeVersion\":1},{\"id\":\"d4ab0d11-b110-46fa-9cd2-6091737c302e\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[620,320],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"\",\"value\":\"={{ $json.id || $json.data[0].id }}\"},\"options\":{},\"operation\":\"download\",\"authentication\":\"oAuth2\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"2TbhWtnbRfSloGxX\",\"name\":\"Google Drive account\"}},\"retryOnFail\":true,\"typeVersion\":1,\"waitBetweenTries\":5000},{\"id\":\"fde9df88-3f9e-4732-bb1c-72eb33ce6826\",\"name\":\"Error Trigger\",\"type\":\"n8n-nodes-base.errorTrigger\",\"position\":[840,660],\"parameters\":{},\"typeVersion\":1},{\"id\":\"ecfe1ad1-6887-492b-a2f7-f9b6c43f9b91\",\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1180,640],\"webhookId\":\"f6729386-9905-45f1-800f-4fe01a06ac9c\",\"parameters\":{\"text\":\"=\ud83d\udd14 ERROR SUBIENDO VIDEOS\",\"chatId\":\"\",\"add"}
{"prompt": "Create an n8n workflow for: AI Automated HR Workflow for CV Analysis and Candidate Evaluation How it Works This workflow automates the process of handling job applications by extracting relevant information from submitted CVs, analyzing the candidate's qualifications against a predefined profile, and storing the results in a Google Sheet. Here\u2019s how it operates: Data Collection and Extraction: The workflow begins with a form submission (On form submission node), which triggers the extraction of data from the uploaded CV file using the Extract from File node. Two informationExtractor nodes (Qualifications and Personal Data) are used to parse specific details such as educational background, work history, skills, city, birthdate, and telephone number from the text content of the CV. Processing and Evaluation: A Merge node combines the extracted personal and qualification data into a single output. This merged data is then passed through a Summarization Chain that generates a concise summary of the candidate\u2019s profile. An HR Expert chain evaluates the candidate against a desired profile (Profile Wanted), assigning a score and providing considerations for hiring. Finally, all collected and processed data including the evaluation results are appended to a Google Sheets document via the Google Sheets node for further review or reporting purposes [[9]]. Set Up Steps To replicate this workflow within your own n8n environment, follow these steps: Configuration: Begin by setting up an n8n instance if you haven't already; you can sign up directly on their website or self-host the application. Import the provided JSON configuration into your n8n workspace. Ensure that all necessary credentials (e.g., Google Drive, Google Sheets, OpenAI API keys) are correctly configured under the Credentials section since some nodes require external service integrations like Google APIs and OpenAI for language processing tasks. Customization: Adjust the parameters of each node according to your specific requirements. For example, modify the fields in the formTrigger node to match what kind of information you wish to collect from applicants. Customize the prompts given to AI models in nodes like Qualifications, Summarization Chain, and HR Expert so they align with the type of analyses you want performed on the candidates' profiles. Update the destination settings in the Google Sheets node to point towards your own spreadsheet where you would like the final outputs recorded. n3w Italia | 25 days ago | Free +2", "completion": "{\"id\":\"t1P14FvfibKYCh3E\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"HR-focused automation pipeline with AI\",\"tags\":[],\"nodes\":[{\"id\":\"b1092f93-502c-4af0-962e-2b69311b92a3\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-520,-200],\"webhookId\":\"2a87705d-8ba1-41f1-80ef-85f364ce253e\",\"parameters\":{\"options\":{},\"formTitle\":\"Send CV\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Name\",\"placeholder\":\"Name\",\"requiredField\":true},{\"fieldType\":\"email\",\"fieldLabel\":\"Email\",\"placeholder\":\"Email\",\"requiredField\":true},{\"fieldType\":\"file\",\"fieldLabel\":\"CV\",\"requiredField\":true,\"acceptFileTypes\":\".pdf\"}]}},\"typeVersion\":2.2},{\"id\":\"77edfe2a-4c6a-48c8-8dc9-b275491be090\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[-160,-200],\"parameters\":{\"options\":{},\"operation\":\"pdf\",\"binaryPropertyName\":\"CV\"},\"typeVersion\":1},{\"id\":\"ebf2e194-3515-4c0a-8745-790b63bf336f\",\"name\":\"Qualifications\",\"type\":\"@n8n/n8n-nodes-langchain.informationExtractor\",\"position\":[160,-100],\"parameters\":{\"text\":\"={{ $json.text }}\",\"options\":{\"systemPromptTemplate\":\"You are an expert extraction algorithm.\\nOnly extract relevant information from the text.\\nIf you do not know the value of an attribute asked to extract, you may omit the attribute's value.\"},\"attributes\":{\"attributes\":[{\"name\":\"Educational qualification\",\"required\":true,\"description\":\"Summary of your academic career. Focus on your high school and university studies. Summarize in 100 words maximum and also include your grade if applicable.\"},{\"name\":\"Job History\",\"required\":true,\"description\":\"Work history summary"}
{"prompt": "Create an n8n workflow for: Automate Content Generator for WordPress with DeepSeek R1 This workflow is designed to generate SEO-friendly content with DeepSeek R1 (or V3), publish it on WordPress, and update a Google Sheets document with the details of the created post. Below is a detailed analysis of what each node in the workflow does: How It Works Triggering the Workflow: The workflow starts with a Manual Trigger node, which is activated when the user clicks \"Test workflow\" in the n8n interface. Fetching Data: The Get Ideas node retrieves data from a Google Sheets document. It reads a specific sheet and filters the data based on the \"ID POST\" column, returning the first matching row. Setting the Prompt: The Set your prompt node extracts the PROMPT field from the Google Sheets data and assigns it to a variable for use in subsequent steps. Generating Content: The Generate article node uses an AI model (DeepSeek) to create an SEO-friendly article based on the prompt. The article includes an introduction, 2-3 chapters, and a conclusion, formatted in HTML. The Generate title node uses the same AI model to generate a concise, SEO-optimized title for the article. Publishing on WordPress: The Create post on WordPress node creates a new draft post on WordPress using the generated title and article content. Generating and Uploading an Image: The Generate Image node creates a photorealistic image based on the article title using an AI model (OpenAI). The Upload image node uploads the generated image to WordPress as a media file. The Set Image node assigns the uploaded image as the featured image for the WordPress post. Updating Google Sheets: The Update Sheet node updates the original Google Sheets document with the post details, including the title, post ID, creation date, and row number. Set Up Steps Configure Google Sheets Integration: Set up the Google Sheets node to connect to your Google account and specify the document ID and sheet name to read from and update. Set Up AI Models: Configure the OpenAI nodes (for generating the article, title, and image) with the appropriate API credentials and model settings (e.g., deepseek-reasoner for text generation). Configure WordPress Integration: Set up the WordPress node with your WordPress site's API credentials to allow creating posts and uploading media. Define the Prompt and Content Structure: In the Set your prompt node, ensure the prompt variable is correctly mapped to the data from Google Sheets. In the Generate article and Generate title nodes, define the instructions for the AI model to generate the desired content. Set Up Image Generation: Configure the Generate Image node with the appropriate prompt and image settings (e.g., size, quality, style). Configure HTTP Requests for Media Upload: Set up the Upload image and Set Image nodes to use the WordPress REST API for uploading and assigning the featured image. Map Data for Google Sheets Update: In the Update Sheet node, map the relevant fields (e.g., title, post ID, date) to the appropriate columns in the Google Sheets document. Test and Activate the Workflow: Run the workflow manually to ensure all steps execute correctly. Once verified, activate the workflow for automated execution. Overall purpose of the workflow This workflow automates the creation of SEO-friendly content for a WordPress blog. Starting from a prompt extracted from a Google Sheets document, it generates an article, a title, and an image, publishes the post on WordPress, and updates the Google Sheets document with the details of the created post. This process is useful for blog managers who want to automate content creation and publishing. n3w Italia | 1 month ago | Free", "completion": "{\"id\":\"p5bfwpcRy6LK33Io\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Automate Content Generator for WordPress with DeepSeek R1\",\"tags\":[],\"nodes\":[{\"id\":\"c4a6995f-7769-4b77-80ca-1e6bccef77c1\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-20,200],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c76b1458-5130-41e7-b2f2-1cfe22eab536\",\"name\":\"Get Ideas\",\"type\":\"n8n-nodes-base.googleSheets\",\"position\":[200,200],\"parameters\":{\"options\":{},\"sheetName\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"=Sheet1\"},\"documentId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"YOURDOCUMENT\"}},\"credentials\":{\"googleSheetsOAuth2Api\":{\"id\":\"JYR6a64Qecd6t8Hb\",\"name\":\"Google Sheets account\"}},\"typeVersion\":4.5},{\"id\":\"8d17a640-3e15-42e9-9481-e3291d395ccd\",\"name\":\"Set your prompt\",\"type\":\"n8n-nodes-base.set\",\"position\":[420,200],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"3e8d2523-66aa-46fe-adcc-39dc78b9161e\",\"name\":\"prompt\",\"type\":\"string\",\"value\":\"={{ $json.PROMPT }}\"}]}},\"typeVersion\":3.4},{\"id\":\"4f0e9065-b331-49ed-acd9-77c7c43e89a5\",\"name\":\"Create post on Wordpress\",\"type\":\"n8n-nodes-base.wordpress\",\"position\":[0,500],\"parameters\":{\"title\":\"={{ $json.message.content }}\",\"additionalFields\":{\"status\":\"draft\",\"content\":\"={{ $('"}
{"prompt": "Create an n8n workflow for: AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants The \"Voice RAG Chatbot with ElevenLabs and OpenAI\" workflow in n8n is designed to create an interactive voice-based chatbot system that leverages both text and voice inputs for providing information. Ideal for shops, commercial activities and restaurants How it works: Here's how it operates: Webhook Activation: The process begins when a user interacts with the voice agent set up on ElevenLabs, triggering a webhook in n8n. This webhook sends a question from the user to the AI Agent node. AI Agent Processing: Upon receiving the query, the AI Agent node processes the input using predefined prompts and tools. It extracts relevant information from the knowledge base stored within the Qdrant vector database. Knowledge Base Retrieval: The Vector Store Tool node interfaces with the Qdrant Vector Store to retrieve pertinent documents or data segments matching the user\u2019s query. Text Generation: Using the retrieved information, the OpenAI Chat Model generates a coherent response tailored to the user\u2019s question. Response Delivery: The generated response is sent back through another webhook to ElevenLabs, where it is converted into speech and delivered audibly to the user. Continuous Interaction: For ongoing conversations, the Window Buffer Memory ensures context retention by maintaining a history of interactions, enhancing the conversational flow. Set up steps: To configure this workflow effectively, follow these detailed setup instructions: ElevenLabs Agent Creation: Begin by creating an agent on ElevenLabs (e.g., named 'test_n8n'). Customize the first message and define the system prompt specific to your use case, such as portraying a character like a waiter at \"Pizzeria da Michele\". Add a Webhook tool labeled 'test_chatbot_elevenlabs' configured to receive questions via POST requests. Qdrant Collection Initialization: Utilize the HTTP Request nodes ('Create collection' and 'Refresh collection') to initialize and clear existing collections in Qdrant. Ensure you update placeholders QDRANTURL and COLLECTION accordingly. Document Vectorization: Use Google Drive integration to fetch documents from a designated folder. These documents are then downloaded and processed for embedding. Employ the Embeddings OpenAI node to generate embeddings for the downloaded files before storing them into Qdrant via the Qdrant Vector Store node. AI Agent Configuration: Define the system prompt for the AI Agent node which guides its behavior and responses based on the nature of queries expected (e.g., product details, troubleshooting tips). Link necessary models and tools including OpenAI language models and memory buffers to enhance interaction quality. Testing Workflow: Execute test runs of the entire workflow by clicking 'Test workflow' in n8n alongside initiating tests on the ElevenLabs side to confirm all components interact seamlessly. Monitor logs and outputs closely during testing phases to ensure accurate data flow between systems. Integration with Website: Finally, integrate the chatbot widget onto your business website replacing placeholder AGENT_ID with the actual identifier created earlier on ElevenLabs. By adhering to these comprehensive guidelines, users can successfully deploy a sophisticated voice-driven chatbot capable of delivering precise answers utilizing advanced retrieval-augmented generation techniques powered by OpenAI and ElevenLabs technologies. n3w Italia | 28 days ago | Free +5", "completion": "{\"id\":\"ibiHg6umCqvcTF4g\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Voice RAG Chatbot with ElevenLabs and OpenAI\",\"tags\":[],\"nodes\":[{\"id\":\"5898da57-38b0-4d29-af25-fe029cda7c4a\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[-180,800],\"parameters\":{\"text\":\"={{ $json.body.question }}\",\"options\":{},\"promptType\":\"define\"},\"typeVersion\":1.7},{\"id\":\"81bbedb6-5a07-4977-a68f-2bdc75b17aba\",\"name\":\"Vector Store Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolVectorStore\",\"position\":[20,1040],\"parameters\":{\"name\":\"company\",\"description\":\"Risponde alle domande relative a ci\u00f2 che ti viene chiesto\"},\"typeVersion\":1},{\"id\":\"fd021f6c-248d-41f4-a4f9-651e70692327\",\"name\":\"Qdrant Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStoreQdrant\",\"position\":[-140,1300],\"parameters\":{\"options\":{},\"qdrantCollection\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"=COLLECTION\"}},\"credentials\":{\"qdrantApi\":{\"id\":\"iyQ6MQiVaF3VMBmt\",\"name\":\"QdrantApi account\"}},\"typeVersion\":1},{\"id\":\"84aca7bb-4812-498f-b319-88831e4ca412\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[-140,1460],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"CDX6QM4gLYanh0P4\",\"name\":\"OpenAi account\"}},\"typeVersion"}
{"prompt": "Create an n8n workflow for: Generate Instagram Content from Top Trends with AI Image Generation How it works This automated workflow discovers trending Instagram posts and creates similar AI-generated content. Here's the high-level process: 1. Content Discovery & Analysis Scrapes trending posts from specific hashtags Analyzes visual elements using AI Filters out videos and duplicates 2. AI Content Generation Creates unique images based on trending content Generates engaging captions with relevant hashtags Maintains brand consistency while being original 3. Automated Publishing Posts content directly to Instagram Monitors publication status Sends notifications via Telegram Set up steps Setting up this workflow takes approximately 15-20 minutes: 1. API Configuration (7-10 minutes) Instagram Business Account setup Telegram Bot creation API key generation (OpenAI, Replicate, Rapid Api) 2. Database Setup (3-5 minutes) Create required database table Configure PostgreSQL credentials 3. Workflow Configuration (5-7 minutes) Set scheduling preferences Configure notification settings Test connection and permissions Detailed technical specifications and configurations are available in sticky notes within the workflow. mustafa kendig\u00fczel | 1 month ago | Free", "completion": "{\"id\":\"H7porcmXYj7StO23\",\"meta\":{\"instanceId\":\"35409808e3cc9dd8ecfa6f7b93ae931f074920a2f681e667da8974c0ecf81c52\",\"templateId\":\"2537\",\"templateCredsSetupCompleted\":true},\"name\":\"Generate Instagram Content from Top Trends with AI Image Generation\",\"tags\":[],\"nodes\":[{\"id\":\"8c49be2b-6320-4eb0-8303-6448ced34636\",\"name\":\"If media status is finished\",\"type\":\"n8n-nodes-base.if\",\"position\":[1420,260],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"0304efee-33b2-499e-bad1-9238c1fc2999\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.status_code }}\",\"rightValue\":\"FINISHED\"}]}},\"typeVersion\":2.2},{\"id\":\"f0cc0be5-6d35-4334-a124-139fa8676d07\",\"name\":\"If media status is finished1\",\"type\":\"n8n-nodes-base.if\",\"position\":[2000,260],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"0304efee-33b2-499e-bad1-9238c1fc2999\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.status_code }}\",\"rightValue\":\"PUBLISHED\"}]}},\"typeVersion\":2.2},{\"id\":\"c8d8d8cd-8501-4d1b-ac28-8cb3fa74d9d7\",\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1580,440],\"parameters\":{\"text\":\"Video upload edilmeden \u00f6nce bir problem oldu\",\"chatId\":\"="}
{"prompt": "Create an n8n workflow for: AI Agent with Ollama for current weather and wiki This workflow template demonstrates how to create an AI-powered agent that provides users with current weather information and Wikipedia summaries. By integrating n8n with Ollama's local Large Language Models (LLMs), this template offers a seamless and privacy-conscious solution for real-time data retrieval and summarization. Who is this for? Developers and Enthusiasts: Individuals interested in building AI-driven workflows without relying on external APIs. Privacy-Conscious Users: Those who prefer processing data locally to maintain control over their information. Educators and Students: Learners seeking hands-on experience with AI integrations and workflow automation. What problem does this workflow solve? Accessing up-to-date weather information and concise Wikipedia summaries typically requires multiple API calls to external services, which can raise privacy concerns and incur costs. This workflow addresses these issues by utilizing Ollama's self-hosted LLMs within n8n, enabling users to retrieve and process information locally. What this workflow does: User Input Capture: Begins with a chat interface where users can input queries. AI Processing: The input is sent to an AI Agent node configured with Ollama's LLMs, which interprets the query and determines the required actions. Weather Retrieval: For weather-related queries, the workflow fetches current weather data from a specified source. Wikipedia Summarization: For queries seeking information, it retrieves relevant Wikipedia content and generates concise summaries. Setup: Install Required Tools: Ollama: Install and run Ollama to manage local LLMs. Configure n8n Workflow: Import the provided workflow template into your n8n instance. Set up the AI Agent node to connect with Ollama's API. Ensure nodes responsible for fetching weather data and Wikipedia content are correctly configured. Run the Workflow: Start the workflow and interact with the chat interface to test various queries. How to customize this workflow to your needs: Automate Triggers: Set up scheduled triggers to provide users with regular updates, such as daily weather forecasts or featured Wikipedia articles. Thomas Chan | 14 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateId\":\"2931\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"100f23d3-cbe9-458a-9ef1-7cc5fcba8f3c\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[640,540],\"parameters\":{\"width\":300,\"height\":205,\"content\":\"### The conversation history(last 20 messages) is stored in a buffer memory\"},\"typeVersion\":1},{\"id\":\"b48f989f-deb9-479c-b163-03f098d00c9c\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[380,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"add8e8df-6b2a-4cbd-84e7-3b006733ef7d\",\"name\":\"Wikipedia\",\"type\":\"@n8n/n8n-nodes-langchain.toolWikipedia\",\"position\":[1180,640],\"parameters\":{},\"typeVersion\":1},{\"id\":\"a97454a8-001d-4986-9cb5-83176229ea70\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[980,540],\"parameters\":{\"width\":300,\"height\":205,\"content\":\"### Tools which agent can use to accomplish the task\"},\"typeVersion\":1},{\"id\":\"52b57e72-8cc9-4865-9a00-d03b2e7f1b92\",\"name\":\"Sticky Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[600,160],\"parameters\":{\"width\":422,\"height\":211,\"content\":\"### Conversational agent will utilise available tools to answer the prompt. \"},\"typeVersion\":1},{\"id\":\"8f0653ab-376b-40b9-b876-e608defdeb89\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n"}
{"prompt": "Create an n8n workflow for: Automated AI image tagging and writing the keywords into the image file Welcome to my Automated Image Metadata Tagging Workflow! This workflow automatically analyzes the image content with the help of AI and writes it directly back into the image file as keywords. This workflow has the following sequence: Google Drive trigger (scan for new files added in a specific folder) Download the added image file Analyse the content of the image and extract the file as Base64 code Merge Metadata and Base64 Code Code Node to write the Keywords into the Metadata (dc:subject) Convert to file and update the original file in the Google Drive folder The following accesses are required for the workflow: Google Drive: Documentation AI API access (e.g. via OpenAI, Anthropic, Google or Ollama) You can contact me via LinkedIn, if you have any questions: https://www.linkedin.com/in/friedemann-schuetz Friedemann Schuetz | 8 days ago | Free", "completion": "{\"id\":\"XbawQw3cvClu2wsx\",\"meta\":{\"instanceId\":\"1acdaec6c8e84424b4715cf41a9f7ec057947452db21cd2e22afbc454c8711cd\",\"templateCredsSetupCompleted\":true},\"name\":\"Automated Image Metadata Tagging\",\"tags\":[],\"nodes\":[{\"id\":\"cd1dba71-345b-45ae-8110-4fb57291f363\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[260,180],\"parameters\":{\"options\":{},\"operation\":\"binaryToPropery\"},\"typeVersion\":1},{\"id\":\"7973b64e-ae92-44c9-aa8e-002c32c25def\",\"name\":\"Convert to File\",\"type\":\"n8n-nodes-base.convertToFile\",\"position\":[920,80],\"parameters\":{\"options\":{},\"operation\":\"toBinary\",\"sourceProperty\":\"data\"},\"typeVersion\":1.1},{\"id\":\"5fe13d4e-566b-459f-8830-f16829a34284\",\"name\":\"Analyze Image Content\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[260,-40],\"parameters\":{\"text\":\"=Deliver a comma seperated list describing the content of this image.\",\"modelId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"chatgpt-4o-latest\",\"cachedResultName\":\"CHATGPT-4O-LATEST\"},\"options\":{},\"resource\":\"image\",\"inputType\":\"base64\",\"operation\":\"analyze\"},\"credentials\":{\"openAiApi\":{\"id\":\"EjchNb5GBqYh0Cqn\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.8},{\"id\":\"876db6b5-6615-4e9d-8e1a-2d8220b2019f\",\"name\":\"Download Image File\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[-20,60],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"={{ $json.id }}\"},\"options\":{},\"operation\":\"download\"},\"credentials"}
{"prompt": "Create an n8n workflow for: Get Daily Exercise Plan with Flex Message via LINE The YogiAI workflow automates sending daily yoga pose reminders and related information via Line Push Messages . This automation leverages data from a Google Sheets database containing yoga pose details such as names, image URLs, and links to ensure users receive personalized and engaging content every day. Purpose Provide users with daily yoga pose suggestions tailored to their practice. Deliver visually appealing and informative content through Line's Flex Messages, including images and clickable links. Log user interactions and preferences back into Google Sheets to refine future recommendations. Key Features Automated Daily Reminders : Sends a curated list of yoga poses at a scheduled time (21:30 Bangkok time). Dynamic Content Generation : Uses AI to rewrite and format messages in a user-friendly manner, complete with emojis and clear instructions. Integration with Google Sheets : Pulls data from a predefined Google Sheet and logs interactions for continuous improvement. Customizable Messaging : Ensures JSON outputs are properly formatted for Line\u2019s Flex Message API, allowing for interactive and visually rich content. Data Source Google Sheets Structure The workflow relies on a Google Sheet structured as follows: PoseName : The name of the yoga pose. uri : The image URL representing the pose. url : A clickable link directing users to more information about the pose. Sample Data Layout Supine Angle https://example.com/SupineAngle-tn146.png https://example.com/pose/SupineAngle Warrior II https://example.com/WarriorII-tn146.png https://example.com/pose/WarriorII *Note : Ensure that you update the Google Sheet with your own data. Refer to this sample sheet for reference. * Scheduled Trigger The workflow is triggered daily at 21:30 (9:30 PM) Bangkok Time (Asia/Bangkok) . This ensures timely delivery of reminders to users, keeping them engaged with their yoga practice. Workflow Process Data Retrieval Node: Get PoseName Fetches yoga pose details from the specified range in the Google Sheet. Content Generation Node: WritePosesToday Utilizes Azure OpenAI to craft user-friendly text, complete with emojis and clear instructions. Node: RewritePosesToday Formats the AI-generated text specifically for Line messaging, ensuring compatibility and visual appeal. JSON Formatting Node: WriteJSONflex Generates JSON structures required for Line\u2019s Flex Messages, enabling carousel displays of yoga pose images and links. Node: Fix JSON Ensures all JSON outputs are correctly formatted before being sent via Line. Message Delivery Node: Line Push with Flex Bubble Sends the final message, including both text and Flex Message carousels, directly to users via Line Push Messages. Logging Interactions Nodes: YogaLog & YogaLog2 Logs each interaction back into Google Sheets to track which poses were sent and how often they appear, refining future recommendations. Setup Prerequisites Google Sheets Account : Set up a Google Sheet with the required structure and populate it with your yoga pose data. Line Developer Account : Create a Line channel to obtain necessary credentials for sending push messages. Azure OpenAI Account : Configure access to Azure OpenAI services for generating and formatting content. Intended Audience This workflow is ideal for: Yoga Instructors : Seeking to engage students with daily pose suggestions. Fitness Enthusiasts : Looking to maintain consistency in their yoga practice. Content Creators : Interested in automating personalized and visually appealing content distribution. lin@davoy.tech | 9 days ago | Free +3", "completion": "{\"id\":\"2DzQ1FH11S3Gp6wn\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"YogiAI\",\"tags\":[],\"nodes\":[{\"id\":\"2afc390e-d774-4db4-a52f-138f13837646\",\"name\":\"Azure OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatAzureOpenAi\",\"position\":[1040,0],\"parameters\":{\"model\":\"4o\",\"options\":{\"temperature\":0.8}},\"credentials\":{\"azureOpenAiApi\":{\"id\":\"5AjoWhww5SQi2VXd\",\"name\":\"Azure Open AI account\"}},\"typeVersion\":1},{\"id\":\"529d9ed9-3ae5-41cb-983e-874aa37ee1c7\",\"name\":\"YogaLog\",\"type\":\"n8n-nodes-base.googleSheets\",\"position\":[2240,-100],\"parameters\":{\"columns\":{\"value\":{\"Date\":\"={{ $('Trigger 2130 YogaPosesToday').first().json.timestamp }}\",\"JSON\":\"={{ $('CombineAll').item.json.LineBody }}\",\"Text\":\"={{ $('RewritePosesToday').item.json.text }}\"},\"schema\":[{\"id\":\"Date\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"Date\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"Text\",\"type\":\"string\",\"display\":true,\"required\":false,\"displayName\":\"Text\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"JSON\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"JSON\",\"defaultMatch\":false,\"canBeUsedToMatch\":true}],\"mappingMode\":\"defineBelow\",\"matchingColumns\":[\"Date\"],\"attemptToConvertTypes\":false,\"convertFieldsToString\":false},\"options\":{},\"operation\":\"append\",\"sheetName\":{\"__rl\":true,\"mode\":\"list\",\"value\":325576327,\"cachedResultUrl"}
{"prompt": "Create an n8n workflow for: Generate & Upload an Audio Summary of a WordPress (or Woocommerce) Article This workflow automates the process of summarizing or transcribing a WordPress article, converting the text into speech using Eleven Labs API, and uploading the resulting MP3 file back to WordPress. How It Works Trigger \u2013 The workflow starts manually when the user clicks \u201cTest Workflow\u201d. Retrieve Article \u2013 It fetches a WordPress article based on a given post ID. Summarize or Transcribe \u2013 An LLM (GPT-4o-mini) generates either: \u2022 A summary of the article, or \u2022 A full transcription, depending on the chosen prompt. Generate Speech \u2013 The processed text (summary or transcription) is converted into an MP3 audio file using Eleven Labs API. Upload MP3 to WordPress \u2013 The generated MP3 file is uploaded to WordPress. Update WordPress Post \u2013 The article is updated with an embedded audio player, allowing users to listen to the summary or transcription. Set Up Steps WordPress API Credentials \u2022 Configure your WordPress API credentials in n8n. Eleven Labs API Key \u2022 Obtain an API Key from Eleven Labs and configure it in n8n. Choose Between Summary or Transcription \u2022 Modify the AI prompt to either generate a summary or keep the full transcription. Test the Workflow \u2022 Run the workflow and ensure the MP3 file is correctly generated and uploaded. \ud83d\udca1 Customization Options \u2022 Modify the AI prompt to switch between a summary and a transcription. \u2022 Change the voice model in Eleven Labs for different speech styles. \u2022 Adjust output format to higher/lower quality MP3. \ud83d\ude80 This automation improves content accessibility and engagement by allowing users to listen to a summarized or full version of the article. Phil | Inforeole phil | 5 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"c911aed9995230b93fd0d9bc41c258d697c2fe97a3bab8c02baf85963eeda618\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"468084ed-ce7d-45c5-bf27-ea9c91d5898a\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"fbde6cfe-9fac-46d2-958a-f42c9ef383a3\",\"name\":\"Retrieve WordPress Article\",\"type\":\"n8n-nodes-base.wordpress\",\"position\":[440,0],\"parameters\":{\"postId\":\"1032\",\"options\":{},\"operation\":\"get\"},\"credentials\":{\"wordpressApi\":{\"id\":\"T0ygUN7hNFQVztP2\",\"name\":\"Wordpress account 2\"}},\"typeVersion\":1},{\"id\":\"54241e39-7a5f-45f4-9dab-72b5424f4061\",\"name\":\"Generate Summary or Transcription\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[680,0],\"parameters\":{\"text\":\"={{ $json.content }}\",\"messages\":{\"messageValues\":[{\"message\":\"Summarize or transcribe this article, depending on the workflow setting.\"}]},\"promptType\":\"define\",\"hasOutputParser\":true},\"typeVersion\":1.5},{\"id\":\"49cfaab6-a0c1-4319-904d-c1e0a2c6aa91\",\"name\":\"Generate Speech\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1120,0],\"parameters\":{\"url\":\"https://api.elevenlabs.io/v1/text-to-speech/voice_id\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"authentication\":\"genericCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"text\",\"value\":\"={{ $json.text }}\"},{\"name\":\"model_id\",\"value\":\"eleven_multilingual_v2\"},{\"name\":\"output_format\",\"value\":\"mp3"}
{"prompt": "Create an n8n workflow for: Create a Branded AI-Powered Website Chatbot Create a Branded AI Website Chatbot Engage website visitors with an intelligent chat widget powered by OpenAI. This template includes: \ud83d\udcac Natural conversation handling \ud83d\udcc5 Microsoft Outlook calendar integration \ud83d\udcdd Lead capture and information gathering \ud83d\udd04 Human handoff capabilities Simply add a JavaScript snippet to your website and configure the workflow to match your needs. Follow our detailed setup guide to get started in minutes. &gt; Note: Widget includes a \"Powered By\" affiliate link Wayne Simpson | 1 month ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"67d4d33d8b0ad4e5e12f051d8ad92fc35893d7f48d7f801bc6da4f39967b3592\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"22c8d63b-ce3c-4aab-b3f6-4bae8c1b9ec5\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1460,880],\"parameters\":{\"sessionKey\":\"={{ $json.sessionId }}\",\"sessionIdType\":\"customKey\",\"contextWindowLength\":20},\"typeVersion\":1.2},{\"id\":\"45403d5c-6e85-424f-b40b-c6214b57457b\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1880,580],\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"1111262a-1743-4bae-abf1-f69d2e1a580c\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1360,760],\"parameters\":{\"model\":\"gpt-4o-2024-08-06\",\"options\":{\"temperature\":0.4}},\"credentials\":{\"openAiApi\":{\"id\":\"XWFTuTtx9oWglhNn\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"df891547-c715-4dc6-bfcc-c0ac5cfcaf02\",\"name\":\"Make Appointment\",\"type\":\"@n8n/n8n-nodes-langchain.toolHttpRequest\",\"position\":[1820,840],\"parameters\":{\"url\":\"https://graph.microsoft.com/v1.0/me/events\",\"method\":\"POST\",\"jsonBody\":\"{\\n \\\"subject\\\": \\\"Meetings with <name> at <company>\\\",\\n \\\"start\\\": {\\n \\\"dateTime\\\": \\\"{dateStartTime}\\\",\\n \\\"timeZone\\\": \\\"Europe/London\\\"\\n },\\n \\\""}
{"prompt": "Create an n8n workflow for: Automatically Create YouTube Metadata with AI This n8n workflow automates YouTube video metadata generation using AI. It extracts video transcripts, analyzes content, and produces optimized titles, descriptions, tags, hashtags, and call-to-action elements. Additionally, the workflow integrates affiliate and promotional links to enhance overall video performance. Key Features Automated Metadata Generation Utilizes an AI agent integrated with OpenAI GPT-4 to generate engaging metadata based on the provided video transcript. SEO and Engagement Optimization Creates keyword-rich, well-structured content that boosts search engine visibility and audience engagement. Affiliate and Promotional Integration Retrieves pre-set promotional and affiliate links using a Google Docs integration. Direct YouTube Update Automatically updates video details on YouTube via the YouTube API. Customization Allows you to modify the AI prompt to tailor metadata for your specific niche. Workflow Breakdown User Submission Users supply the YouTube video link, transcript, and optionally, focus keywords. Video ID Extraction The workflow converts the YouTube URL into a video ID to streamline automation. Link Retrieval Affiliate and course links are fetched from a designated Google Docs file. AI-Powered Metadata Generation The AI agent generates the video title, description, tags, hashtags, and call-to-action elements. Metadata Formatting and Update The generated metadata is structured and directly updated on YouTube. Confirmation A success message is displayed upon completion of the update process. Setup and Configuration Deploying the Workflow Deploy the workflow in n8n and ensure all integrations are properly set up. Configuring Integrations Google Docs:** Configure credentials to retrieve affiliate and promotional links. OpenAI (GPT-4):** Set up credentials for AI-powered metadata generation. YouTube API:** Enter your API credentials to enable automatic video updates. User Input Requirements Provide a valid YouTube video link and its corresponding transcript. Optionally, include focus keywords to further enhance metadata accuracy. Ideal For YouTube Content Creators:** Automate video descriptions and boost SEO. Digital Marketers:** Enhance content for improved search rankings and audience engagement. Affiliate Marketers:** Simplify the insertion of promotional and affiliate links. AI & Automation Enthusiasts:** Explore the integration of AI into automated workflows. Additional Resources For further guidance, refer to the tutorial video on this workflow. More courses and resources are available on the SyncBricks website. For support or inquiries, contact Amjid Ali at info@syncbricks.com. You can also support this work via PayPal donations and subscribe for additional AI and automation workflows. Watch the Tutorial:** YouTube Video on This Workflow More Courses & Resources:** SyncBricks LMS Full Course on ERPNext & AI Automation Connect:** Email: info@syncbricks.com Website: SyncBricks YouTube: SyncBricks Channel LinkedIn: Amjid Ali Support & Subscribe:** Donate via PayPal Subscribe for More AI & Automation Workflows Amjid Ali | 10 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"dd130a849d7b29e5541b05d2f7f86a4acd4f1ec598c1c9438783f56bc4f0ff80\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"07433551-9fa9-421c-a0bf-721fa1624304\",\"name\":\"Sticky Note11\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1380,-320],\"parameters\":{\"color\":4,\"width\":1075,\"height\":736,\"content\":\"## Developed by Amjid Ali\\n\\nThank you for using this workflow template. It has taken me countless hours of hard work, research, and dedication to develop, and I sincerely hope it adds value to your work.\\n\\nIf you find this template helpful, I kindly ask you to consider supporting my efforts. Your support will help me continue improving and creating more valuable resources.\\n\\nYou can contribute via PayPal here:\\n\\nhttp://paypal.me/pmptraining\\n\\nFor Full Course about ERPNext or Automation using AI follow below link\\n\\nhttp://lms.syncbricks.com\\n\\nAdditionally, when sharing this template, I would greatly appreciate it if you include my original information to ensure proper credit is given.\\n\\nThank you for your generosity and support!\\nEmail : amjid@amjidali.com\\nhttps://linkedin.com/in/amjidali\\nhttps://syncbricks.com\\nhttps://youtube.com/@syncbricks\"},\"typeVersion\":1},{\"id\":\"31a0c5e9-c6f6-4921-8f92-be84cc669869\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-220,-80],\"webhookId\":\"syncbricks-youtube-meta-automation\",\"parameters\":{\"options\":{\"buttonLabel\":\"Update Youtube Video\"},\"formTitle\":\"Syncbricks Youtube\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Youtube Video Link\",\"requiredField\":true},{\"fieldLabel\":\"Video Transcript\",\"requiredField\":true},{\"fieldLabel\":\"Focus Keywords\",\"placeholder\":\"Focus Keywords (Optional)\"}]},\"form"}
{"prompt": "Create an n8n workflow for: Effortless Email Management with AI-Powered Summarization & Review How it Works This workflow automates the handling of incoming emails, summarizes their content, generates appropriate responses using a retrieval-augmented generation (RAG) approach, and obtains approval or suggestions before sending replies. Below is an explanation of its functionality divided into two main sections: Email Handling and Summarization: The process begins with the Email Trigger (IMAP) node which listens for new emails in a specified inbox. Once an email is received, the Markdown node converts its HTML content into plain text if necessary, followed by the Email Summarization Chain that uses AI to create a concise summary of up to 100 words. Response Generation and Approval: A Write email node generates a professional response based on the summarized content, ensuring brevity and professionalism while keeping within the word limit. Before sending out any automated replies, the system sends these drafts via Gmail for human review and approval through the Gmail node configured with free-text response options. If approved, the finalized email is sent back to the original sender using the Send Email node; otherwise, it loops back for further edits or manual intervention. Additionally, there's a Text Classifier node designed to categorize feedback from humans as either \"Approved\" or \"Declined\", guiding whether the email should proceed directly to being sent or require additional editing. Set Up Steps To replicate this workflow within your own n8n environment, follow these essential configuration steps: Configuration: Begin by setting up an n8n instance either locally or via cloud services offered directly from their official site. Import the provided JSON configuration file into your workspace, making sure all required credentials such as IMAP, SMTP, OpenAI API keys, etc., are properly set up under Credentials since multiple nodes rely heavily on external integrations for functionalities like reading emails, generating summaries, crafting replies, and managing approvals. Customization: Adjust parameters according to specific business needs, including but not limited to adjusting the conditions used during conditional checks performed by nodes like Approve?. Modify the template messages given to AI models so they align closely with organizational tone & style preferences while maintaining professionalism expected in business communications. Ensure correct mappings between fields when appending data to external systems where records might need tracking post-interaction completion, such as Google Sheets or similar platforms. n3w Italia | 25 days ago | Free +7", "completion": "{\"id\":\"nkPjDxMrrkKbgHaV\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Effortless Email Management with AI\",\"tags\":[],\"nodes\":[{\"id\":\"9d77e26f-de2b-4bd4-b0f0-9924a8f459a6\",\"name\":\"Email Trigger (IMAP)\",\"type\":\"n8n-nodes-base.emailReadImap\",\"position\":[-2000,-180],\"parameters\":{\"options\":{}},\"credentials\":{\"imap\":{\"id\":\"k31W9oGddl9pMDy4\",\"name\":\"IMAP info@n3witalia.com\"}},\"typeVersion\":2},{\"id\":\"cf2d020b-b125-4a20-8694-8ed0f7acf755\",\"name\":\"Markdown\",\"type\":\"n8n-nodes-base.markdown\",\"position\":[-1740,-180],\"parameters\":{\"html\":\"={{ $json.textHtml }}\",\"options\":{}},\"typeVersion\":1},{\"id\":\"41bfceff-0155-4643-be60-ee301e2d69e1\",\"name\":\"Send Email\",\"type\":\"n8n-nodes-base.emailSend\",\"position\":[400,-320],\"webhookId\":\"a79ae1b4-648c-4cb4-b6cd-04ea3c1d9314\",\"parameters\":{\"html\":\"={{ $('Edit Fields').item.json.email }}\",\"options\":{},\"subject\":\"=Re: {{ $('Email Trigger (IMAP)').item.json.subject }}\",\"toEmail\":\"={{ $('Email Trigger (IMAP)').item.json.from }}\",\"fromEmail\":\"={{ $('Email Trigger (IMAP)').item.json.to }}\"},\"credentials\":{\"smtp\":{\"id\":\"hRjP3XbDiIQqvi7x\",\"name\":\"SMTP info@n3witalia.com\"}},\"typeVersion\":2.1},{\"id\":\"2aff581a-8b64-405c-b62f-74bf189fd"}
{"prompt": "Create an n8n workflow for: AI-Powered Gratitude Reminder Workflow for LINE This workflow template, \"Daily Gratitude Reminder Bot for LINE,\" is designed to help users cultivate a habit of gratitude by sending personalized, AI-generated reminders every evening at 9:00 PM. Using Azure OpenAI , the bot generates varied and engaging messages to prompt users to reflect on the positive aspects of their day. The reminders are then sent directly to users via the LINE messaging platform , ensuring a seamless and impactful experience. Whether you're a developer, counselor, or business owner, this template offers a customizable and scalable solution for promoting mental wellness and fostering a culture of gratitude. profile Qwen2.5-Max 2:56 pm Here\u2019s a polished and detailed description for your \"Daily Gratitude Reminder Bot for LINE\" template, following the submission guidelines provided. This description is structured to meet all the requirements, including SEO optimization, clarity, and user convenience. Daily Gratitude Reminder Bot for LINE This workflow template, \"Daily Gratitude Reminder Bot for LINE,\" is designed to help users cultivate a habit of gratitude by sending personalized, AI-generated reminders every evening at 9:00 PM. Using Azure OpenAI , the bot generates varied and engaging messages to prompt users to reflect on the positive aspects of their day. The reminders are then sent directly to users via the LINE messaging platform , ensuring a seamless and impactful experience. Whether you're a developer, counselor, or business owner, this template offers a customizable and scalable solution for promoting mental wellness and fostering a culture of gratitude. Who Is This Template For? Developers who want to integrate AI-powered workflows into messaging platforms like LINE. Counselors & Therapists looking to encourage mindfulness and emotional well-being among their clients. Businesses & Organizations focused on employee wellness or customer engagement through positive reinforcement. Educators & Nonprofits seeking tools to promote mental health awareness and self-care practices. What Problem Does This Workflow Solve? Gratitude journaling has been proven to improve mental health, reduce stress, and increase overall happiness. However, many people struggle to maintain the habit due to busy schedules or forgetfulness. This workflow solves that problem by automating daily reminders to reflect on positive experiences, making it easier for users to build and sustain a gratitude practice. What This Workflow Does Scheduled Trigger: The workflow is triggered every evening at 9:00 PM using a schedule node. AI-Powered Message Generation: An Azure OpenAI Chat Model generates a unique and engaging reminder message with a temperature setting of 0.9 to ensure variety and creativity. Message Formatting: The generated message is reformatted to comply with the LINE Push API requirements, ensuring smooth delivery. Push Notification via LINE: The formatted message is sent to the user via the LINE Push API , delivering the reminder directly to their chat. Setup Guide Pre-Requisites Access to an Azure OpenAI account with credentials. A LINE Developers Console account with access to the Push API. Basic knowledge of n8n workflows and JSON formatting. How to Customize This Workflow to Your Needs Change the Time: Adjust the schedule trigger to send reminders at a different time. Modify the Prompt: Edit the AI model's input prompt to generate messages tailored to your audience (e.g., focus on work achievements or personal growth). Expand Recipients: Update the LINE Push API node to send reminders to multiple users or groups. Integrate Additional Features: Add nodes to log user responses or track engagement metrics. Why Use This Template? Promotes Mental Wellness: Encourages users to reflect on positive experiences, improving emotional well-being. Highly Customizable: Easily adapt the workflow to suit different audiences and use cases. Scalable: Send reminders to one user or thousands, making it suitable for both personal and organizational use. AI-Powered Creativity: Avoid repetitive messages by leveraging AI to generate fresh and engaging content. lin@davoy.tech | 4 days ago | Free", "completion": "{\"id\":\"Sebvr1R2t4zkAg1V\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Gratitude Jar Reminder\",\"tags\":[],\"nodes\":[{\"id\":\"ac48becc-e207-489b-a8e4-a8f69780c626\",\"name\":\"Trigger 2100 Bear Gratitude Jar Notice\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[-80,-100],\"parameters\":{\"rule\":{\"interval\":[{\"triggerAtHour\":21}]}},\"typeVersion\":1.2},{\"id\":\"37f46ac1-5c0b-4cdf-aa33-67fad80dafdd\",\"name\":\"WriteReminder\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[180,-100],\"parameters\":{\"text\":\"=Today is a wonderful day! \ud83c\udf1f What or who brought a smile to your face today? \ud83d\ude0a\\n\",\"messages\":{\"messageValues\":[{\"message\":\"You'll rewrite this message to send reminder to user to record good thing today.\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"816f8089-a54f-4860-a658-448ab53a08fd\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-180,-240],\"parameters\":{\"width\":300,\"height\":360,\"content\":\"## Trigger \\nWe schedule the trigger at 9.00 pm before going to bed. This flow is to reflect what is the great thing that happened today.\"},\"typeVersion\":1},{\"id\":\"c7a620fe-2a50-4cfb-af91-8a4b4ca58adb\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[160,-240],\"parameters\":{\"color\":5,\"width\":300,\"height\":360,\"content\":\"## Write Reminder\\nAfter getting the same reminder, we tend to ignore it. This is to generate variations of reminder by setting the temperature of the model at 0"}
{"prompt": "Create an n8n workflow for: Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI The provided workflow in n8n is designed to create a Business WhatsApp AI RAG (Retrieval-Augmented Generation) Chatbot. How it works: Webhook Setup: The workflow begins by setting up webhooks for verification and response. The Verify webhook receives GET requests and sends back a verification code, while the Respond webhook handles incoming POST requests from Meta regarding WhatsApp messages. Message Handling: Once a message is received, the workflow checks if the incoming JSON contains a user message. If it does, the message is processed further; otherwise, a generic response is sent. AI Agent Interaction: The user's message is passed to the AI Agent node, which uses a conversational agent with a predefined system message tailored for an electronics store. This ensures that the AI provides accurate and professional responses based on the knowledge base. Knowledge Base Utilization: The AI Agent references a knowledge base stored in Qdrant, a vector database. Documents from Google Drive are downloaded, vectorized using OpenAI embeddings, and stored in Qdrant for retrieval during conversations. Response Generation: The AI Agent generates a response using the OpenAI chat model (gpt-4o-mini) and sends it back to the user via WhatsApp. Set up steps: Create Qdrant Collection: Update the QDRANTURL and COLLECTION variables in the workflow. Use the Create collection HTTP request node to initialize the collection in Qdrant. Vectorize Documents: Configure the Get folder and Download Files nodes to fetch documents from a specified Google Drive folder. Use the Embeddings OpenAI node to generate embeddings for the downloaded files. Store the vectorized documents in Qdrant using the Qdrant Vector Store node. Configure Webhooks: Ensure both Verify and Respond webhooks have the same URL. Set the Verify webhook to use the GET HTTP method and the Respond webhook to use the POST HTTP method. Set Up AI Agent: Define the system prompt for the AI Agent, specifying guidelines for product information, technical support, customer service, and knowledge base usage. Link the AI Agent to the OpenAI chat model and configure any additional tools as needed. Test Workflow: Trigger the workflow manually using the When clicking \u2018Test workflow\u2019 node to ensure all components are functioning correctly. Monitor the flow of data through the nodes and verify that responses are being generated and sent accurately. By following these steps, the workflow will be fully operational, enabling a robust AI-powered chatbot capable of handling customer inquiries via WhatsApp. n3w Italia | 28 days ago | Free +5", "completion": "{\"id\":\"APCp9GPNjUSFPSfJ\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Business WhatsApp AI RAG Chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"2c5b2dd1-c63f-4bc9-909e-5f4b2a385d01\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1020,1040],\"parameters\":{\"options\":{},\"respondWith\":\"text\",\"responseBody\":\"={{ $json.query['hub.challenge'] }}\"},\"typeVersion\":1.1},{\"id\":\"cc230fcd-f88c-40d4-8835-ac9dc6228b18\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[1560,1380],\"parameters\":{\"text\":\"={{ $('Respond').item.json.body.entry[0].changes[0].value.messages[0].text.body }}\",\"agent\":\"conversationalAgent\",\"options\":{\"systemMessage\":\"You are an AI-powered assistant for an electronics store. Your primary goal is to assist customers by providing accurate and helpful information about products, troubleshooting tips, and general support. Use the provided knowledge base (retrieved documents) to answer questions with precision and professionalism.\\n\\n**Guidelines**:\\n1. **Product Information**:\\n - Provide detailed descriptions of products, including specifications, features, and compatibility.\\n - Highlight key selling points and differences between similar products.\\n - Mention availability, pricing, and promotions if applicable.\\n\\n2. **Technical Support**:\\n - Offer step-by-step troubleshooting guides for common issues.\\n - Suggest solutions for setup, installation, or configuration problems.\\n - If the issue is complex, recommend contacting the store\u2019s support team for further assistance.\\n\\n3. **Customer Service**:\\n - Respond politely and professionally to all inquiries.\\n - If a question is unclear, ask for clarification to provide the best possible answer.\\n - For order-"}
{"prompt": "Create an n8n workflow for: \ud83d\udd25\ud83d\udcc8\ud83e\udd16 AI Agent for n8n Creators Leaderboard - Find Popular Workflows n8n Creators Leaderboard Workflow Why Use This Workflow? The n8n Creators Leaderboard Workflow is a powerful tool for analyzing and presenting detailed statistics about workflow creators and their contributions within the n8n community. It provides users with actionable insights into popular workflows, community trends, and top contributors, all while automating the process of data retrieval and report generation. Benefits Discover Popular Workflows**: Identify workflows with the most unique visitors and inserters (weekly and monthly). Understand Community Trends**: Gain insights into what workflows are resonating with the community. Recognize Top Contributors**: Highlight impactful creators to foster collaboration and inspiration. Save Time with Automation**: Automates data fetching, processing, and reporting for efficiency. Use Cases For Workflow Creators**: Track performance metrics of your workflows to optimize them for better engagement. For Community Managers**: Identify trends and recognize top contributors to improve community resources. For New Users**: Explore popular workflows as inspiration for building your own automations. How It Works This workflow aggregates data from GitHub repositories containing statistics about workflow creators and their templates. It processes this data, filters it based on user input, and generates a detailed Markdown report using an AI agent. Key Features Data Aggregation: Fetches creator and workflow statistics from GitHub JSON files. Custom Filtering: Focuses on specific creators based on a username provided via chat. AI-Powered Reports: Generates comprehensive Markdown reports with summaries, tables, and insights. Output Flexibility: Saves reports locally with timestamps for easy access. Data Retrieval & Processing Creators Data**: Retrieved via an HTTP Request node from a JSON file containing aggregated statistics about creators. Workflows Data**: Pulled from another JSON file with workflow metrics like visitor counts and inserter statistics. Data Merging**: Combines creator and workflow data by matching usernames to provide enriched statistics. Report Generation The AI agent generates a Markdown report that includes: A summary of the creator\u2019s contributions. A table of workflows with key metrics (e.g., unique visitors, inserters). Insights into trends or community feedback. The report is saved locally as a file with a timestamp for tracking purposes. Quick Start Guide Prerequisites Ensure your n8n instance is running. Verify that the GitHub base URL and file variables are correctly set in the Global Variables node. Confirm that your OpenAI credentials are configured for the AI Agent node. How to Start Activate the Workflow: Make sure the workflow is active in your n8n environment. Trigger via Chat: Use the Chat Trigger node to initiate the workflow by sending a message like: show me stats for username [desired_username] Replace [desired_username] with the username you want to analyze. Processing & Report Generation: The workflow fetches data, processes it, and generates a Markdown report. View Output: The final report is saved locally as a file (with a timestamp), which you can review to explore leaderboard insights. Joseph LePage | 13 days ago | Free", "completion": "{\"id\":\"b8a4IwiwD9SlgF42\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83d\udd25\ud83d\udcc8\ud83e\udd16 AI Agent for n8n Creators Leaderboard - Find Popular Workflows\",\"tags\":[],\"nodes\":[{\"id\":\"fcda047d-b609-4791-b3ae-f359d0c6a071\",\"name\":\"stats_aggregate_creators\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1240,1280],\"parameters\":{\"url\":\"={{ $json.path }}{{ $json['creators-filename'] }}.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"fa1f51fd-6019-4d47-b17e-8c5621e6ab4c\",\"name\":\"stats_aggregate_workflows\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1240,1500],\"parameters\":{\"url\":\"={{ $json.path }}{{ $json['workflows-filename'] }}.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"34c2d0d3-0474-4a69-b1a5-14c9021865cd\",\"name\":\"Global Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[-1660,1480],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"4bcb91c6-d250-4cb4-8ee1-022df13550e1\",\"name\":\"path\",\"type\":\"string\",\"value\":\"https://raw.githubusercontent.com/teds-tech-talks/n8n-community-leaderboard/refs/heads/main/\"},{\"id\":\"a910a798-0bfe-41b1-a4f1-41390c7f6997\",\"name\":\"workflows-filename\",\"type\":\"string\",\"value\":\"=stats_aggregate_workflows\"},{\"id\":\"e977e816-dc1e-43ce-9393-"}
{"prompt": "Create an n8n workflow for: \ud83c\udf10\ud83e\ude9b AI Agent Chatbot with Jina.ai Webpage Scraper The \ud83c\udf10\ud83e\udd16 AI Agent Chatbot with Jina.ai Webpage Scraper workflow is a powerful automation designed to integrate real-time web scraping capabilities into an AI-driven chatbot. Here's how it works and why it's important: How It Works \ud83d\udcac Chat Trigger: The workflow begins when a user sends a chat message, triggering the \"When chat message received\" node. \ud83e\udde0 AI Agent Processing: The input is passed to the \"Jina.ai Web Scraping Agent,\" which uses advanced AI logic to interpret the user\u2019s query and determine the information needed. \ud83c\udf10 Web Scraping: The agent utilizes the \"HTTP Request\" node to scrape real-time data from a user-provided URL, enabling the chatbot to fetch and analyze live website content. \ud83d\uddc2\ufe0f Memory Management: The \"Window Buffer Memory\" node ensures context retention by storing and managing conversational history, allowing for seamless interactions. \ud83e\udd16 Language Model Integration: The scraped data is processed using the \"gpt-4o-mini\" language model, which generates clear, accurate, and contextually relevant responses for the user. Why It's Cool \u23f1\ufe0f Real-Time Information Retrieval**: This workflow empowers users to access up-to-date web content directly through a chatbot, eliminating manual web searches. \u2728 Enhanced User Experience**: By combining web scraping with conversational AI, it delivers precise answers tailored to user queries in real time. \ud83d\udd04 Versatility**: It can be applied across various domains, such as customer support, research, or data analysis, making it a valuable tool for businesses and individuals alike. \u2699\ufe0f Automation Efficiency**: Automating web scraping and response generation saves time and effort while ensuring accuracy. Joseph LePage | 13 days ago | Free", "completion": "{\"id\":\"xEij0kj2I1DHbL3I\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83c\udf10\ud83e\ude9b AI Agent Chatbot with Jina.ai Webpage Scraper\",\"tags\":[],\"nodes\":[{\"id\":\"ea5369a0-4283-46fc-b738-8cf787181e93\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[0,-280],\"webhookId\":\"e298fd8c-2af9-4db2-bb8b-94d70fbc2938\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"07c8338b-d47e-467b-996f-99c9fbe67f89\",\"name\":\"Sticky Note14\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[240,-460],\"parameters\":{\"color\":5,\"width\":680,\"height\":700,\"content\":\"## AI Agent Chatbot with Jina.ai Web Scraper\\n### https://jina.ai/\\n\"},\"typeVersion\":1},{\"id\":\"00da1c9b-b5f7-42b8-8bdd-938a8daf7410\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[520,20],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"f14426ee-709d-4651-a0b7-e823bff5ee74\",\"name\":\"Jina.ai Web Scraping Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[440,-280],\"parameters\":{\"text\":\"=You have access to a powerful scrape_website tool that can retrieve real-time web content. Use this tool to extract any needed information from the website, analyze the data, and craft a clear, accurate, and concise answer to the user's question. Be sure to include relevant details from the scraped content. \\n\\nUser Question: {{"}
{"prompt": "Create an n8n workflow for: \ud83d\udcc4\u2728 Easy WordPress Content Creation from PDF Docs + Human in the Loop Gmail \ud83d\udcc4\u2728 Easy WordPress Content Creation from PDF Docs + Human in the Loop Gmail This n8n workflow automates the process of transforming PDF documents into engaging, SEO-friendly WordPress blog posts. It incorporates AI-powered text analysis, automatic image generation, and a human review step to ensure quality before publishing. \ud83d\ude80 How It Works \ud83d\uddc2\ufe0f PDF Upload & Text Extraction Users upload a PDF document through a form trigger. The workflow extracts text from the uploaded file, ensuring compatibility with supported formats. \ud83e\udd16 AI-Powered Blog Post Generation The extracted text is analyzed by an AI model (GPT-based) to create a structured blog post. The AI generates: A captivating SEO-friendly title. Well-formatted HTML content, including an introduction, chapters with subheadings, and a conclusion. \ud83c\udfa8 Image Creation & Integration An image is generated using Pollinations.ai based on the blog post title. The vibrant image is uploaded to WordPress and set as the featured image for the post. \ud83d\udcdd WordPress Draft Creation A draft blog post is created on WordPress with the AI-generated title, content, and featured image. \u2705 Human-in-the-Loop Approval The draft content is sent via Gmail to a reviewer for manual approval. If approved, the post is published on WordPress. If not, an error message is sent for troubleshooting. \ud83d\udce2 Multi-Channel Notifications Once published, notifications are sent via Gmail and Telegram to relevant stakeholders. \ud83d\udd27 Setup Steps \ud83d\udd11 Configure API Credentials Set up API connections for: OpenAI (for AI content generation). WordPress (for post creation and media uploads). Gmail (for sending approval emails). Telegram (for notifications). imgbb (for saving blog image). \u2699\ufe0f Customize Workflow Parameters Adjust the AI prompt to match your desired blog structure and tone. Modify the image generation parameters to align with your branding needs. \ud83e\uddea Test & Deploy Test the workflow with sample PDFs to ensure: Accurate text extraction. Proper formatting of generated content. Seamless approval and publishing processes. This workflow streamlines content creation while maintaining quality control through human oversight, making it an ideal solution for efficient blog management! \ud83c\udf89 Joseph LePage | 7 days ago | Free +2", "completion": "{\"id\":\"fSG22q8TeUtsGUGD\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83d\udcc4\u2728 Easy Wordpress Content Creation from PDF Document + Human In The Loop with Gmail Approval\",\"tags\":[],\"nodes\":[{\"id\":\"77d8c73c-1cdd-4795-841c-29c3b85040e0\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1140,-860],\"parameters\":{\"color\":4,\"width\":461,\"height\":319,\"content\":\"## Upload PDF and Extract Text\"},\"typeVersion\":1},{\"id\":\"62dc4474-1803-4b6e-8fe5-530e9baf80f7\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1640,-860],\"parameters\":{\"color\":5,\"width\":649,\"height\":452,\"content\":\"## Create Blog Post\"},\"typeVersion\":1},{\"id\":\"b8da4206-f1b5-4a8f-b056-70c7558c825d\",\"name\":\"Upload PDF\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[1220,-760],\"webhookId\":\"6c4a4180-7206-469f-a645-f41824ccbf42\",\"parameters\":{\"path\":\"pdf\",\"options\":{},\"formTitle\":\"PDF2Blog\",\"formFields\":{\"values\":[{\"fieldType\":\"file\",\"fieldLabel\":\"Upload PDF File\",\"multipleFiles\":false,\"requiredField\":true,\"acceptFileTypes\":\".pdf\"}]},\"formDescription\":\"Transform PDFs into captivating blog posts\"},\"typeVersion\":2.1},{\"id\":\"ef70bbe2-d66c-4c91-96cf-2d0500522e70\",\"name\":\"Extract Text\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[1420,-760],\"parameters\":{\"options\":{},\"operation\":\"pdf\",\"binaryPropertyName\":\"Upload_PDF_File\"},\"typeVersion\":1},{\"id\":\"5de90"}
{"prompt": "Create an n8n workflow for: RAG Chatbot for Company Documents using Google Drive and Gemini This workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in Google Drive. It automatically indexes new or updated documents in a Pinecone vector database, allowing the chatbot to provide accurate and up-to-date information. The workflow uses Google's Gemini AI for both embeddings and response generation. How it works The workflow uses two Google Drive Trigger nodes: one for detecting new files added to a specified Google Drive folder, and another for detecting file updates in that same folder. Automated Indexing: When a new or updated document is detected The Google Drive node downloads the file. The Default Data Loader node loads the document content. The Recursive Character Text Splitter node breaks the document into smaller text chunks. The Embeddings Google Gemini node generates embeddings for each text chunk using the text-embedding-004 model. The Pinecone Vector Store node indexes the text chunks and their embeddings in a specified Pinecone index. 7.The Chat Trigger node receives user questions through a chat interface. The user's question is passed to an AI Agent node. The AI Agent node uses a Vector Store Tool node, linked to a Pinecone Vector Store node in query mode, to retrieve relevant text chunks from Pinecone based on the user's question. The AI Agent sends the retrieved information and the user's question to the Google Gemini Chat Model (gemini-pro). The Google Gemini Chat Model generates a comprehensive and informative answer based on the retrieved documents. A Window Buffer Memory node connected to the AI Agent provides short-term memory, allowing for more natural and context-aware conversations. Set up steps Google Cloud Project and Vertex AI API: Create a Google Cloud project. Enable the Vertex AI API for your project. Google AI API Key: Obtain a Google AI API key from Google AI Studio. Pinecone Account: Create a free account on the Pinecone website. Obtain your API key from your Pinecone dashboard. Create an index named company-files in your Pinecone project. Google Drive: Create a dedicated folder in your Google Drive where company documents will be stored. Credentials in n8n: Configure credentials in your n8n environment for: Google Drive OAuth2 Google Gemini(PaLM) Api (using your Google AI API key) Pinecone API (using your Pinecone API key) Import the Workflow: Import this workflow into your n8n instance. Configure the Workflow: Update both Google Drive Trigger nodes to watch the specific folder you created in your Google Drive. Configure the Pinecone Vector Store nodes to use your company-files index. Mihai Farcas | 1 month ago | Free +4", "completion": "{\"id\":\"7cXvgkl9170QXzT2\",\"meta\":{\"instanceId\":\"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\",\"templateCredsSetupCompleted\":true},\"name\":\"RAG Workflow For Company Documents stored in Google Drive\",\"tags\":[],\"nodes\":[{\"id\":\"753455a3-ddc8-4a74-b043-70a0af38ff9e\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[680,0],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"company-files\",\"cachedResultName\":\"company-files\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"a7c8fa7f-cad2-4497-a295-30aa2e98cacc\",\"name\":\"Embeddings Google Gemini\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsGoogleGemini\",\"position\":[640,280],\"parameters\":{\"modelName\":\"models/text-embedding-004\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"jLOqyTR4yTT1nYKi\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"215f0519-4359-4e4b-a90c-7e54b1cc52b5\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[840,220],\"parameters\":{\"options\":{},\"dataType\":\"binary\",\"binaryMode\":\"specificField\"},\"typeVersion\":1},{\"id\":\"863d3d1d-1621-406e-8320-688f64b07b09\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[820,420],\""}
{"prompt": "Create an n8n workflow for: \ud83d\udc0bDeepSeek V3 Chat & R1 Reasoning Quick Start This n8n workflow demonstrates multiple ways to harness DeepSeek's AI models in your automation pipeline! \ud83c\udf1f Core Features Multiple Integration Methods \ud83d\udd0c Local deployment using Ollama for DeepSeek-R1 Direct API integration with DeepSeek Chat V3 Conversational agent with memory buffer HTTP request implementation with both raw and JSON formats Model Options \ud83e\udde0 DeepSeek Chat V3 for general conversation DeepSeek-R1 for advanced reasoning Memory-enabled agent for persistent context Quick Setup \ud83d\udee0\ufe0f API Configuration Base URL: https://api.deepseek.com Get your API key from platform.deepseek.com/api_keys Local Setup \ud83d\udcbb Install Ollama for local deployment Set up DeepSeek-R1 via Ollama Configure local credentials in n8n Implementation Details \ud83d\udd27 Conversational Agent Window Buffer Memory for context Customizable system messages Built-in error handling with retries API Endpoints \ud83c\udf10 Chat completions for V3 and R1 models OpenAI API format compatibles Joseph LePage | 1 month ago | Free", "completion": "{\"id\":\"IyhH1KHtXidKNSIA\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83d\udc0bDeepSeek V3 Chat & R1 Reasoning Quick Start\",\"tags\":[],\"nodes\":[{\"id\":\"54c59cae-fbd0-4f0d-b633-6304e6c66d89\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-840,-740],\"webhookId\":\"b740bd14-1b9e-4b1b-abd2-1ecf1184d53a\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"ef85680e-569f-4e74-a1b4-aae9923a0dcb\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"onError\":\"continueErrorOutput\",\"position\":[-320,40],\"parameters\":{\"agent\":\"conversationalAgent\",\"options\":{\"systemMessage\":\"You are a helpful assistant.\"}},\"retryOnFail\":true,\"typeVersion\":1.7,\"alwaysOutputData\":true},{\"id\":\"07a8c74c-768e-4b38-854f-251f2fe5b7bf\",\"name\":\"DeepSeek\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-360,220],\"parameters\":{\"model\":\"=deepseek-reasoner\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"MSl7SdcvZe0SqCYI\",\"name\":\"deepseek\"}},\"typeVersion\":1.1},{\"id\":\"a6d58a8c-2d16-4c91-adde-acac98868150\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[-220,220],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"401a5932-9f"}
{"prompt": "Create an n8n workflow for: A Very Simple \"Human in the Loop\" Email Response System Using AI and IMAP Functionality This workflow automates the handling of incoming emails by summarizing their content, generating appropriate responses, and validating the responses through a \"Human-in-the-Loop\" system. It integrates with IMAP email services (e.g., Gmail, Outlook) and uses AI models to streamline the email response process. The workflow ensures that all AI-generated responses are reviewed by a human before being sent, maintaining a high level of professionalism and accuracy. This approach is particularly useful for businesses that receive a high volume of emails and need to respond quickly while ensuring quality control. How It Works Email Trigger: The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new messages. When a new email arrives, it triggers the workflow. Email Preprocessing: The Markdown node converts the email's HTML content into plain text for easier processing by the AI models. Email Summarization: The Email Summarization Chain node uses an AI model (OpenAI) to generate a concise summary of the email. The summary is limited to 100 words and is written in a professional tone. Email Response Generation: The Write email node uses an AI model (OpenAI) to draft a professional response to the email. The response is based on the email content and is limited to 100 words. Human-in-the-Loop Approval: The Set Email text node prepares the drafted response for approval. The Approve Email node sends the drafted response to a human approver (e.g., an internal email address) for review. The email includes: The original message. The AI-generated response. The Approved? node checks if the response has been approved by the human reviewer. If approved, the workflow proceeds to send the response; otherwise, it stops. Sending the Response: The Send Email node sends the approved response back to the original sender. Key Features Automated Email Summarization**: Summarizes incoming emails to provide a quick overview of the content. AI-Powered Response Generation**: Drafts professional responses to emails using AI. Human-in-the-Loop Approval**: Ensures all AI-generated responses are reviewed and approved by a human before being sent. IMAP Integration**: Works with IMAP email services like Gmail and Outlook. Efficient Email Management**: Reduces the time and effort required to handle incoming emails while maintaining high-quality responses. This workflow is ideal for businesses looking to automate their email response process while maintaining control over the quality of outgoing communications. It leverages AI to handle repetitive tasks and ensures that all responses are reviewed by a human, providing a balance between automation and human oversight. n3w Italia | 17 days ago | Free", "completion": "{\"id\":\"Nvn78tMRNnKji7Fg\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Very simple Human in the loop system email with AI e IMAP\",\"tags\":[],\"nodes\":[{\"id\":\"271bb16f-9b62-41d9-ab76-114cd7ba915a\",\"name\":\"Email Trigger (IMAP)\",\"type\":\"n8n-nodes-base.emailReadImap\",\"position\":[-1300,1340],\"parameters\":{\"options\":{}},\"credentials\":{\"imap\":{\"id\":\"k31W9oGddl9pMDy4\",\"name\":\"IMAP info@n3witalia.com\"}},\"typeVersion\":2},{\"id\":\"42d150d8-d574-49f9-9c0e-71a2cdea3b79\",\"name\":\"Markdown\",\"type\":\"n8n-nodes-base.markdown\",\"position\":[-1040,1340],\"parameters\":{\"html\":\"={{ $json.textHtml }}\",\"options\":{}},\"typeVersion\":1},{\"id\":\"e9498a60-0078-4581-b269-7ff552f4047a\",\"name\":\"Send Email\",\"type\":\"n8n-nodes-base.emailSend\",\"position\":[920,1320],\"webhookId\":\"a79ae1b4-648c-4cb4-b6cd-04ea3c1d9314\",\"parameters\":{\"html\":\"={{ $('Set Email text').item.json.email }}\",\"options\":{},\"subject\":\"=Re: {{ $('Email Trigger (IMAP)').item.json.subject }}\",\"toEmail\":\"={{ $('Email Trigger (IMAP)').item.json.from }}\",\"fromEmail\":\"={{ $('Email Trigger (IMAP)').item.json.to }}\"},\"credentials\":{\"smtp\":{\"id\":\"hRjP3XbDiIQqvi7x\",\"name\":\"SMTP info@n3witalia.com\"}},\"typeVersion\":2.1},{\"id\":\"ab9f6ac3-2095-44df-"}
{"prompt": "Create an n8n workflow for: \ud83e\udd16 Telegram Messaging Agent for Text/Audio/Images \ud83e\udd16 This n8n workflow creates an intelligent Telegram bot that processes multiple types of messages and provides automated responses using AI capabilities. The bot serves as a personal assistant that can handle text, voice messages, and images through a sophisticated processing pipeline. Core Components Message Reception and Validation \ud83d\udce5 \ud83d\udd04 Implements webhook-based message reception for real-time processing \ud83d\udd10 Features a robust user validation system that verifies sender credentials \ud83d\udd00 Supports both testing and production webhook endpoints for development flexibility Message Processing Pipeline \u26a1 \ud83d\udd04 Uses a smart router to detect and categorize incoming message types \ud83d\udcdd Processes three main message formats: \ud83d\udcac Text messages \ud83c\udfa4 Voice recordings \ud83d\udcf8 Images with captions AI Integration \ud83e\udde0 \ud83e\udd16 Leverages OpenAI's GPT-4 for message classification and processing \ud83d\udde3\ufe0f Incorporates voice transcription capabilities for audio messages \ud83d\udc41\ufe0f Features image analysis using GPT-4 Vision API for processing visual content Technical Architecture Webhook Management \ud83d\udd0c \ud83c\udf10 Maintains separate endpoints for testing and production environments \ud83d\udcca Implements automatic webhook status monitoring \u26a1 Provides real-time webhook configuration updates Error Handling \u26a0\ufe0f \ud83d\udd0d Features comprehensive error detection and reporting \ud83d\udd04 Implements fallback mechanisms for unprocessable messages \ud83d\udcac Provides user feedback for failed operations Message Classification System \ud83d\udccb \ud83c\udff7\ufe0f Categorizes incoming messages into tasks and general conversation \ud83d\udd00 Implements separate processing paths for different message types \ud83e\udde9 Maintains context awareness across message processing Security Features User Authentication \ud83d\udd12 \u2705 Validates user credentials against predefined parameters \ud83d\udc64 Implements first name, last name, and user ID verification \ud83d\udeab Restricts access to authorized users only Response System Intelligent Responses \ud83d\udca1 \ud83e\udd16 Generates contextual responses based on message classification Joseph LePage | 1 month ago | Free", "completion": "{\"id\":\"8jDt77Y4FaV6ARYG\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83e\udd16 Telegram Messaging Agent for Text/Audio/Images\",\"tags\":[],\"nodes\":[{\"id\":\"1656be7a-7a27-47f3-b511-3634a65a97a2\",\"name\":\"Check User & Chat ID\",\"type\":\"n8n-nodes-base.if\",\"position\":[100,160],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"5fe3c0d8-bd61-4943-b152-9e6315134520\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.first_name }}\",\"rightValue\":\"={{ $json.first_name }}\"},{\"id\":\"98a0ea91-0567-459c-bbce-06abc14a49ce\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.last_name }}\",\"rightValue\":\"={{ $json.last_name }}\"},{\"id\":\"18a96c1f-f2a0-4a2a-b789-606763df4423\",\"operator\":{\"type\":\"number\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Listen for Telegram Events').item.json.body.message.from.id }}\",\"rightValue\":\"={{ $json.id }}\"}]},\"looseTypeValidation\":\"=\"},\"typeVersion\":2.2},{\"id\":\"73b0fedb-eb82-4464-a08f-397a3fe69480\",\"name\":\"Error message\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[320,440],\"parameters\":{\"text\":\"=Unable to process your message.\",\"chatId\":\"={{ $json."}
{"prompt": "Create an n8n workflow for: Turn BBC News Articles into Podcasts using Hugging Face and Google Gemini Turn BBC News Articles into Podcasts using Hugging Face and Google Gemini Effortlessly transform BBC news articles into engaging podcasts with this automated n8n workflow. Who is this for? This template is perfect for: Content creators** who want to quickly produce podcasts from current events. Students** looking for an efficient way to create audio content for projects or assignments. Individuals** interested in generating their own podcasts without technical expertise. Setup Information Install n8n: If you haven't already, download and install n8n from n8n.io. Import the Workflow: Copy the JSON code for this workflow and import it into your n8n instance. Configure Credentials: Gemini API: Set up your Gemini API credentials in the workflow's LLM nodes. Hugging Face Token: Obtain an access token from Hugging Face and add it to the HTTP Request node for the text-to-speech model. Customize (Optional): Filtering Criteria: Adjust the News Classifier node to fine-tune the selection of news articles based on your preferences. Output Options: Modify the workflow to save the generated audio file to a cloud storage service or publish it to a podcast hosting platform. Prerequisites An active n8n instance. Basic understanding of n8n workflows (no coding required). API credentials for Gemini and a Hugging Face account with an access token. What problem does it solve? This workflow eliminates the manual effort involved in creating podcasts from news articles. It automates the entire process, from fetching and filtering news to generating the final audio file. What are the benefits? Time-saving:** Create podcasts in minutes, not hours. Easy to use:** No coding or technical skills required. Customizable:** Adapt the workflow to your specific needs and preferences. Cost-effective:** Leverage free or low-cost services like Gemini and Hugging Face. How does it work? The workflow fetches news articles from the BBC website. It filters articles based on their suitability for a podcast. It extracts the full content of the selected articles. It uses Gemini LLM to create a podcast script. It converts the script to speech using Hugging Face's text-to-speech model. The final podcast audio is ready for use. Nodes in the Workflow Fetch BBC News Page: Retrieves the main BBC News page. News Classifier: Categorizes news articles using Gemini LLM. Fetch BBC News Detail: Extracts detailed content from suitable articles. Basic Podcast LLM Chain: Generates a podcast script using Gemini LLM. HTTP Request: Converts the script to speech using Hugging Face. Add Story I'm excited to share this workflow with the n8n community and help content creators and students easily produce engaging podcasts! Additional Tips Explore the n8n documentation and community resources for more advanced customization options. Experiment with different filtering criteria and LLM prompts to achieve your desired podcast style. Onur | 10 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"5287ddd2fa569cf8e4c5a724666246a45305c032a19bb677c9e4b963d365f84b\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"95c798a4-bc34-4219-b7c3-6b4a4070886b\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-320,1080],\"parameters\":{},\"typeVersion\":1},{\"id\":\"09987590-1ec2-48d4-aa04-32b85addd9e6\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[420,1080],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"newsTitle\"},\"typeVersion\":1},{\"id\":\"758e3f60-01dc-46c7-bb53-7460eaed92e3\",\"name\":\"Extract News Block\",\"type\":\"n8n-nodes-base.html\",\"position\":[220,1080],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"newsTitle\",\"cssSelector\":\".eGcloy\",\"returnArray\":true,\"returnValue\":\"html\"}]}},\"typeVersion\":1.2},{\"id\":\"20440f9a-a40c-4419-af6d-383de041d078\",\"name\":\"Extract News Content\",\"type\":\"n8n-nodes-base.html\",\"position\":[600,1080],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"dataPropertyName\":\"newsTitle\",\"extractionValues\":{\"values\":[{\"key\":\"title\",\"cssSelector\":\"h2\"},{\"key\":\"link\",\"attribute\":\"href\",\"cssSelector\":\"a\",\"returnValue\":\"attribute\"},{\"key\":\"description\",\"cssSelector\":\".kYtujW\"}]}},\"typeVersion\":1.2},{\"id\":\"47a50ada-127a-4037-8fe7-41c0caebb3de\",\"name\":\"Aggregate\",\"type\":\"n8n-nodes-base.aggregate\",\"position\":[2000,1060],\"parameters\":{\"options\":{},\"aggregate\":\"aggregateAllItemData\"},\"type"}
{"prompt": "Create an n8n workflow for: Line Chatbot Handling AI Responses with Groq and Llama3 Workflow overview: This workflow is designed for dynamic and intelligent conversational capabilities. It incorporates Meta's llama3.3-versatile model for personal assistant. There are no issues when sending simple text to the LINE reply API, so in this workflow you can see how to handle large and complex text sending from AI chat without any errors. Workflow description: User uses Line Messaging API to send message to the chatbot, create line business ID from here: Line Business Set the message from Step 1 to the proper value Send the message to process at Groq using API key that we have created from Groq Send the reply message from AI Agent back to Line Messaging API account Key Features: Utilizes Meta's llama 3.3 model for robust conversational capabilities Handles large and complex text interactions with ease, ensuring reliable connections to LINE Messaging API Demonstrates effective strategies for processing and responding to large and complex text inputs from AI chat To use this template, you need to be on n8n version 1.79.0 or later. Sirisak Chantanate | 9 days ago | Free", "completion": "{\"id\":\"xibc6WDU53isYN1o\",\"meta\":{\"instanceId\":\"b3225e6e1bdf5f128a5dd199e31e87e9e2b7cb5f141a1bbe60059a1964dd7091\",\"templateCredsSetupCompleted\":true},\"name\":\"Line Chatbot Handling AI Responses with Groq and Llama3\",\"tags\":[],\"nodes\":[{\"id\":\"9b936123-7f31-4ddc-b1ca-fd172da9c5a8\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[0,-200],\"parameters\":{\"width\":440,\"content\":\"## Line AI Chatbot with Groq \\nThis workflow automates the process of handling messages from Line Messaging API by sending message to Groq as your AI assistant and reply back to you. In this workflow, you can see that there is no JSON error when sending long and complex message.\"},\"typeVersion\":1},{\"id\":\"0d75416e-58f0-4411-8904-7051f0d1c06a\",\"name\":\"Line: Messaging API\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[0,0],\"webhookId\":\"befed026-573c-4d3a-9113-046ea8ae5930\",\"parameters\":{\"path\":\"befed026-573c-4d3a-9113-046ea8ae5930\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":2},{\"id\":\"e363c981-acdf-4048-a531-31808cd3edc5\",\"name\":\"Get Messages\",\"type\":\"n8n-nodes-base.set\",\"position\":[300,0],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"654c2465-5531-46fb-9b11-74cc23c899a9\",\"name\":\"body.events[0].message.text\",\"type\":\"string\",\"value\":\"={{ $json.body.events[0].message.text }}\"},{\"id\":\"be878a5c-f3e2-40c4-b8f2-6c6708b3b2ad\",\"name\":\"body.events"}
{"prompt": "Create an n8n workflow for: Automate WooCommerce SEO with Yoast & AI-Powered Meta Tag Generation for FREE This workflow is designed to automate the generation and updating of SEO meta titles and descriptions for WooCommerce products using n8n. It leverages Google Sheets for data input, a FREE language model (Gemini 2.0 Flash Exp. via OpenRouter) for generating SEO-optimized meta tags, and WooCommerce for updating product details. How It Works: Trigger: The workflow can be triggered manually or on a schedule. The manual trigger allows for testing, while the schedule trigger can be set to run at regular intervals (e.g., every few minutes) to process new products. Data Retrieval: The workflow starts by retrieving product IDs from a Google Sheets document. It looks for products that do not yet have meta titles or descriptions. Using the retrieved product ID, the workflow fetches the corresponding product details from WooCommerce, including the product name, description, short description, and categories. Meta Tag Generation: The product details are passed to a language model (Gemini 2.0 Flash Exp) via OpenRouter. The model generates SEO-optimized meta titles and descriptions based on the provided content. The generated meta tags are structured and validated to ensure they meet SEO best practices, such as character limits and keyword inclusion. Update WooCommerce: The generated meta title and description are then updated in the WooCommerce product metadata using the Yoast SEO fields. Update Google Sheets: Finally, the workflow updates the Google Sheets document with the newly generated meta tags, along with the product URL, title, and the timestamp of the update. Set Up Steps: Google Sheets Setup: Create a copy of the provided Google Sheets template and insert WooCommerce product IDs in column \"B\". Ensure the Google Sheets document has columns for METATITLE, METADESCRIPTION, URL, TITLE POST, and DATA (timestamp). n8n Workflow Configuration: Google Sheets Node: Configure the \"Get product ID\" node to connect to your Google Sheets document. Use OAuth2 for authentication. WooCommerce Node: Set up the WooCommerce nodes to connect to your WooCommerce store using the WooCommerce API credentials. OpenRouter Node: Configure the \"Gemini 2.0 Flash Exp\" node with your OpenRouter API credentials to access the language model. Structured Output Parser: Ensure the output parser is set to handle the structured data format for meta titles and descriptions. Workflow Execution: Trigger the workflow manually to test the process or set up a schedule trigger to automate the workflow at regular intervals. Monitor the workflow execution to ensure that meta tags are generated and updated correctly in both WooCommerce and Google Sheets. Validation: After the workflow runs, verify that the meta titles and descriptions in WooCommerce are correctly updated and that the Google Sheets document reflects the changes. This workflow streamlines the process of optimizing WooCommerce product pages for SEO, saving time and ensuring consistency in meta tag generation. n3w Italia | 11 days ago | Free", "completion": "{\"id\":\"JCp6tMYrzFDSOQ1C\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"WooCommerce SEO Yoast Meta Tag Generation\",\"tags\":[],\"nodes\":[{\"id\":\"02c4abf8-64f5-45e5-934f-0f1db4484c78\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[120,-480],\"parameters\":{},\"typeVersion\":1},{\"id\":\"d545f0e0-76dd-490b-85b6-7a83b65eb8f6\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[920,-280],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n\\t\\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n\\t\\t\\\"metatitle\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t},\\n\\t\\t\\\"metadescription\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t}\\n\\t}\\n}\"},\"typeVersion\":1.2},{\"id\":\"a9873f0e-8325-4434-9219-45c711a7afad\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[120,-660],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"minutes\"}]}},\"typeVersion\":1.2},{\"id\":\"9a232d7e-97e7-4e47-85ab-5069108d53ab\",\"name\":\"Get product ID\",\"type\":\"n8n-nodes-base.googleSheets\",\"position\":[320,-480],\"parameters\":{\"options\":{\"returnFirstMatch\":true},\"filtersUI\":{\"values\":[{\"lookupColumn\":\"METATITLE\"}]},\"sheetName\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gid=0\",\"cachedResultUrl"}
{"prompt": "Create an n8n workflow for: RAG:Context-Aware Chunking | Google Drive to Pinecone via OpenRouter & Gemini Workflow based on the following article. https://www.anthropic.com/news/contextual-retrieval This n8n automation is designed to extract, process, and store content from documents into a Pinecone vector store using context-based chunking. The workflow enhances retrieval accuracy in RAG (Retrieval-Augmented Generation) setups by ensuring each chunk retains meaningful context. Workflow Breakdown: \ud83d\udd39 Google Drive - Retrieve Document: The automation starts by fetching a source document from Google Drive. This document contains structured content, with predefined boundary markers for easy segmentation. \ud83d\udd39 Extract Text Content - Once retrieved, the document\u2019s text is extracted for processing. Special section boundary markers are used to divide the text into logical sections. \ud83d\udd39 Code Node - Create Context-Based Chunks: A custom code node processes the extracted text, identifying section boundaries and splitting the document into meaningful chunks. Each chunk is structured to retain its context within the entire document. \ud83d\udd39 Loop Node - Process Each Chunk: The workflow loops through each chunk, ensuring they are processed individually while maintaining a connection to the overall document context. \ud83d\udd39 Agent Node - Generate Context for Each Chunk: We use an Agent node powered by OpenAI\u2019s GPT-4.0-mini via OpenRouter to generate contextual metadata for each chunk, ensuring better retrieval accuracy. \ud83d\udd39 Prepend Context to Chunks & Create Embeddings - The generated context is prepended to the original chunk, creating context-rich embeddings that improve searchability. \ud83d\udd39 Google Gemini - Text Embeddings: The processed text is passed through Google Gemini text-embedding-004, which converts the text into semantic vector representations. \ud83d\udd39 Pinecone Vector Store - Store Embeddings: The final embeddings, along with the enriched chunk content and metadata, are stored in Pinecone, making them easily retrievable for RAG-based AI applications. Use Case: This automation enhances RAG retrieval by ensuring each chunk is contextually aware of the entire document, leading to more accurate AI responses. It\u2019s perfect for applications that require semantic search, AI-powered knowledge management, or intelligent document retrieval. By implementing context-based chunking, this workflow ensures that LLMs retrieve the most relevant data, improving response quality and accuracy in AI-driven applications. Udit Rawat | 22 days ago | Free +2", "completion": "{\"id\":\"VY4WBXuNDPxmOO5e\",\"meta\":{\"instanceId\":\"d16fb7d4b3eb9b9d4ad2ee6a7fbae593d73e9715e51f583c2a0e9acd1781c08e\",\"templateCredsSetupCompleted\":true},\"name\":\"RAG:Context-Aware Chunking | Google Drive to Pinecone via OpenRouter & Gemini\",\"tags\":[{\"id\":\"XZIQK6NdzGvgbZFd\",\"name\":\"Sell\",\"createdAt\":\"2025-01-15T12:28:48.424Z\",\"updatedAt\":\"2025-01-15T12:28:48.424Z\"}],\"nodes\":[{\"id\":\"7abbfa6e-4b17-4656-9b82-377b1bacf539\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"448ec137-bf64-46b4-bf15-c7a040faa306\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[1100,0],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"f22557ee-7f37-40cd-9063-a9a759274663\",\"name\":\"OpenRouter Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\"position\":[20,440],\"parameters\":{\"options\":{}},\"credentials\":{\"openRouterApi\":{\"id\":\"ddH6iNlm09UxrXvu\",\"name\":\"Auto: OpenRouter\"}},\"typeVersion\":1},{\"id\":\"57e8792e-25ae-43d5-b4e9-e87642365ee9\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[780,360],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"context-rag-test\",\"cachedResult"}
{"prompt": "Create an n8n workflow for: \ud83d\udd10\ud83e\udd99\ud83e\udd16 Private & Local Ollama Self-Hosted AI Assistant Transform your local N8N instance into a powerful chat interface using any local & private Ollama model, with zero cloud dependencies \u2601\ufe0f. This workflow creates a structured chat experience that processes messages locally through a language model chain and returns formatted responses \ud83d\udcac. How it works \ud83d\udd04 \ud83d\udcad Chat messages trigger the workflow \ud83e\udde0 Messages are processed through Llama 3.2 via Ollama (or any other Ollama compatible model) \ud83d\udcca Responses are formatted as structured JSON \u26a1 Error handling ensures robust operation Set up steps \ud83d\udee0\ufe0f \ud83d\udce5 Install N8N and Ollama \u2699\ufe0f Download Ollama 3.2 model (or other model) \ud83d\udd11 Configure Ollama API credentials \u2728 Import and activate workflow This template provides a foundation for building AI-powered chat applications while maintaining full control over your data and infrastructure \ud83d\ude80. Joseph LePage | 1 month ago | Free", "completion": "{\"id\":\"Telr6HU0ltH7s9f7\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\"},\"name\":\"\ud83d\udde8\ufe0fOllama Chat\",\"tags\":[],\"nodes\":[{\"id\":\"9560e89b-ea08-49dc-924e-ec8b83477340\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[280,60],\"webhookId\":\"4d06a912-2920-489c-a33c-0e3ea0b66745\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"c7919677-233f-4c48-ba01-ae923aef511e\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"onError\":\"continueErrorOutput\",\"position\":[640,60],\"parameters\":{\"text\":\"=Provide the users prompt and response as a JSON object with two fields:\\n- Prompt\\n- Response\\n\\nAvoid any preample or further explanation.\\n\\nThis is the question: {{ $json.chatInput }}\",\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"b9676a8b-f790-4661-b8b9-3056c969bdf5\",\"name\":\"Ollama Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmOllama\",\"position\":[740,340],\"parameters\":{\"model\":\"llama3.2:latest\",\"options\":{}},\"credentials\":{\"ollamaApi\":{\"id\":\"IsSBWGtcJbjRiKqD\",\"name\":\"Ollama account\"}},\"typeVersion\":1},{\"id\":\"61dfcda5-083c-43ff-8451-b2417f1e4be4\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-380,-380],\"parameters\":{\"color\":4,\"width\":520,\"height\":860,\"content\":\"# \ufffd"}
{"prompt": "Create an n8n workflow for: Personal Portfolio CV Rag Chatbot - with Conversation Store and Email Summary Personal Portfolio CV Rag Chatbot - with Conversation Store and Email Summary Target Audience This template is perfect for: Individuals looking to create a working professional and interactive personal portfolio chatbot. Developers interested in integrating RAG Chatbot functionality with conversation storage. 1. Description Create a stunning Personal Portfolio CV with integrated RAG Chatbot capabilities, including conversation storage and daily email summaries. 2.Features: Training: Setup Ingestion stage Upload your CV to Google Drive and let the Drive trigger updates to read your resume cv and convert it into your vector database (RAG purpose). Modify any parts as needed. Chat & Track: Use any frontend/backend interface to call the chat API and chat history API. Reporting Daily Chat Conversations: Receive daily automatic summaries of chat conversations. Data stored via NocoDB. 3.Setup Guide: Step-by-Step Instructions: Ensure all credentials are ready. Follow the notes provided. Ingestion: Upload your CV to Google Drive. The Drive triggers RAG update in your vector database. You can change the folder name, files and indexname of the vector database accordingly. Chat: Use any frontend/backend interface to call the chat API (refer to the notes for details) . [optional] Use any frontend/backend interface to call the update chat history API (refer to the notes for details). 3.Tracking Chat: Get daily automatic summaries of chat conversations.Format email conversations report as you like. You are ready to go! Sean Lon | 9 days ago | Free +8", "completion": "{\"id\":\"hzwyrm761fxBLiG8\",\"meta\":{\"instanceId\":\"ad5495d3968354550b9eb7602d38b52edcc686292cf1307ba0b9ddf53ca0622e\",\"templateId\":\"2753\",\"templateCredsSetupCompleted\":true},\"name\":\"Personal Portfolio Resume CV Chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"cfe6fd0a-546b-4f5d-8dbd-6ff2dd123a67\",\"name\":\"Embeddings Google Gemini\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsGoogleGemini\",\"position\":[880,640],\"parameters\":{\"modelName\":\"models/text-embedding-004\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"cSntB2ONStvkOFU7\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"bea384d2-a847-467d-a3eb-80e96bfb5a99\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1380,380],\"parameters\":{\"color\":3,\"width\":660,\"height\":960,\"content\":\"## Set up steps\\n\\n1. **Google Cloud Project and Vertex AI API**:\\n - Create a Google Cloud project.\\n - Enable the Vertex AI API for your project.\\n\\n2. **Google AI API Key**:\\n - Obtain a Google AI API key from Google AI Studio.\\n\\n3. **Pinecone Account**:\\n - Create a free account on the Pinecone website.\\n - Obtain your API key from your Pinecone dashboard.\\n - Create an index named `seanrag` or any other name in your Pinecone project.\\n\\n4. **Google Drive**:\\n - Create a dedicated folder in your Google Drive to store company documents.\\n\\n5. **Credentials in n8n**:\\n - Configure the following credentials in your n8n environment:\\n - Google Drive OAuth2\\n - Google Gemini (PaLM) API (using your Google AI API key)\\n - Pinecone API (using your Pinecone API key"}
{"prompt": "Create an n8n workflow for: Author and Publish Blog Posts From Google Sheets What it is: An automation to plan\u2192draft\u2192finalize and publish your textual blog post ideas to your wordpress blog Works in stages and hand back control to you in between those You can use a Google Spreadsheet for planning topics and configuring LLM models and prompts What it does: plans\u2192drafts\u2192finalizes blog post topics you specify in a Google Spreadsheet using an LLM with prompts that also ar configured in that spreadsheet (even which model to use) savs the results in the corresponding columns of the \"Schedule\" sheet in the spreadsheet hands control back to the user for inspecting or changing the results and for setting the next \"Action\" for th workflow Finally publishes the blog post to your Wordpress instance Limitations Probably slightly over-engineered ;-) No media generation yet some LLM models don't work because of their output format How it works: The Workflow is triggered manually or scheduled every hour It ingests a Google Spreadsheet to get Config for prompts/context tc Blog-Topics and their status and next action Depending on each blog topics \"Status\" and \"Action\" it then either uses an LLM for th next action (\"plan\"\u2192\"draft\"\u2192\"final\" actions) or publishes the written content to your Wordpress instance (\"publish\" actions) Set up steps: Import the workflow Make your own copy of the Google Spreadsheet Update the credentials using your individual credentials for: Google Spreadsheets OpenRouter Edit the \"Settings\" node and enter your individual values for Your spreadsheet copy URL Your wordpress blog URL Your wordpress blog username Your wordpress blog app password (a 4x4 alphanumeric sequence), that you probably have to create first, for which your wordpress user has to have 2-factor-authentication enabled. In your own copy of the spreadsheet: individualize the \"Config\" sheet's \"Value\" column for the prompts/context/etc Populate the \"Schedule\" sheet with at least one line in which you specify a \"Topic\" a \"Schedulded\" date (YYYY-MM-DD HH:mm:ss) a \"Status\" of \"idea\" an \"Action\" of \"plan\" (to kick off that action) Daniel Nolde | 22 days ago | Free", "completion": "{\"id\":\"b0KRVIuuUxE5afHo\",\"meta\":{\"instanceId\":\"98bf0d6aef1dd8b7a752798121440fb171bf7686b95727fd617f43452393daa3\",\"templateCredsSetupCompleted\":true},\"name\":\"Blog Automation TEMPLATE\",\"tags\":[{\"id\":\"uumvgGHY5e6zEL7V\",\"name\":\"Published Template\",\"createdAt\":\"2025-02-10T11:18:10.923Z\",\"updatedAt\":\"2025-02-10T11:18:10.923Z\"}],\"nodes\":[{\"id\":\"20e00146-6bda-4a8a-9544-bf7e5fd4e12e\",\"name\":\"Settings\",\"type\":\"n8n-nodes-base.set\",\"position\":[-420,-160],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"528b371f-0fba-4be1-9801-0502652da23e\",\"name\":\"urlSpreadsheet\",\"type\":\"string\",\"value\":\"https://docs.google.com/spreadsheets/d/1Kg1-U6mJF4bahH1jCw8kT48MiKz1UMC5n-9q77BHM3Q/edit?gid=0#gid=0\"},{\"id\":\"1be018c7-51fe-4ea2-967d-ce47a2e8795c\",\"name\":\"urlWordpress\",\"type\":\"string\",\"value\":\"SUBDOMAIN.wordpress.com\"},{\"id\":\"95377f4f-184b-46a7-94c7-b2313c314cb2\",\"name\":\"wordpressUsername\",\"type\":\"string\",\"value\":\"YourUserName\"},{\"id\":\"fdc99dc6-d9b0-4d2f-b770-1d8b6b360cad\",\"name\":\"wordpressApplicationPassword\",\"type\":\"string\",\"value\":\"y0ur app1 p4ss w0rd\"},{\"id\":\"517cb9ff-24fc-41d6-8bcc-253078f56356\",\"name\":\"sheetSchedule\",\"type\":\"string\",\"value\":\"=Schedule\"},{\"id\":\"584e11da-546b-4472-8674"}
{"prompt": "Create an n8n workflow for: Chat with your event schedule from Google Sheets in Telegram What it is Chat with your event schedule from Google Sheets in Telegram: \"When is the next meetup?\" \"How many events are there next month?\" \"Who presented most often?\" \"Which future meetups have no presenters yet?\" This workflow lets you chat with a telegram bot about past, present and future events that are scheduled in a Google Spreadsheet. (Info: This proof-of-concept was created as a demo for a hackathon of an AI & Developer Meetup in Da Nang (Vietnam) that uses a telegram group to organize) Who it is for If you want an easy way for your audience to get information about your events, you can us this workflow for the same purpose, or easily adapt it to your needs and different use-cases where you want to query smaller amounts of tabular data in natural language. How it works Upon getting triggered by a chat message to a telegram bot, the schedule of meetups is retrieved from Google Spreadsheets, converted into a markdown table syntax and fed into the system prompt of an LLM (we're using OpenRouter in this example), whose output is posted back as answer into the same telegram chat. Setup steps TO REVIEWING IN ACTION As the reviewer of this workflow, you can temporarily use it via an existing telegram bot, simply point your telegram client to https://t.me/AiDaNangBot and start to ask questions like: \"When is the next meetup?\" \"What future meetings do not have presenters?\" \"Who presented on Future of Human Relationships?\" To build upon this workflow: Import the workflow Customize the Google Docs credentials for your individual access Create a telegram bot and connect it to the workflow by entering its API token into the credentials used in the telegram trigger node In the \"Settings\" node, replace the \"scheduleURL\" with the URL of your own copy of the Google Spreadsheet or a copy of the Event Schedule Template Sheet to spin off your own \u2013 whereby the structure of the spreadsheet doesn't matter, it's just important that you semantically structure your information in dedicated columns clearly labeled in the header row. Daniel Nolde | 14 days ago | Free", "completion": "{\"id\":\"bV0JTA5NtRZxiD1q\",\"meta\":{\"instanceId\":\"98bf0d6aef1dd8b7a752798121440fb171bf7686b95727fd617f43452393daa3\",\"templateCredsSetupCompleted\":true},\"name\":\"Telegram-bot AI Da Nang\",\"tags\":[],\"nodes\":[{\"id\":\"ae5f9ca6-6bba-4fe8-b955-6c615d8a522f\",\"name\":\"SendTyping\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[-1780,-260],\"webhookId\":\"26ea953e-93d9-463e-ad90-95ea8ccb449f\",\"parameters\":{\"chatId\":\"={{ $('telegramInput').item.json.message.chat.id }}\",\"operation\":\"sendChatAction\"},\"credentials\":{\"telegramApi\":{\"id\":\"V3EtQBeqEvnOtl9p\",\"name\":\"Telegram account\"}},\"typeVersion\":1.2},{\"id\":\"244e7be3-2caa-46f7-8628-d063a3b84c12\",\"name\":\"SetResponse\",\"type\":\"n8n-nodes-base.set\",\"notes\":\"Assemble response etc.\",\"position\":[40,-420],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"fba8dc48-1484-4aae-8922-06fcae398f05\",\"name\":\"responseMessage\",\"type\":\"string\",\"value\":\"={{ $json.output }}\"},{\"id\":\"df8243e6-6a24-4bad-8807-63d75c828150\",\"name\":\"\",\"type\":\"string\",\"value\":\"\"}]},\"includeOtherFields\":true},\"notesInFlow\":true,\"typeVersion\":3.4},{\"id\":\"192aa194-f131-4ba3-8842-7c88da1a6129\",\"name\":\"Settings\",\"type\":\"n8n-nodes-base.set\",\"position\":[-1260,-420],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"6714203d-04b3-4a3c-9183-09cddcffdfe8\",\""}
{"prompt": "Create an n8n workflow for: \ud83e\udd16\ud83e\uddd1\u200d\ud83d\udcbb AI Agent for Top n8n Creators Leaderboard Reporting This n8n workflow is designed to automate the aggregation, processing, and reporting of community statistics related to n8n creators and workflows. Its primary purpose is to generate insightful reports that highlight top contributors, popular workflows, and key trends within the n8n ecosystem. Here's how it works and why it's important: How It Works Data Retrieval: The workflow fetches JSON data files from a GitHub repository containing statistics about creators and workflows. It uses HTTP requests to access these files dynamically based on pre-defined global variables. Data Processing: The data is parsed into separate streams for creators and workflows. It processes the data to identify key metrics such as unique weekly and monthly inserters/visitors. Ranking and Filtering: The workflow sorts creators by their weekly inserts and workflows by their popularity. It selects the top 10 creators and top 50 workflows for detailed analysis. Report Generation: Using AI tools like GPT-4 or Google Gemini, the workflow generates a Markdown report summarizing trends, contributors, and workflow statistics. The report includes tables with detailed metrics (e.g., unique visitors, inserters) and insights into why certain workflows are popular. Distribution: The report is saved locally or uploaded to Google Drive. It can also be shared via email or Telegram for broader accessibility. Automation: A schedule trigger ensures the workflow runs daily or as needed, keeping the reports up-to-date. Why It's Important Community Insights**: This workflow provides actionable insights into the n8n community by identifying impactful contributors and popular workflows. This fosters collaboration and innovation within the ecosystem. Time Efficiency**: By automating data collection, processing, and reporting, it saves significant time and effort for community managers or administrators. Recognition of Contributors**: Highlighting top creators encourages engagement and recognizes individuals driving value in the community. Trend Analysis**: The workflow helps uncover patterns in usage, enabling better decision-making for platform improvements or feature prioritization. Scalability**: With its modular design, this workflow can be easily adapted to include additional metrics or integrate with other tools. Joseph LePage | 13 days ago | Free +4", "completion": "{\"id\":\"6zSE618gr9fDtAfF\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83e\udd16\ud83e\uddd1\u200d\ud83d\udcbb AI Agent for Top n8n Creators Leaderboard Reporting\",\"tags\":[],\"nodes\":[{\"id\":\"5b9537db-41d3-4d8a-bf41-f875e4044224\",\"name\":\"stats_aggregate_creators\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1240,1300],\"parameters\":{\"url\":\"={{ $json.path }}{{ $json['creators-filename'] }}.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"feb2328b-57b0-4280-98d8-6b946db0c947\",\"name\":\"stats_aggregate_workflows\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1240,1500],\"parameters\":{\"url\":\"={{ $json.path }}{{ $json['workflows-filename'] }}.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"53f8b825-b030-4541-b12b-6df6702f7d1b\",\"name\":\"Global Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[-1660,1460],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"4bcb91c6-d250-4cb4-8ee1-022df13550e1\",\"name\":\"path\",\"type\":\"string\",\"value\":\"https://raw.githubusercontent.com/teds-tech-talks/n8n-community-leaderboard/refs/heads/main/\"},{\"id\":\"a910a798-0bfe-41b1-a4f1-41390c7f6997\",\"name\":\"workflows-filename\",\"type\":\"string\",\"value\":\"=stats_aggregate_workflows\"},{\"id\":\"e977e816-dc1e-43ce-9393-d6488e68"}
{"prompt": "Create an n8n workflow for: \ud83c\udfa6\ud83d\ude80 YouTube Video Comment Analysis Agent \ud83c\udfa6\ud83d\ude80 YouTube Video Comment Analysis Agent This n8n workflow is designed to help YouTube creators analyze video details and comments to generate a comprehensive and actionable report. The workflow provides insights into video performance, audience engagement, and viewer feedback, helping creators identify trends, interests, and opportunities for future content creation. \u2728 Key Features Video Performance Analysis: Extracts metrics like views, likes, and comments to evaluate the video's success. Comment Sentiment Analysis: Determines the tone of comments (positive, neutral, or negative) to understand audience sentiment. Recurring Themes Detection: Identifies common topics or questions in comments to highlight viewer interests. Engagement Drivers: Pinpoints what aspects of the video resonated most with viewers. Actionable Recommendations: Offers strategies for creating follow-up content or improving future videos. Keyword Suggestions: Extracts frequently mentioned terms for better discoverability on YouTube. Collaboration Opportunities: Suggests potential partnerships based on viewer feedback or related channels. \ud83d\udee0\ufe0f How to Use Set Up Workflow Variables: Add your GOOGLE_API_KEY and the VIDEO_ID of the YouTube video you want to analyze in the \"Workflow Variables\" node. Ensure your Google API key has access to the YouTube Data API. Run the Workflow: Trigger the workflow manually or through another workflow using the \"Execute Workflow Trigger\" node. The workflow will fetch video details and comments using pagination to ensure all data is captured. Generate Insights: The workflow processes video details and comments to create a detailed report with actionable insights. Outputs include sentiment analysis, engagement drivers, content opportunities, and audience profiling. View or Share Results: The report is converted into Markdown and can be emailed via Gmail or saved to Google Drive as a document. \ud83c\udf1f Value from This Workflow Gain a deeper understanding of your audience's preferences and feedback. Identify trends and engagement drivers to replicate success in future videos. Discover new content opportunities based on viewer questions and suggestions. Improve discoverability by leveraging keyword suggestions extracted from comments. Build stronger connections with your audience by addressing their needs effectively. Joseph LePage | 10 days ago | Free", "completion": "{\"id\":\"d23vz3qcBf6KfuZA\",\"meta\":{\"instanceId\":\"31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83c\udfa6\ud83d\ude80 YouTube Video Comment Analysis Agent\",\"tags\":[],\"nodes\":[{\"id\":\"6661e7c3-ec1e-43b0-8bc6-44abbefbbcea\",\"name\":\"When Executed by Another Workflow\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"disabled\":true,\"position\":[-160,80],\"parameters\":{\"inputSource\":\"jsonExample\",\"jsonExample\":\"{\\n \\\"query\\\": {\\n\\t\\\"videoId\\\": \\\"YouTube video id\\\"\\n }\\n}\"},\"typeVersion\":1.1},{\"id\":\"729edcc9-7eda-4ad0-b168-5e5a57cdbf6a\",\"name\":\"Sticky Note10\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-300,-80],\"parameters\":{\"color\":7,\"width\":1730,\"height\":760,\"content\":\"## \ud83d\udee0\ufe0fYouTube Video Details & Comments Processing Tool\"},\"typeVersion\":1},{\"id\":\"454c3494-9808-475d-ad53-decd54d99783\",\"name\":\"Create YouTube API URL\",\"type\":\"n8n-nodes-base.code\",\"position\":[500,100],\"parameters\":{\"jsCode\":\"// Define the base URL for the YouTube Data API\\nconst BASE_URL = 'https://www.googleapis.com/youtube/v3/videos';\\n\\n// Get the first input item\\nconst item = $input.first();\\n\\n// Extract the videoId and google_api_key from the input JSON\\nconst VIDEO_ID = item.json.VIDEO_ID;\\nconst GOOGLE_API_KEY = item.json.GOOGLE_API_KEY; // Dynamically retrieve API key\\n\\nif (!VIDEO_ID) {\\n throw new Error('The video ID parameter is empty.');\\n}\\n\\nif (!GOOGLE_API_KEY) {\\n throw new"}
{"prompt": "Create an n8n workflow for: Free YouTube Video Analyzer with AI-Powered Summaries & Email Alerts This workflow is designed to analyze YouTube videos by extracting their transcripts, summarizing the content using AI models, and sending the analysis via email. This workflow is ideal for content creators, marketers, or anyone who needs to quickly analyze and summarize YouTube videos for research, content planning, or educational purposes. How It Works: Trigger: The workflow starts with a manual trigger, allowing you to test it by clicking \"Test workflow.\" You can also set a YouTube video URL manually or dynamically. YouTube Video ID Extraction: The workflow extracts the YouTube video ID from the provided URL using a custom JavaScript function. This ID is necessary for fetching the transcript. Transcript Generation: The video ID is sent via an HTTP request to generate the transcript. You need to replace APIKEY with a free API key from the service. Transcript Validation: The workflow checks if a transcript exists for the video. If a transcript is available, it proceeds; otherwise, it stops. Full Text Extraction: If a transcript exists, the workflow combines all transcript segments into a single text variable for further analysis. AI-Powered Analysis: The full transcript is passed to an AI model (DeepSeek, OpenAI, or OpenRouter) for analysis. The AI generates a structured summary, including a title and key points, formatted in markdown. Email Notification: The analysis results (title and summary) are sent via email using SMTP credentials. The email contains the structured summary of the video. Set Up Steps: YouTube Transcript API: Obtain a free API key from youtube-transcript.io and replace APIKEY in the \"Generate transcript\" node with your key. AI Model Configuration: Configure the AI model nodes (DeepSeek, OpenAI, or OpenRouter) with the appropriate API credentials. You can choose one or multiple models depending on your preference. Email Setup: Configure the \"Send Email\" node with your SMTP credentials (e.g., Gmail, Outlook, or any SMTP service). Ensure the email settings are correct to send the analysis results. Key Features: Free Tools: Uses **youtube-transcript.io for free transcript generation. AI Models**: Supports multiple AI models (DeepSeek, OpenAI, OpenRouter) for flexible analysis. Email Notifications**: Sends the analysis results directly to your inbox. Customizable**: Easily adapt the workflow to analyze different videos or use different AI models. n3w Italia | 10 days ago | Free", "completion": "{\"id\":\"G3yjjk93c1bBM5tc\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"YouTube Video Analyzer with AI\",\"tags\":[],\"nodes\":[{\"id\":\"fbf55337-4b64-43f5-9fed-a08b4ab43a8c\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-80,-160],\"parameters\":{},\"typeVersion\":1},{\"id\":\"48f88f6d-9817-4984-beb0-e37fff747317\",\"name\":\"YouTube Video ID\",\"type\":\"n8n-nodes-base.code\",\"position\":[360,-160],\"parameters\":{\"jsCode\":\"const extractYoutubeId = (url) => {\\n // Regex pattern that matches both youtu.be and youtube.com URLs\\n const pattern = /(?:youtube\\\\.com\\\\/(?:[^\\\\/]+\\\\/.+\\\\/|(?:v|e(?:mbed)?)\\\\/|.*[?&]v=)|youtu\\\\.be\\\\/)([^\\\"&?\\\\/\\\\s]{11})/;\\n const match = url.match(pattern);\\n return match ? match[1] : null;\\n};\\n\\n// Input URL from previous node\\nconst youtubeUrl = items[0].json.youtubeUrl; // Adjust this based on your workflow\\n\\n// Process the URL and return the video ID\\nreturn [{\\n json: {\\n videoId: extractYoutubeId(youtubeUrl)\\n }\\n}];\\n\"},\"typeVersion\":2},{\"id\":\"88b5df30-064a-4735-9753-96ca7c272642\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1520,140],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAi"}
{"prompt": "Create an n8n workflow for: Fetch Keyword From Google Sheet and Classify Them Using AI Who is this template for This template is for marketers, SEO specialists, or content managers who need to analyze keywords to identify which ones contain references to a specific area or topic, in this case \u2013 IT software, services, tools, or apps. Use case Automating the process of scanning a large list of keywords to determine if they reference known IT products or services (like ServiceNow, Salesforce, etc.), and updating a Google Sheet with this classification. This helps in categorizing keywords for targeted SEO campaigns, content creation, or market analysis. How this workflow works Fetches keyword data from a Google Sheet Processes keywords in batches to prevent rate limiting Uses an AI agent (OpenAI) to analyze each keyword and determine if it contains a reference to an IT service/software Updates the original Google Sheet with the results in a \"Service?\" column Continues processing until all keywords are analyzed Set up steps Connect your Google Sheets account credentials Set the Google Sheet document ID (currently using \"Copy of Sheet1 1\") Configure the OpenAI API credentials for the AI agent Adjust the batch size (currently 6) if needed based on your API rate limits Ensure the Google Sheet has the required columns: \"Number\", \"Keyword\", and \"Service?\" The AI agent's prompt is highly customizable to match different identification needs. For example, instead of looking for IT software/services, you could modify the prompt to identify: Industry-specific terms (healthcare, finance, education) Geographic references (cities, countries, regions) Product categories (electronics, clothing, food) Competitor brand mentions Here's how you could modify the prompt for different use cases: Copy // For identifying educational content keywords \"Check the keyword I provided and define if this keyword relates to educational content, courses, or learning materials and return yes or no.\" // For identifying local service keywords \"Check the keyword I provided and determine if it contains location-specific terms (city names, neighborhoods, regions) that suggest local service intent and return yes or no.\" // For identifying competitor mentions \"Check the keyword I provided and determine if it mentions any of our competitors (CompetitorA, CompetitorB, CompetitorC) and return yes or no.\" ` Polina Medvedieva | 13 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"53e93a66-468a-4df8-b2cb-58ff0563f83f\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-160,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"70692fd5-d575-49d2-9e3c-71bdddb0782e\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[1000,0],\"parameters\":{\"text\":\"=keyword: {{ $json.Keyword }}\",\"options\":{\"systemMessage\":\"=Check the keyword I provided and define if this keyword has a name of the known IT software, service, tool or app as a part of it (for example, ServiceNow or Salesforce) and return yes or no.\"},\"promptType\":\"define\",\"hasOutputParser\":true},\"typeVersion\":1.7},{\"id\":\"587e6283-32c0-4599-a024-2ce0079bdaeb\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1000,240],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"ju5aHhTljmCDxSl9\",\"name\":\"OpenAi account Polina's\"}},\"typeVersion\":1.2},{\"id\":\"0e3e7d09-202e-47cc-8704-16ab70bc4077\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1180,240],\"parameters\":{\"jsonSchemaExample\":\"{\\n\\t\\\"Isservice\\\": \\\"yes\\\"\\n}\"},\"typeVersion\":1.2},{\"id\":\"900ac097-c6de-41c0-8"}
{"prompt": "Create an n8n workflow for: Social Media Analysis and Automated Email Generation Social Media Analysis and Automated Email Generation &gt; by Thomas Vie Thomas@pollup.net Who is this for? This template is ideal for marketers, lead generation specialists, and business professionals seeking to analyze social media profiles of potential leads and automate personalized email outreach efficiently. What problem is this workflow solving? Manually analyzing social media profiles and crafting personalized emails can be time-consuming and prone to errors. This workflow streamlines the process by integrating social media APIs with AI to generate tailored communication, saving time and increasing outreach effectiveness. What this workflow does: Google Sheets Integration: Start with a Google Sheet containing lead information such as LinkedIn URL, Twitter handle, name, and email. Social Media Data Extraction: Automatically fetch profile and activity data from Twitter and LinkedIn using RapidAPI integrations. AI-Powered Content Generation: Use OpenAI's Chat Model to analyze the extracted data and generate personalized email subject lines and cover letters. Automated Email Dispatch: Send the generated email directly to the lead, with a copy sent to yourself for tracking purposes. Progress Tracking: Update the Google Sheet to indicate completed actions. Setup: Google Sheets: Create a sheet with the columns: LinkedIn URL, name, Twitter handle, email, and a \"done\" column for tracking. Populate the sheet with your leads. RapidAPI Accounts: Sign up for RapidAPI and subscribe to the Twitter and LinkedIn API plans. Configure API authentication keys in the workflow. AI Configuration: Connect OpenAI Chat Model with your API key for text generation. Email Integration: Add your email credentials or service (SMTP or third-party service like Gmail) for sending automated emails. How to customize this workflow to your needs: Modify the AI Prompt:** Adapt the prompt in the AI node to better align with your tone, style, or specific messaging framework. Expand Data Fields:** Add additional data fields in Google Sheets if you require further personalization. API Limits:** Adjust API configurations to fit your usage limits or upgrade to higher tiers for increased data scraping capabilities. Personalize Email Templates:** Tweak email formats to suit different audiences or use cases. Extend Functionality:** Integrate additional social media platforms or CRM tools as needed. By implementing this workflow, you\u2019ll save time on repetitive tasks and create more effective lead generation strategies. PollupAI | 1 month ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"a768bce6-ae26-464c-95fc-009edea4f94d\",\"name\":\"Set your company's variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[440,0],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"6a8063b6-1fd8-429a-9f13-b7512066c702\",\"name\":\"your_company_name\",\"type\":\"string\",\"value\":\"Pollup Data Services\"},{\"id\":\"3e6780d6-86d0-4353-aa17-8470a91f63a8\",\"name\":\"your_company_activity\",\"type\":\"string\",\"value\":\"Whether it\u2019s automating recurring tasks, analysing data faster, or personalising customer interactions, we build bespoke AI agents to help your workforce work smarter.\"},{\"id\":\"1b42f1b3-20ed-4278-952d-f28fe0f03fa3\",\"name\":\"your_email\",\"type\":\"string\",\"value\":\"thomas@pollup.net\"},{\"id\":\"7c109ba2-d855-49d5-8700-624b01a05bc1\",\"name\":\"your_name\",\"type\":\"string\",\"value\":\"Justin\"}]}},\"typeVersion\":3.4},{\"id\":\"ca729f8d-cab8-4221-addb-aa23813d80b4\",\"name\":\"Get linkedin Posts\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1300,0],\"parameters\":{\"url\":\"https://fresh-linkedin-profile-data.p.rapidapi.com/get-profile-posts\",\"options\":{},\"sendQuery\":true,\"sendHeaders\":true,\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"queryParameters\":{\"parameters\":[{\"name\":\"linkedin_url\",\"value\":\"={{ $('Google Sheets Trigger').item.json.linkedin_url }}\"},{\"name\":\"type\",\"value\":\"posts\"}]},\"headerParameters\":{\"parameters\":[{\"name\":\"x-rapidapi-host\",\"value\":\"fresh-linkedin-profile-data.p.rapidapi.com\"}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"nhoVFnkO31mejJrI\",\""}
{"prompt": "Create an n8n workflow for: AI-powered email processing autoresponder and response approval (Yes/No) How it Works This workflow is designed to automate the process of handling incoming emails, summarizing their content, generating appropriate responses, and obtaining approval before sending replies. Below are the key operational steps: Email Reception and Summarization: The workflow starts with an Email Trigger (IMAP) node that listens for new emails in a specified inbox. Once an email is received, its HTML content is processed by a Markdown node to convert it into plain text if necessary, followed by an Email Summarization Chain node which uses AI to create a concise summary of the email's content using prompts tailored for this purpose. Response Generation and Approval: A Write email node generates a professional response based on the summarized content, utilizing predefined templates and guidelines such as keeping responses under 100 words and ensuring they're formatted correctly in HTML. Before sending out any automated replies, the system sends these drafts via Gmail for human review and approval through a Gmail node configured with double-approval settings. If approved (Approve?), the finalized email is sent back to the original sender using the Send Email node; otherwise, it loops back for further edits or manual intervention. Set Up Steps To replicate this workflow within your own n8n environment, follow these essential configuration steps: Configuration: Begin by setting up an n8n instance either locally or via cloud services offered directly from their official site. Import the provided JSON configuration file into your workspace, making sure all required credentials like IMAP, SMTP, OpenAI API keys, etc., are properly set up under Credentials section since multiple nodes rely heavily on external integrations for functionalities like reading emails, generating summaries, crafting replies, and managing approvals. Customization: Adjust parameters according to specific business needs, including but not limited to adjusting the conditions used during conditional checks performed by nodes like Approve?. Modify the template messages given to AI models so they align closely with organizational tone & style preferences while maintaining professionalism expected in business communications. Ensure correct mappings between fields when appending data to external systems like Google Sheets or similar platforms where records might need tracking post-interaction completion. n3w Italia | 25 days ago | Free +2", "completion": "{\"id\":\"OuHrYOR3uWGmrhWQ\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"AI Email processing autoresponder with approval (Yes/No)\",\"tags\":[],\"nodes\":[{\"id\":\"06a098db-160b-45f7-aeac-a73ef868148e\",\"name\":\"Email Trigger (IMAP)\",\"type\":\"n8n-nodes-base.emailReadImap\",\"position\":[-180,-100],\"parameters\":{\"options\":{}},\"credentials\":{\"imap\":{\"id\":\"k31W9oGddl9pMDy4\",\"name\":\"IMAP info@n3witalia.com\"}},\"typeVersion\":2},{\"id\":\"9589443b-efb7-4e0d-bafc-0be9858a4755\",\"name\":\"Markdown\",\"type\":\"n8n-nodes-base.markdown\",\"position\":[40,-100],\"parameters\":{\"html\":\"={{ $json.textHtml }}\",\"options\":{}},\"typeVersion\":1},{\"id\":\"8de7b2f3-bf75-4f3c-a1ee-eec047a7b82e\",\"name\":\"DeepSeek R1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[240,80],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"deepseek/deepseek-r1:free\",\"cachedResultName\":\"deepseek/deepseek-r1:free\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"XJTqRiKFJpFs5MuX\",\"name\":\"OpenRouter account\"}},\"typeVersion\":1.2},{\"id\":\"babf37dc-99ca-439a-b094-91c52799b8df\",\"name\":\"Send Email\",\"type\":\"n8n-nodes-base.emailSend\",\"position\":[1840,-120],\"webhookId\":\"f84fcde7-6aac-485a-9a08-96"}
{"prompt": "Create an n8n workflow for: Email Summary Agent Problem Teams often struggle with email overload, leading to missed actions and inefficient meeting preparation. Solution This workflow automates email management using n8n and AI. It fetches emails, summarizes key points and actions, and sends two concise updates\u2014one in the morning and one at night. How It Works Triggers at 7 AM and 9 PM: Automates the process to summarize emails received during specific time blocks. Fetches Emails: Retrieves emails from the last 24 hours or after a specific time. Summarizes with AI: Uses OpenAI to process the email content into actionable summaries. Sends Team Updates: Compiles the summaries into a concise, formatted email and sends it to the team. Expected Results Significant reduction in missed actions and follow-ups. Customizations Adjust timings, filters, and recipients to suit your team\u2019s needs. Vishal Kumar | 1 month ago | Free", "completion": "{\"id\":\"M8oLW9Qd59zNJzg2\",\"meta\":{\"instanceId\":\"1abe0e4c2be794795d12bf72aa530a426a6f87aabad209ed6619bcaf0f666fb0\",\"templateCredsSetupCompleted\":true},\"name\":\"Email Summary Agent\",\"tags\":[{\"id\":\"G1v7CnFpOHsReVhM\",\"name\":\"Product\",\"createdAt\":\"2025-01-13T17:04:34.969Z\",\"updatedAt\":\"2025-01-13T17:04:34.969Z\"},{\"id\":\"RagrXIh5iBDseqvj\",\"name\":\"AI\",\"createdAt\":\"2025-01-09T09:18:12.756Z\",\"updatedAt\":\"2025-01-09T09:18:12.756Z\"},{\"id\":\"Yg2lfYteJZAoIeaC\",\"name\":\"Building blocks\",\"createdAt\":\"2025-01-13T17:05:49.788Z\",\"updatedAt\":\"2025-01-13T17:05:49.788Z\"},{\"id\":\"ZuS1C3NpE8uBlFq4\",\"name\":\"Finance\",\"createdAt\":\"2025-01-13T17:05:03.996Z\",\"updatedAt\":\"2025-01-13T17:05:03.996Z\"},{\"id\":\"aqlZb2qfWiaT4Xr5\",\"name\":\"IT Ops\",\"createdAt\":\"2025-01-03T12:20:11.917Z\",\"updatedAt\":\"2025-01-03T12:20:11.917Z\"},{\"id\":\"fX8hRnEv4D8sLSzF\",\"name\":\"OpenAI\",\"createdAt\":\"2025-01-09T09:18:12.757Z\",\"updatedAt\":\"2025-01-09T09:18:12.757Z\"},{\"id\":\"j1qBXzFADkR3sHSa\",\"name\":\"Marketing\",\"createdAt\":\"2025-01-13T17:03:54.468Z\",\"updatedAt\":\"2025-01-13T17:03:54.468Z\"},{\"id\":\"x3OVvOuZkLx1hYp"}
{"prompt": "Create an n8n workflow for: Hacker News Job Listing Scraper and Parser This automated workflow scrapes and processes the monthly \"Who is Hiring\" thread from Hacker News, transforming raw job listings into structured data for analysis or integration with other systems. Perfect for job seekers, recruiters, or anyone looking to monitor tech job market trends. How it works Automatically fetches the latest \"Who is Hiring\" thread from Hacker News Extracts and cleans relevant job posting data using the HN API Splits and processes individual job listings into structured format Parses key information like location, role, requirements, and company details Outputs clean, structured data ready for analysis or export Set up steps Configure API access to [Hacker News](https://github.com/HackerNews/API ) (no authentication required) Follow the steps to get your cURL command from https://hn.algolia.com/ Set up desired output format (JSON structured data or custom format) Optional: Configure additional parsing rules for specific job listing information Optional: Set up integration with preferred storage or analysis tools The workflow transforms unstructured job listings into clean, structured data following this pattern: Input: Raw HN thread comments Process: Extract, clean, and parse text Output: Structured job listing data This template saves hours of manual work collecting and organizing job listings, making it easier to track and analyze tech job opportunities from Hacker News's popular monthly hiring threads. Julian Kaiser | 15 days ago | Free", "completion": "{\"id\":\"0JsHmmyeHw5Ffz5m\",\"meta\":{\"instanceId\":\"d4d7965840e96e50a3e02959a8487c692901dfa8d5cc294134442c67ce1622d3\",\"templateCredsSetupCompleted\":true},\"name\":\"HN Who is Hiring Scrape\",\"tags\":[],\"nodes\":[{\"id\":\"f7cdb3ee-9bb0-4006-829a-d4ce797191d5\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-20,-220],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0475e25d-9bf4-450d-abd3-a04608a438a4\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[60,-620],\"parameters\":{\"width\":460,\"height\":340,\"content\":\"## Go to https://hn.algolia.com\\n- filter by \\\"Ask HN: Who is hiring?\\\" (important with quotes for full match)\\n- sort by date\\n- Chrome Network Tab > find API call > click \\\"Copy as cURL\\\"\\n- n8n HTTP node -> import cURL and paste \\n- I've set the API key as Header Auth so you will have to do the above yourself to make this work\"},\"typeVersion\":1},{\"id\":\"a686852b-ff84-430b-92bb-ce02a6808e19\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[400,-220],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"hits\"},\"typeVersion\":1},{\"id\":\"cdaaa738-d561-4fa0-b2c7-8ea9e6778eb1\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1260,-620],\"parameters\":{\"width\":500,\"height\":340,\"content\":\"## Go to HN API \\nhttps://github.com/HackerNews/API\\n\\nWe'll need following endpoints: \\n- For example, a story: https://h"}
{"prompt": "Create an n8n workflow for: HR & IT Helpdesk Chatbot with Audio Transcription An intelligent chatbot that assists employees by answering common HR or IT questions, supporting both text and audio messages. This unique feature ensures employees can conveniently ask questions via voice messages, which are transcribed and processed just like text queries. How It Works Message Capture: When an employee sends a message to the chatbot in WhatsApp or Telegram (text or audio), the chatbot captures the input. Audio Transcription: For audio messages, the chatbot transcribes the content into text using an AI-powered transcription service (e.g., Whisper, Google Cloud Speech-to-Text). Query Processing: The transcribed text (or directly entered text) is sent to an AI service (e.g., OpenAI) to generate embeddings. These embeddings are used to search a vector database (e.g., Supabase or Qdrant) containing the company\u2019s internal HR and IT documentation. The most relevant data is retrieved and sent back to the AI service to compose a concise and helpful response. Response Delivery: The chatbot sends the final response back to the employee, whether the input was text or audio. Set Up Steps Estimated Time**: 20\u201325 minutes Prerequisites**: Create an account with an AI provider (e.g., OpenAI). Connect WhatsApp or Telegram credentials in n8n. Set up a transcription service (e.g., Whisper or Google Cloud Speech-to-Text). Configure a vector database (e.g., Supabase or Qdrant) and add your internal HR and IT documentation. Import the workflow template into n8n and update environment variables for your credentials. Felipe Braga | 1 month ago | Free +6", "completion": "{\"id\":\"zmgSshZ5xESr3ozl\",\"meta\":{\"instanceId\":\"1fedaf0aa3a5d200ffa1bbc98554b56cac895dd5d001907cb6f1c7a3c0a78215\",\"templateCredsSetupCompleted\":true},\"name\":\"HR & IT Helpdesk Chatbot with Audio Transcription\",\"tags\":[],\"nodes\":[{\"id\":\"c6cb921e-97ac-48f6-9d79-133993dd6ef7\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-300,-280],\"parameters\":{\"color\":7,\"width\":780,\"height\":460,\"content\":\"## 1. Download & Extract Internal Policy Documents\\n[Read more about the HTTP Request Tool](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest)\\n\\nBegin by importing the PDF documents that contain your internal policies and FAQs\u2014these will become the knowledge base for your Internal Helpdesk Assistant. For example, you can store a company handbook or IT/HR policy PDFs on a shared drive or cloud storage and reference a direct download link here.\\n\\nIn this demonstration, we'll use the **HTTP Request node** to fetch the PDF file from a given URL and then parse its text contents using the **Extract from File node**. Once extracted, these text chunks will be used to build the vector store that underpins your helpdesk chatbot\u2019s responses.\\n\\n[Example Employee Handbook with Policies](https://s3.amazonaws.com/scschoolfiles/656/employee_handbook_print_1.pdf)\"},\"typeVersion\":1},{\"id\":\"450a254c-eec3-41ea-a11d-eb87b62ee4f4\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-80,20],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0972f31c-1f62-430c-8beb-bef8976cd0eb\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base."}
{"prompt": "Create an n8n workflow for: AI-Powered Email Automation for Business: Summarize & Respond with RAG This workflow is ideal for businesses looking to automate their email responses, especially for handling inquiries about company information. It leverages AI to ensure accurate and professional communication. How It Works Email Trigger: The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new messages. When a new email arrives, it triggers the workflow. Email Preprocessing: The Markdown node converts the email's HTML content into plain text for easier processing by the AI models. Email Summarization: The Email Summarization Chain node uses an AI model (DeepSeek R1) to generate a concise summary of the email. The summary is limited to 100 words and is written in Italian. Email Classification: The Email Classifier node categorizes the email into predefined categories (e.g., \"Company info request\"). If the email does not fit any category, it is classified as \"other\". Email Response Generation: The Write email node uses an AI model (OpenAI) to draft a professional response to the email. The response is based on the email content and is limited to 100 words. The Review email node uses another AI model (DeepSeek) to review and format the drafted response. It ensures the response is professional and formatted in HTML (e.g., using `, , , ` tags where necessary). Email Sending: The Send Email node sends the reviewed and formatted response back to the original sender. Vector Database Integration: The Qdrant Vector Store node retrieves relevant information from a vector database (Qdrant) to assist in generating accurate responses. This is particularly useful for emails classified as \"Company info request\". The Embeddings OpenAI node generates embeddings for the email content, which are used to query the vector database. Document Vectorization: The workflow includes steps to create and refresh a Qdrant collection (Create collection and Refresh collection nodes). Documents from Google Drive are downloaded (Get folder and Download Files nodes), processed into embeddings (Embeddings OpenAI1 node), and stored in the Qdrant vector store (Qdrant Vector Store1 node). Set Up Steps Configure Email Trigger: Set up the Email Trigger (IMAP) node with the appropriate IMAP credentials to monitor the email inbox. Set Up AI Models: Configure the DeepSeek R1, OpenAI, and DeepSeek nodes with the appropriate API credentials for text summarization, response generation, and review. Set Up Email Classification: Define the categories in the Email Classifier node (e.g., \"Company info request\", \"Other\"). Ensure the OpenAI 4-o-mini node is configured to assist in classification. Set Up Vector Database: Configure the Qdrant Vector Store and Qdrant Vector Store1 nodes with the appropriate Qdrant API credentials and collection details. Set up the Embeddings OpenAI and Embeddings OpenAI1 nodes to generate embeddings for the email content and documents. Set Up Document Processing: Configure the Get folder and Download Files nodes to access and download documents from Google Drive. Use the Token Splitter and Default Data Loader nodes to process and split the documents into manageable chunks for vectorization. Set Up Email Sending: Configure the Send Email node with the appropriate SMTP credentials to send responses. Test the Workflow: Trigger the workflow manually using the When clicking \u2018Test workflow\u2019 node to ensure all steps execute correctly. Verify that emails are summarized, classified, and responded to accurately. Activate the Workflow: Once tested, activate the workflow to automate the process of handling incoming emails. Key Features Automated Email Handling**: Automatically processes incoming emails, summarizes them, and generates professional responses. AI-Powered Classification**: Uses AI to classify emails into relevant categories for targeted responses. Vector Database Integration**: Retrieves relevant information from a vector database to enhance response accuracy. Document Vectorization**: Processes and stores documents from Google Drive in a vector database for quick retrieval. Professional Email Formatting**: Ensures responses are professionally formatted and concise. n3w Italia | 27 days ago | Free +7", "completion": "{\"id\":\"q8IFGLeOCGSfoWZu\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Email AI Auto-responder. Summerize and send email\",\"tags\":[],\"nodes\":[{\"id\":\"59885699-0f6c-4522-acff-9e28b2a07b82\",\"name\":\"Email Trigger (IMAP)\",\"type\":\"n8n-nodes-base.emailReadImap\",\"position\":[-440,-20],\"parameters\":{\"options\":{}},\"credentials\":{\"imap\":{\"id\":\"k31W9oGddl9pMDy4\",\"name\":\"IMAP info@n3witalia.com\"}},\"typeVersion\":2},{\"id\":\"b268ab9d-b2e3-46e6-b7ae-70aff0b5484d\",\"name\":\"Markdown\",\"type\":\"n8n-nodes-base.markdown\",\"position\":[-220,-20],\"parameters\":{\"html\":\"={{ $json.textHtml }}\",\"options\":{}},\"typeVersion\":1},{\"id\":\"13c2d151-6f59-4e1f-a174-02d4d0bcaefd\",\"name\":\"DeepSeek R1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-20,160],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"deepseek/deepseek-r1:free\",\"cachedResultName\":\"deepseek/deepseek-r1:free\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"XJTqRiKFJpFs5MuX\",\"name\":\"OpenRouter account\"}},\"typeVersion\":1.2},{\"id\":\"8149e40d-64e6-4fb9-aebc-2a2483961f07\",\"name\":\"Send Email\",\"type\":\"n8n-nodes-base.emailSend\",\"position\":[500,340],\"parameters\":{\"html\":\"={{ $json.text }}\",\"options\":{},\"subject\":\"=Re: {{"}
{"prompt": "Create an n8n workflow for: AI Agent : Google calendar assistant using OpenAI This template is a simple AI Agent that acts as a Google Calendar Assistant. It is designed for beginners to have their \"first AI Agent\" performing common tasks and to help them understand how it works. For new users of n8n, AI Agents, and OpenAI: This template involves using an OpenAI API Key. If you are new to AI Agents, make sure to research and understand key concepts such as: \"Tokens\"** (used for API requests), \"Tool calling\"** (how the AI interacts with external tools), OpenAI's usage costs** (how you will be billed for API usage). Functionality It has two main functionalities: Create events** in a calendar Retrieve events** from a calendar How you can use it Everything is explained with sticky notes in the workflow. It is ready-to-use: all you need to do is connect your OpenAI credentials, and you can start using the workflow. Dataki | 1 month ago | Free", "completion": "{\"id\":\"ITH6r6UYtlCyUcpj\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\"},\"name\":\"AI Agent : Google calendar assistant using OpenAI\",\"tags\":[],\"nodes\":[{\"id\":\"2e670a54-f789-4c8b-abba-ae35c458f5ed\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-280,0],\"webhookId\":\"5308edc9-738b-4aae-a789-214e2392579a\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"96bf895f-a18c-4a4c-bc26-3ec5d2372de5\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[160,820],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"\",\"name\":\"OpenAi\"}},\"typeVersion\":1},{\"id\":\"270176df-9c2d-4f1a-b017-9349cb249341\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[580,820],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"5cdece35-bd69-4c77-b240-963df8781d64\",\"name\":\"Google Calendar - Get Events\",\"type\":\"n8n-nodes-base.googleCalendarTool\",\"position\":[960,800],\"parameters\":{\"options\":{\"timeMax\":\"={{ $fromAI('end_date') }}\",\"timeMin\":\"={{ $fromAI('start_date') }}\"},\"calendar\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"\",\"cachedResultName\":\"\"},\"operation\":\"getAll\",\"descriptionType\":\"manual\",\"toolDescription\":\"Use this tool when you\u2019re asked to retrieve events data.\"},\"credentials\":{\"googleCalendarOAuth2"}
{"prompt": "Create an n8n workflow for: \ud83d\udd0d Perplexity Research to HTML: AI-Powered Content Creation Transform simple queries into comprehensive, well-structured content with this n8n workflow that leverages Perplexity AI for research and GPT-4 for content transformation. Create professional blog posts and HTML content automatically while maintaining accuracy and depth. Intelligent Research & Analysis \ud83d\ude80 Automated Research Pipeline Harnesses Perplexity AI's advanced research capabilities Processes complex topics into structured insights Delivers comprehensive analysis in minutes instead of hours \ud83e\udde0 Smart Content Organization Automatically structures content with clear hierarchies Identifies and highlights key concepts Maintains technical accuracy while improving readability Creates SEO-friendly content structure Content Transformation Features \ud83d\udcdd Dynamic Content Generation Converts research into professional blog articles Generates clean, responsive HTML output Implements proper semantic structure Includes metadata and categorization \ud83c\udfa8 Professional Formatting Responsive Tailwind CSS styling Clean, modern HTML structure Proper heading hierarchy Mobile-friendly layouts Blockquote highlighting for key insights Perfect For \ud83d\udcda Content Researchers Save hours of manual research by automating the information gathering and structuring process. \u270d\ufe0f Content Writers Focus on creativity while the workflow handles research and technical formatting. \ud83c\udf10 Web Publishers Generate publication-ready HTML content with modern styling and proper structure. Technical Implementation \u26a1 Workflow Components Webhook endpoint for query submission Perplexity AI integration for research GPT-4 powered content structuring HTML transformation engine Telegram notification system (optional) Transform your content creation process with an intelligent system that handles research, writing, and formatting while you focus on strategy and creativity. Joseph LePage | 2 months ago | Free +2", "completion": "{\"id\":\"HnqGW0eq5asKfZxf\",\"meta\":{\"instanceId\":\"03907a25f048377a8789a4332f28148522ba31ee907fababf704f1d88130b1b6\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83d\udd0d\ud83d\udee0\ufe0fPerplexity Researcher to HTML Web Page\",\"tags\":[],\"nodes\":[{\"id\":\"ad5d96c6-941a-4ab3-b349-10bae99e5988\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[320,1360],\"parameters\":{\"color\":3,\"width\":625.851492623043,\"height\":465.2493344282225,\"content\":\"## Create Article from Perplexity Research\"},\"typeVersion\":1},{\"id\":\"19b3ca66-5fd2-4d04-b25a-a17fb38642f8\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1240,1360],\"parameters\":{\"color\":4,\"width\":479.02028317328745,\"height\":464.14912719677955,\"content\":\"## Convert Article into HTML\"},\"typeVersion\":1},{\"id\":\"7fad54e8-5a50-42da-b38d-08f6912615ab\",\"name\":\"gpt-4o-mini\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1380,1660],\"parameters\":{\"model\":\"gpt-4o-mini-2024-07-18\",\"options\":{\"responseFormat\":\"text\"}},\"credentials\":{\"openAiApi\":{\"id\":\"h597GY4ZJQD47RQd\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"5291869f-3ac6-4ce2-88f3-b572924b6082\",\"name\":\"gpt-4o-mini1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1560,1040],\"parameters\":{\"options\":{\"topP\":1,\"timeout\":60000,\"max"}
{"prompt": "Create an n8n workflow for: Query Perplexity AI from your n8n workflows This workflow illustrates how to use Perplexity AI in your n8n workflow. Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question. Credentials Setup 1/ Go to the perplexity dashboard, purchase some credits and create an API Key https://www.perplexity.ai/settings/api 2/ In the perplexity Request node, use Generic Credentials, Header Auth. For the name, use the value \"Authorization\" And for the value \"Bearer pplx-e4...59ea\" (Your Perplexity Api Key) AI Model Sonar Pro is the current top model used by perplexity. If you want to use a different one, check this page: https://docs.perplexity.ai/guides/model-cards Emmanuel Bernard | 1 month ago | Free", "completion": "{\"nodes\":[{\"id\":\"293b70f0-06e8-4db5-befd-bfaed1f3575a\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-460,80],\"parameters\":{},\"typeVersion\":1},{\"id\":\"1c473546-6280-412d-9f8e-b43962365d78\",\"name\":\"Set Params\",\"type\":\"n8n-nodes-base.set\",\"position\":[-160,-60],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"8b5c6ca0-5ca8-4f67-abc1-44341cf419bc\",\"name\":\"system_prompt\",\"type\":\"string\",\"value\":\"You are an n8n fanboy.\"},{\"id\":\"7c36c362-6269-4564-b6fe-f82126bc8f5e\",\"name\":\"user_prompt\",\"type\":\"string\",\"value\":\"What are the differences between n8n and Make?\"},{\"id\":\"4366d2b5-ad22-445a-8589-fddab1caa1ab\",\"name\":\"domains\",\"type\":\"string\",\"value\":\"n8n.io, make.com\"}]}},\"typeVersion\":3.4},{\"id\":\"894bd6a4-5db7-45fb-a8e0-1a81af068bbf\",\"name\":\"Clean Output\",\"type\":\"n8n-nodes-base.set\",\"position\":[580,-100],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"5859093c-6b22-41db-ac6c-9a9f6f18b7e3\",\"name\":\"output\",\"type\":\"string\",\"value\":\"={{ $json.choices[0].message.content }}\"},{\"id\":\"13208fff-5153-45a7-a1cb-fe49e32d9a03\",\"name\":\"citations\",\"type\":\"array\",\"value\":\"={{ $json.citations }}\"}]}},\"typeVersion\":3.4},{\"id\":\"52d3a832-8c9b-4356-ad2a-377340678a58\",\"name\":\""}
{"prompt": "Create an n8n workflow for: Split Test Different Agent Prompts with Supabase and OpenAI Split Test Agent Prompts with Supabase and OpenAI Use Case Oftentimes, it's useful to test different settings for a large language model in production against various metrics. Split testing is a good method for doing this. What it Does This workflow randomly assigns chat sessions to one of two prompts, the baseline and the alternative. The agent will use the same prompt for all interactions in that chat session. How it Works When messages arrive, a table containing information regarding session ID and which prompt to use is checked to see if the chat already exists If it does not, the session ID is added to the table and a prompt is randomly assigned These values are then used to generate a response Setup Create a table in Supabase called split_test_sessions. It needs to have the following columns: session_id (text) and show_alternative (bool) Add your Supabase, OpenAI, and PostgreSQL credentials Modify the Define Path Values node to set the baseline and alternative prompt values. Activate the workflow and test by sending messages through n8n's inbuilt chat Experiment with different chat sessions to test see both prompts in action Next Steps Modify the workflow to test different LLM settings such as temperature Add a method to measure the efficacy of the two alternative prompts Chris Carr | 8 days ago | Free", "completion": "{\"id\":\"TEA7K9MSVQGCWKe6\",\"meta\":{\"instanceId\":\"ac63467607103d9c95dd644384984672b90b1cb03e07edbaf18fe72b2a6c45bb\",\"templateCredsSetupCompleted\":true},\"name\":\"A/B Split Testing\",\"tags\":[],\"nodes\":[{\"id\":\"e8404493-4297-4169-a72f-89e668ae5fbc\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-1460,-140],\"webhookId\":\"334e3a8d-73d2-4d3c-9927-158c1169ef5e\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"582e1c1b-12ff-42ff-8130-48f94eebd706\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[220,-160],\"parameters\":{\"text\":\"={{ $('When chat message received').item.json.chatInput }}\",\"options\":{\"systemMessage\":\"={{ $json.prompt }}\"},\"promptType\":\"define\"},\"typeVersion\":1.7},{\"id\":\"39ca5c70-11d4-4f86-bde5-0f9827297be9\",\"name\":\"Check If Session Exists\",\"type\":\"n8n-nodes-base.supabase\",\"position\":[-960,-140],\"parameters\":{\"filters\":{\"conditions\":[{\"keyName\":\"session_id\",\"keyValue\":\"={{ $('When chat message received').item.json.sessionId }}\"}]},\"tableId\":\"split_test_sessions\",\"operation\":\"get\"},\"credentials\":{\"supabaseApi\":{\"id\":\"1iEg1EzFrF29iqp2\",\"name\":\"Supabase (bsde.ai)\"}},\"executeOnce\":false,\"typeVersion\":1,\"alwaysOutputData\":true},{\"id\":\"35f2c270-9571-41ba-ab7c-47a6742d7d90\",\"name\":\"If Session Does Exist\",\"type\":\"n8n-nodes-base.if\",\"position\":[-720,-140],\"parameters\":{\""}
{"prompt": "Create an n8n workflow for: Building Your First WhatsApp Chatbot This n8n template builds a simple WhatsApp chabot acting as a Sales Agent. The Agent is backed by a product catalog vector store to better answer user's questions. This template is intended to help introduce n8n users interested in building with WhatsApp. How it works This template is in 2 parts: creating the product catalog vector store and building the WhatsApp AI chatbot. A product brochure is imported via HTTP request node and its text contents extracted. The text contents are then uploaded to the in-memory vector store to build a knowledgebase for the chatbot. A WhatsApp trigger is used to capture messages from customers where non-text messages are filtered out. The customer's message is sent to the AI Agent which queries the product catalogue using the vector store tool. The Agent's response is sent back to the user via the WhatsApp node. How to use Once you've setup and configured your WhatsApp account and credentials First, populate the vector store by clicking the \"Test Workflow\" button. Next, activate the workflow to enable the WhatsApp chatbot. Message your designated WhatsApp number and you should receive a message from the AI sales agent. Tweak datasource and behaviour as required. Requirements WhatsApp Business Account OpenAI for LLM Customising this workflow Upgrade the vector store to Qdrant for persistance and production use-cases. Handle different WhatsApp message types for a more rich and engaging experience for customers. Jimleuk | 4 months ago | Free +5", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"77ee6494-4898-47dc-81d9-35daf6f0beea\",\"name\":\"WhatsApp Trigger\",\"type\":\"n8n-nodes-base.whatsAppTrigger\",\"position\":[1360,-280],\"webhookId\":\"aaa71f03-f7af-4d18-8d9a-0afb86f1b554\",\"parameters\":{\"updates\":[\"messages\"]},\"credentials\":{\"whatsAppTriggerApi\":{\"id\":\"H3uYNtpeczKMqtYm\",\"name\":\"WhatsApp OAuth account\"}},\"typeVersion\":1},{\"id\":\"57210e27-1f89-465a-98cc-43f890a4bf58\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1960,-200],\"parameters\":{\"model\":\"gpt-4o-2024-08-06\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"e1053235-0ade-4e36-9ad2-8b29c78fced8\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[2080,-200],\"parameters\":{\"sessionKey\":\"=whatsapp-75-{{ $json.messages[0].from }}\",\"sessionIdType\":\"customKey\"},\"typeVersion\":1.2},{\"id\":\"69f1b78b-7c93-4713-863a-27e04809996f\",\"name\":\"Vector Store Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolVectorStore\",\"position\":[2200,-200],\"parameters\":{\"name\":\"query_product_brochure\",\"description\":\"Call this tool to query the product brochure. Valid for the year 2024.\"},\"typeVersion\":1},{\"id\":\"170e8"}
{"prompt": "Create an n8n workflow for: Scrape Trustpilot Reviews with DeepSeek, Analyze Sentiment with OpenAI Workflow Overview This workflow automates the process of scraping Trustpilot reviews, extracting key details, analyzing sentiment, and saving the results to Google Sheets. It uses OpenAI for sentiment analysis and HTML parsing for review extraction. How It Works 1. Scrape Trustpilot Reviews HTTP Request**: Fetches review pages from Trustpilot (https://it.trustpilot.com/review/{{company_id}}). Paginates through pages (up to max_page limit). HTML Parsing**: Extracts review URLs using CSS selectors Splits the URLs into individual review links. 2. Extract Review Details Information Extractor**: Uses DeepSeek to extract structured data from the review: Author: Name of the reviewer. Rating: Numeric rating (1-5). Date: Review date in YYYY-MM-DD format. Title: Review title. Text: Full review text. Total Reviews: Number of reviews by the user. Country: Reviewer\u2019s country (2-letter code). 3. Sentiment Analysis Sentiment Analysis Node**: Uses OpenAI to classify the review text as Positive, Neutral, or Negative. Example output: { \"category\": \"Positive\", \"confidence\": 0.95 } 4. Save to Google Sheets Google Sheets Node**: Appends or updates the extracted data to a Google Sheet Set Up Steps 1. Configure Trustpilot Scraping Edit Fields1 Node**: Set company_id to the Trustpilot company name Set max_page to limit the number of pages scraped. 2. Configure Google Sheets Google Sheets Node**: Update the documentId with your Google Sheet ID Ensure the sheet has the required columns (Id, Data, Nome, etc.). 3. Configure OpenAI OpenAI Chat Model Node**: Add your OpenAI API key. Sentiment Analysis Node**: Ensure the categories match your desired sentiment labels (Positive, Neutral, Negative). Key Components Nodes**: HTTP Request/HTML: Scrape and parse Trustpilot reviews. Information Extractor: Extract structured review data using DeepSeek. Sentiment Analysis: Classify review sentiment. Google Sheets: Save and update review data. Credentials**: OpenAI API key. DeepSeek API key. Google Sheets OAuth2. n3w Italia | 1 month ago | Free", "completion": "{\"id\":\"w434EiZ2z7klQAyp\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Scrape Trustpilot Reviews with DeepSeek, Analyze Sentiment with OpenAI\",\"tags\":[{\"id\":\"2VG6RbmUdJ2VZbrj\",\"name\":\"Google Drive\",\"createdAt\":\"2024-12-04T16:50:56.177Z\",\"updatedAt\":\"2024-12-04T16:50:56.177Z\"},{\"id\":\"paTcf5QZDJsC2vKY\",\"name\":\"OpenAI\",\"createdAt\":\"2024-12-04T16:52:10.768Z\",\"updatedAt\":\"2024-12-04T16:52:10.768Z\"}],\"nodes\":[{\"id\":\"095a8e10-1630-4a1a-b6c9-7950ae1ed803\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[320,-380],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"recensioni\"},\"typeVersion\":1},{\"id\":\"6ff4dd9d-eedd-4d84-b13a-b3c0db717409\",\"name\":\"Information Extractor\",\"type\":\"@n8n/n8n-nodes-langchain.informationExtractor\",\"position\":[-440,140],\"parameters\":{\"text\":\"=You need to extract the review from the following HTML: {{ $json.recensione }}\",\"options\":{\"systemPromptTemplate\":\"You are a review expert. You need to extract only the required information and report it without changing anything.\\nAll the required information is in the text.\"},\"attributes\":{\"attributes\":[{\"name\":\"autore\",\"required\":true,\"description\":\"Extract the name of the review author\"},{\"name\":\"valutazione\",\"type\":\"number\",\"required\":true,\"description\":\"Extract the rating given to the review (from 1 to 5)\"},{\"name\":\"data\",\"required\":true,\"description\":\"Extract review date in YYYY-MM-DD format\"},{\"name\":\"titolo\",\"required\":true,\"description\":\"Extract the review title"}
{"prompt": "Create an n8n workflow for: Simple Expense Tracker with n8n Chat, AI Agent and Google Sheets Use Case It is very convenient to add expenses via simple chat message. This workflow attempts to do exactly this using AI-powered n8n magic! Send message to a chat, something like \"car wash; 59.3 usd; 25 jan 2024\" And get a response: Your expense saved, here is the output of save sub-workflow:{\"cost\":59.3,\"descr\":\"car wash\",\"date\":\"2024-01-25\",\"msg\":\"car wash; 59.3 usd; 25 jan 2024\"} LLM will smartly parse your message to structured JSON and save the expense as a new row into Google Sheet! Installation 1. Set up Google Sheets: Clone this Sheet: https://docs.google.com/spreadsheets/d/1D0r3tun7LF7Ypb21CmbTKEtn76WE-kaHvBCM5NdgiPU/edit?gid=0#gid=0 (File -&gt; Make a copy) Choose this sheet into \"Save expense into Google Sheets\" node. 2. Fix sub-workflow dropdown: open \"Parse msg and save to Sheets\" node (which is an n8n sub-workflow executor tool) and make sure the SAME workflow is chosen in the dropdown. it will allow n8n to locate and call \"Workflow Input Trigger\" properly when needed. 3. Activate the workflow to make chat work properly. Sent message to chat, something like \"car wash; 59.3 usd; 25 jan 2024\" you should get a response: Your expense saved, here is the output of save sub-workflow:{\"cost\":59.3,\"descr\":\"car wash\",\"date\":\"2024-01-25\",\"msg\":\"car wash; 59.3 usd; 25 jan 2024\"} and new row in Google sheets should be inserted! Anthony | 1 month ago | Free", "completion": "{\"id\":\"aLTkMiEDYXbMK4fT\",\"meta\":{\"instanceId\":\"5b860a91d7844b5237bb51cc58691ca8c3dc5b576f42d4d6bbedfb8d43d58ece\",\"templateCredsSetupCompleted\":true},\"name\":\"AI agent: expense tracker in Google Sheets and n8n chat\",\"tags\":[],\"nodes\":[{\"id\":\"9260b53e-6848-4f34-9643-311c58c807f6\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[360,40],\"parameters\":{\"options\":{\"maxIterations\":3,\"systemMessage\":\"You are a helpful accountant. Use save to db tool to save expense message to DB. respond with \\\"Your expense saved, here is the output of save sub-workflow:[data]\\\"\"}},\"typeVersion\":1.7},{\"id\":\"0d7a686c-42c2-4223-9f78-b454788fb6da\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[0,40],\"webhookId\":\"6a34ec84-459d-4cc4-83b6-06ae4c99dc8f\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"f1f27aaf-cf13-40d9-b8f9-800a862f8bf0\",\"name\":\"Workflow Input Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[180,600],\"parameters\":{\"workflowInputs\":{\"values\":[{\"name\":\"input1\"}]}},\"typeVersion\":1.1},{\"id\":\"a1530601-1a91-45be-adef-2e0608bfe773\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[340,300],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"vHFEeel4RHFsjcMI\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\""}
{"prompt": "Create an n8n workflow for: AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack Check Legal Regulations: This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry! \ud83d\udccc Purpose This workflow enables automated and AI-driven topic monitoring, delivering concise article summaries directly to a Slack channel in a structured and easy-to-read format. It allows users to stay informed on specific topics of interest effortlessly, without manually checking multiple sources, ensuring a time-efficient and focused monitoring experience. To get started, copy the Google Sheets template required for this workflow from here. \ud83c\udfaf Target Audience This workflow is designed for: Industry professionals** looking to track key developments in their field. Research teams** who need up-to-date insights on specific topics. Companies** aiming to keep their teams informed with relevant content. \u2699\ufe0f How It Works Trigger: A Scheduler initiates the workflow at regular intervals (default: every hour). Data Retrieval: RSS feeds are fetched using the RSS Read node. Previously monitored articles are checked in Google Sheets to avoid duplicates. Content Processing: The article relevance is assessed using OpenAI (GPT-4o-mini). Relevant articles are scraped using Jina AI to extract content. Summaries are generated and formatted for Slack. Output: Summaries are posted to the specified Slack channel. Article metadata is stored in Google Sheets for tracking. \ud83d\udee0\ufe0f Key APIs and Nodes Used Scheduler Node:** Triggers the workflow periodically. RSS Read:** Fetches the latest articles from defined RSS feeds. Google Sheets:** Stores monitored articles and manages feed URLs. OpenAI API (GPT-4o-mini):** Classifies article relevance and generates summaries. Jina AI API:** Extracts the full content of relevant articles. Slack API:** Posts formatted messages to Slack channels. This workflow provides an efficient and intelligent way to stay informed about your topics of interest, directly within Slack. Dataki | 1 month ago | Free +2", "completion": "{\"id\":\"Xk0W98z9DVrNHeku\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\",\"templateCredsSetupCompleted\":true},\"name\":\"AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack\",\"tags\":[],\"nodes\":[{\"id\":\"704de862-43e5-4322-ae35-45b505e68bb6\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[4220,380],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"\",\"name\":\"OpenAi Connection\"}},\"typeVersion\":1.1},{\"id\":\"eaae54b0-0500-47a7-ad8f-097e0882d21c\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[4180,-120],\"parameters\":{\"text\":\"={{ $json.data }}\",\"messages\":{\"messageValues\":[{\"message\":\"=You are an AI assistant responsible for summarizing articles **in English** and formatting them into Slack-compatible messages. \\nYour job is to create a clear and concise summary following the guidelines below and format it in Slack-specific Markdown format. \\n\\n---\\n\\n## 1. Title with Link \\n\\n- Format the article title as a **clickable link** using Slack's Markdown syntax: \\n `<URL|*Title of the article*>`. \\n- The title should be clear and engaging to encourage readers to click. \\n\\n---\\n\\n## 2. Section Headings \\n\\n- Use **bold text** to introduce different sections of the summary by wrapping the text with `*` symbols. \\n- Ensure headings are descriptive and guide the reader through the content effectively. \\n\\n---\\n\\n## 3. Key Points \\n\\n- Present key insights using **bullet points**, using the `\u2022` symbol for listing important information. \\n- Each point should be concise"}
{"prompt": "Create an n8n workflow for: AI Agent To Chat With Files In Supabase Storage Video Guide I prepared a detailed guide explaining how to set up and implement this scenario, enabling you to chat with your documents stored in Supabase using n8n. Youtube Link Who is this for? This workflow is ideal for researchers, analysts, business owners, or anyone managing a large collection of documents. It's particularly beneficial for those who need quick contextual information retrieval from text-heavy files stored in Supabase, without needing additional services like Google Drive. What problem does this workflow solve? Manually retrieving and analyzing specific information from large document repositories is time-consuming and inefficient. This workflow automates the process by vectorizing documents and enabling AI-powered interactions, making it easy to query and retrieve context-based information from uploaded files. What this workflow does The workflow integrates Supabase with an AI-powered chatbot to process, store, and query text and PDF files. The steps include: Fetching and comparing files to avoid duplicate processing. Handling file downloads and extracting content based on the file type. Converting documents into vectorized data for contextual information retrieval. Storing and querying vectorized data from a Supabase vector store. File Extraction and Processing: Automates handling of multiple file formats (e.g., PDFs, text files), and extracts document content. Vectorized Embeddings Creation: Generates embeddings for processed data to enable AI-driven interactions. Dynamic Data Querying: Allows users to query their document repository conversationally using a chatbot. Setup N8N Workflow Fetch File List from Supabase: Use Supabase to retrieve the stored file list from a specified bucket. Add logic to manage empty folder placeholders returned by Supabase, avoiding incorrect processing. Compare and Filter Files: Aggregate the files retrieved from storage and compare them to the existing list in the Supabase files table. Exclude duplicates and skip placeholder files to ensure only unprocessed files are handled. Handle File Downloads: Download new files using detailed storage configurations for public/private access. Adjust the storage settings and GET requests to match your Supabase setup. File Type Processing: Use a Switch node to target specific file types (e.g., PDFs or text files). Employ relevant tools to process the content: For PDFs, extract embedded content. For text files, directly process the text data. Content Chunking: Break large text data into smaller chunks using the Text Splitter node. Define chunk size (default: 500 tokens) and overlap to retain necessary context across chunks. Vector Embedding Creation: Generate vectorized embeddings for the processed content using OpenAI's embedding tools. Ensure metadata, such as file ID, is included for easy data retrieval. Store Vectorized Data: Save the vectorized information into a dedicated Supabase vector store. Use the default schema and table provided by Supabase for seamless setup. AI Chatbot Integration: Add a chatbot node to handle user input and retrieve relevant document chunks. Use metadata like file ID for targeted queries, especially when multiple documents are involved. Testing Upload sample files to your Supabase bucket. Verify if files are processed and stored successfully in the vector store. Ask simple conversational questions about your documents using the chatbot (e.g., \"What does Chapter 1 say about the Roman Empire?\"). Test for accuracy and contextual relevance of retrieved results. Mark Shcherbakov | 2 months ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"6a2a7715680b8313f7cb4676321c5baa46680adfb913072f089f2766f42e43bd\"},\"nodes\":[{\"id\":\"f577f6bd-b1a4-48ec-9329-7bccc3fc1463\",\"name\":\"Get All files\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[400,-100],\"parameters\":{\"url\":\"=https://yqtvdcvjboenlblgcivl.supabase.co/storage/v1/object/list/private\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"prefix\\\": \\\"\\\",\\n \\\"limit\\\": 100,\\n \\\"offset\\\": 0,\\n \\\"sortBy\\\": {\\n \\\"column\\\": \\\"name\\\",\\n \\\"order\\\": \\\"asc\\\"\\n }\\n}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"supabaseApi\"},\"credentials\":{\"supabaseApi\":{\"id\":\"t8AQJzvZvrOMDLec\",\"name\":\"Supabase account My Airtable Gen\"}},\"typeVersion\":4.2},{\"id\":\"10693bc8-560d-4cf6-8bd0-2fe3f4d863d1\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[1780,100],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"=file_id\",\"value\":\"={{ $json.id }}\"}]}},\"jsonData\":\"={{ $('Merge').item.json.data ?? $('Merge').item.json.text }}\",\"jsonMode\":\"expressionData\"},\"typeVersion\":1},{\"id\":\"49428060-e707-4269-8344-77b301f56f7c\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[1780,280],\"parameters\":{\"options\":{},\"chunkSize\":500,\"chunkOverlap\":200},\"typeVersion\":1},{\"id\":\"08742063-e235-"}
{"prompt": "Create an n8n workflow for: Proxmox AI Agent with n8n and Generative AI Integration Proxmox AI Agent with n8n and Generative AI Integration This template automates IT operations on a Proxmox Virtual Environment (VE) using an AI-powered conversational agent built with n8n. By integrating Proxmox APIs and generative AI models (e.g., Google Gemini), the workflow converts natural language commands into API calls, enabling seamless management of your Proxmox nodes, VMs, and clusters. Watch Video on Youtube How It Works Trigger Mechanism The workflow can be triggered through multiple channels like chat (Telegram, email, or n8n's built-in chat). Interact with the AI agent conversationally. AI-Powered Parsing A connected AI model (Google Gemini or other compatible models like OpenAI or Claude) processes your natural language input to determine the required Proxmox API operation. API Call Generation The AI parses the input and generates structured JSON output, which includes: response_type: The HTTP method (GET, POST, PUT, DELETE). url: The Proxmox API endpoint to execute. details: Any required payload parameters for the API call. Proxmox API Execution The structured output is used to make HTTP requests to the Proxmox VE API. The workflow supports various operations, such as: Retrieving cluster or node information. Creating, deleting, starting, or stopping VMs. Migrating VMs between nodes. Updating or resizing VM configurations. Response Formatting The workflow formats API responses into a user-friendly summary. For example: Success messages for operations (e.g., \"VM started successfully\"). Error messages with missing parameter details. Extensibility You can enhance the workflow by connecting additional triggers, external services, or AI models. It supports: Telegram/Slack integration for real-time notifications. Backup and restore workflows. Cloud monitoring extensions. Key Features Multi-Channel Input**: Use chat, email, or custom triggers to communicate with the AI agent. Low-Code Automation**: Easily customize the workflow to suit your Proxmox environment. Generative AI Integration**: Supports advanced AI models for precise command interpretation. Proxmox API Compatibility**: Fully adheres to Proxmox API specifications for secure and reliable operations. Error Handling**: Detects and informs you of missing or invalid parameters in your requests. Example Use Cases Create a Virtual Machine Input: \"Create a VM with 4 cores, 8GB RAM, and 50GB disk on psb1.\" Action: Sends a POST request to Proxmox to create the VM with specified configurations. Start a VM Input: \"Start VM 105 on node psb2.\" Action: Executes a POST request to start the specified VM. Retrieve Node Details Input: \"Show the memory usage of psb3.\" Action: Sends a GET request and returns the node's resource utilization. Migrate a VM Input: \"Migrate VM 202 from psb1 to psb3.\" Action: Executes a POST request to move the VM with optional online migration. Pre-Requisites Proxmox API Configuration Enable the Proxmox API and generate API keys in the Proxmox Data Center. Use the Authorization header with the format: PVEAPIToken=&lt;user&gt;@&lt;realm&gt;!&lt;token-id&gt;=&lt;token-value&gt; n8n Setup Add Proxmox API credentials in n8n using Header Auth. Connect a generative AI model (e.g., Google Gemini) via the relevant credential type. Access the Workflow Import this template into your n8n instance. Replace placeholder credentials with your Proxmox and AI service details. Additional Notes This template is designed for Proxmox 7.x and above. For advanced features like backup, VM snapshots, and detailed node monitoring, you can extend this workflow. Always test with a non-production Proxmox environment before deploying in live systems. Amjid Ali | 1 month ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"n8n.syncbricks.com\"},\"nodes\":[{\"id\":\"e6d85380-7cfa-4c6e-9b0f-d390ad0cbc67\",\"name\":\"HTTP Request1\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1400,-180],\"parameters\":{\"url\":\"=https://proxmox.syncbricks.com/api2/json{{ $json.output.url }}\",\"method\":\"=POST\",\"options\":{\"allowUnauthorizedCerts\":true},\"jsonBody\":\"={{ $json.output.details }}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"pJcVQegRQ5mpraoQ\",\"name\":\"Proxmox\"}},\"typeVersion\":4.2},{\"id\":\"9b497de8-0f01-40b1-8f8e-28fad1f758c4\",\"name\":\"Proxmox API Documentation\",\"type\":\"@n8n/n8n-nodes-langchain.toolHttpRequest\",\"position\":[-300,40],\"parameters\":{\"url\":\"https://pve.proxmox.com/pve-docs/api-viewer/index.html\",\"toolDescription\":\"This is Proxmox API Documentation ensure to read the details from here\"},\"typeVersion\":1.1},{\"id\":\"e7ac54a9-37be-44b5-b58e-8b631892367e\",\"name\":\"Auto-fixing Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserAutofixing\",\"position\":[40,60],\"parameters\":{\"options\":{\"prompt\":\"Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:\"}},\"typeVersion\":1},{\"id\":\"5d8c8c6d-d5de"}
{"prompt": "Create an n8n workflow for: Use OpenRouter in n8n versions <1.78 What it is: In version 1.78, n8n introduced a dedicated node to use the OpenRouter service, which lets you to use a lot of different LLM models and providers and change models on the fly in an agentic workflow. For prior n8n versions, there's a workaround to make OpenRouter accessible, by using the OpenAI node with a OpenRouter-specific BaseURL. This trivial workflow demonstrates this for version before 1.78, so that you can use different LLM model dynamically with the available n8n nodes for OpenAI LLM and OpenAI credentials. What you can do: Use any of the OpenRouter models Have the model even dynamically configured or changing (by some external config, some rule, or some specific chat message) Setup steps: Import the workflow Ensure you have registered and account, purchased some credits and created and API key for OpenRouter.ai Configure the \"OpenRouter\" credentials with your own credentials, using an OpenAI type credential, but making sure in the credential's config form its \"Base URL\" is set to https://openrouter.ai/api/v1 so OpenRouter is used instead of OpenAI. Open the \"Settings\" node and change the model value to any valid model id from the OpenRouter models list or even have the model property set dynamically Daniel Nolde | 19 days ago | Free", "completion": "{\"id\":\"VhN3CX6QPBkX77pZ\",\"meta\":{\"instanceId\":\"98bf0d6aef1dd8b7a752798121440fb171bf7686b95727fd617f43452393daa3\",\"templateCredsSetupCompleted\":true},\"name\":\"Use any LLM-Model via OpenRouter\",\"tags\":[{\"id\":\"uumvgGHY5e6zEL7V\",\"name\":\"Published Template\",\"createdAt\":\"2025-02-10T11:18:10.923Z\",\"updatedAt\":\"2025-02-10T11:18:10.923Z\"}],\"nodes\":[{\"id\":\"b72721d2-bce7-458d-8ff1-cc9f6d099aaf\",\"name\":\"Settings\",\"type\":\"n8n-nodes-base.set\",\"position\":[-420,-640],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"3d7f9677-c753-4126-b33a-d78ef701771f\",\"name\":\"model\",\"type\":\"string\",\"value\":\"deepseek/deepseek-r1-distill-llama-8b\"},{\"id\":\"301f86ec-260f-4d69-abd9-bde982e3e0aa\",\"name\":\"prompt\",\"type\":\"string\",\"value\":\"={{ $json.chatInput }}\"},{\"id\":\"a9f65181-902d-48f5-95ce-1352d391a056\",\"name\":\"sessionId\",\"type\":\"string\",\"value\":\"={{ $json.sessionId }}\"}]}},\"typeVersion\":3.4},{\"id\":\"a4593d64-e67a-490e-9cb4-936cc46273a0\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-460,-740],\"parameters\":{\"width\":180,\"height\":400,\"content\":\"## Settings\\nSpecify the model\"},\"typeVersion\":1},{\"id\":\"3ea3b09a-0ab7-4e0f-bb4f-3d807d072d4e\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":["}
{"prompt": "Create an n8n workflow for: Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API. You could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use. How it works: Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository. Chunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning. Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database. Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model. Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding. Response Generation: The retrieved chunks and your original question are fed to OpenAI's gpt-4o-mini LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable. Set up steps: Create accounts: You'll need accounts with OpenAI and Pinecone. API keys: Obtain API keys for both services. Configure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys. Import the workflow: Import this workflow into your n8n instance. Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box. Setup Time: Approximately 15-20 minutes. Why use this workflow? Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot. Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases. n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs. Mihai Farcas | 1 month ago | Free +4", "completion": "{\"id\":\"FD0bHNaehP3LzCNN\",\"meta\":{\"instanceId\":\"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"},\"name\":\"Chat with GitHub OpenAPI Specification using RAG (Pinecone and OpenAI)\",\"tags\":[],\"nodes\":[{\"id\":\"362cb773-7540-4753-a401-e585cdf4af8a\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"45470036-cae6-48d0-ac66-addc8999e776\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[300,0],\"parameters\":{\"url\":\"https://raw.githubusercontent.com/github/rest-api-description/refs/heads/main/descriptions/api.github.com/api.github.com.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"a9e65897-52c9-4941-bf49-e1a659e442ef\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[520,0],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n-demo\",\"cachedResultName\":\"n8n-demo\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"c2a2354b-5457-4ceb-abfc-9a58e8593b81\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[660,180],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"7338d9ea-ae8f-46eb-807f-a15dc7639fc9\",\""}
{"prompt": "Create an n8n workflow for: Personal Shopper Chatbot for WooCommerce with RAG using Google Drive and openAI This workflow combines OpenAI, Retrieval-Augmented Generation (RAG), and WooCommerce to create an intelligent personal shopping assistant. It handles two scenarios: Product Search: Extracts user intent (keywords, price ranges, SKUs) and fetches matching products from WooCommerce. General Inquiries: Answers store-related questions (e.g., opening hours, policies) using RAG and documents stored in Google Drive. How It Works 1. Chat Interaction & Intent Detection Chat Trigger**: Starts when a user sends a message (\"When chat message received\"). Information Extractor**: Uses OpenAI to analyze the message and determine if the user is searching for a product or asking a general question. Extracts: search (true/false). keyword, priceRange, SKU, category (if product-related). Example: { \"search\": true, \"keyword\": \"red handbags\", \"priceRange\": { \"min\": 50, \"max\": 100 }, \"SKU\": \"BAG123\", \"category\": \"women's accessories\" } 2. Product Search (WooCommerce Integration) AI Agent**: If search: true, routes the request to the personal_shopper tool. WooCommerce Node: Queries the WooCommerce store using extracted parameters (keyword, priceRange, SKU). Filters products in stock (stockStatus: \"instock\"). Returns matching products (e.g., \"red handbags under \u20ac100\"). 3. General Inquiries (RAG System) RAG Tool**: If search: false, uses the Qdrant Vector Store to retrieve store information from documents. Google Drive Integration: Documents (e.g., store policies, FAQs) are stored in Google Drive. Downloaded, split into chunks, and embedded into Qdrant for semantic search. OpenAI Chat Model: Generates answers based on retrieved documents (e.g., \"Our store opens at 9 AM\"). Set Up Steps 1. Configure the RAG System Google Drive Setup**: Upload store documents . Update the Google Drive2 node with your folder ID. Qdrant Vector Database**: Clean the collection (update Qdrant Vector Store node with your URL). Use Embeddings OpenAI to convert documents into vectors. 2. Configure OpenAI & WooCommerce OpenAI Credentials**: Add your API key to all OpenAI nodes (OpenAI Chat Model, Embeddings OpenAI, etc.). WooCommerce Integration**: Connect your WooCommerce store (credentials in the personal_shopper node). Ensure product data is synced and accessible. 3. Customize the AI Agent Intent Detection**: Modify the Information Extractor\u2019s system prompt to align with your store\u2019s terminology. RAG Responses**: Update the tool description to reflect your store\u2019s documents. Notes This template is ideal for e-commerce businesses needing a hybrid assistant for product discovery and customer support. n3w Italia | 1 month ago | Free +7", "completion": "{\"id\":\"fqQcmSdoVqnPeGHj\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"OpenAI Personal Shopper with RAG and WooCommerce\",\"tags\":[],\"nodes\":[{\"id\":\"635901e5-4afd-4c81-a63e-52f1b863a025\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-200,280],\"webhookId\":\"bd3a878c-50b0-4d92-906f-e768a65c1485\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"d11cd97c-1539-462d-858c-8758cf1a8278\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[620,580],\"parameters\":{\"sessionKey\":\"={{ $('Edit Fields').item.json.sessionId }}\",\"sessionIdType\":\"customKey\"},\"typeVersion\":1.3},{\"id\":\"02bb43e4-f26e-4906-8049-c49d3fecd817\",\"name\":\"Calculator\",\"type\":\"@n8n/n8n-nodes-langchain.toolCalculator\",\"position\":[760,580],\"parameters\":{},\"typeVersion\":1},{\"id\":\"ad6058dd-b429-4f3c-b68a-7e3d98beec83\",\"name\":\"Edit Fields\",\"type\":\"n8n-nodes-base.set\",\"position\":[20,280],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"7015c229-f9fe-4c77-b2b9-4ac09a3a3cb1\",\"name\":\"sessionId\",\"type\":\"string\",\"value\":\"={{ $json.sessionId }}\"},{\"id\":\"f8fc0044-6a1a-455b-a435-58931a8c4c8e\",\"name\":\"chatInput\",\"type"}
{"prompt": "Create an n8n workflow for: Extract license plate number from image uploaded via an n8n form What it does This is a simplistic demo workflow showing how to extract a license plate number from an image of a car submitted via a form \u2013 or in more general terms showcasing how you can: use a form trigger to upload files and feed it into an LLM use a changeable LLM model for image-to-text analysis Set up steps Import the workflow Ensure you have registered and account, purchased some credits and created and API key for OpenRouter.ai Create/adapt the OpenRouter credential with your indivial API key for OpenRouter \"Test workflow\" and submit an image of a car with license plate to extract its number How to adapt By changing the \"prompt\" in th \"Settings\" node you can quickly adapt this exemplatory workflow to other image-to-text use cases, such as: summarization: \"summarize what's seen in the image\" location finding: \"identify the location where the image was taken\" text extraction: \"extract all text from the image and return it as markdown\" Thanks to using OpenRouter, you also can quickly experiment with finding good model choices by simply changing the \"model\" in the \"Settings\" node. The following models gave good results for this demo use-case: google/gemini-2.0-flash-001 meta-llama/llama-3.2-90b-vision-instruct openai/gpt-4o The llama-3.2-11b and even claude-3.5-sonnet didn't recognize all characters in all test images. Using a generic LLM-model offers a quick way of prototyping an image-to-text application. For specific use cases in serious and scalable production deployments, consider using an API based service specifically made to that purpose, such as: Google Cloud Vision API Microsoft Azure Computer Vision Azure AI Document Intelligence Amazon Textract Daniel Nolde | 17 days ago | Free", "completion": "{\"id\":\"B37wvB0tdKgjuabw\",\"meta\":{\"instanceId\":\"98bf0d6aef1dd8b7a752798121440fb171bf7686b95727fd617f43452393daa3\",\"templateCredsSetupCompleted\":true},\"name\":\"Image to license plate number\",\"tags\":[],\"nodes\":[{\"id\":\"a656334a-0135-4d93-a6df-ca97222c9753\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[-140,-380],\"parameters\":{\"text\":\"={{ $json.prompt }}\",\"messages\":{\"messageValues\":[{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"Image\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"41a90592-2a91-40ff-abf4-3a795733d521\",\"name\":\"FormResultPage\",\"type\":\"n8n-nodes-base.form\",\"position\":[220,-380],\"webhookId\":\"218822fe-5eb9-4451-ae8a-14b8f484fdde\",\"parameters\":{\"options\":{\"formTitle\":\"\"},\"operation\":\"completion\",\"completionTitle\":\"Extracted information:\",\"completionMessage\":\"={{ $json.text }}\"},\"typeVersion\":1},{\"id\":\"c23b95d9-b7a2-4e9e-a019-5724a9662abd\",\"name\":\"OpenRouter LLM\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\"position\":[-60,-180],\"parameters\":{\"model\":\"={{ $json.model }}\",\"options\":{}},\"credentials\":{\"openRouterApi\":{\"id\":\"bs7tPtvgDTJNGAFJ\",\"name\":\"OpenRouter account\"}},\"typeVersion\":1},{\"id\":\"8298cd51-8c47-4bc4-af78-2c216207ef76\",\"name\":\"Settings\",\"type\":\"n8n-nodes-base.set\",\"position\":[-340,-380],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\""}
{"prompt": "Create an n8n workflow for: AI Youtube Trend Finder Based On Niche Youtube Video This n8n workflow is designed to assist YouTube content creators in identifying trending topics within a specific niche. By leveraging YouTube's search and data APIs, it gathers and analyzes video performance metrics from the past two days to provide insights into what content is gaining traction. Here's how the workflow operates: Trigger Setup: The workflow begins when a user sends a query through the chat_message_received node. If no niche is provided, the AI prompts the user to select or input one. AI Agent (Language Model): The central node utilizes a GPT-based AI agent to: Understand the user's niche or content preferences. Generate tailored search terms related to the niche. Process YouTube API responses and summarize trends using insights such as common themes, tags, and audience engagement metrics (views, likes, and comments). YouTube Search: The youtube_search node runs a secondary workflow to query YouTube for relevant videos published within the last two days. It retrieves basic video data such as video IDs, relevance scores, and publication dates. Video Details Retrieval: The workflow fetches additional details for each video: Video Snippet: Metadata like title, description, and tags. Video Statistics: Metrics such as views, likes, and comments. Content Details: Video duration, ensuring only content longer than 3 minutes and 30 seconds is analyzed. Data Processing: Video metadata is cleaned, sanitized, and stored in memory. Tags, titles, and descriptions are analyzed to identify patterns and trends across multiple videos. Output: The workflow compiles insights and presents them to the user, highlighting: The most common themes or patterns within the niche. URLs to trending videos and their respective channels. Engagement statistics, helping the user understand the popularity of the content. Key Notes for Setup: API Keys**: Ensure valid YouTube API credentials are configured in the get_videos, find_video_snippet, find_video_statistics, and find_video_data nodes. Memory Buffer**: The window_buffer_memory node ensures the AI agent retains context during analysis, enhancing the quality of the generated insights. Search Term Customization**: The AI agent dynamically creates search terms based on the user\u2019s niche to improve search precision. Use Case: This workflow is ideal for YouTubers or marketers seeking data-driven inspiration for creating content that aligns with current trends, maximizing the potential to engage their audience. Example Output: For the niche \"digital marketing\": Trending Topic: Videos about \"mental triggers\" and \"psychological marketing.\" Tags: \"SEO,\" \"Conversion Rates,\" \"Social Proof.\" Engagement: Videos with over 200K views and high likes/comment ratios are leading trends. Video links: https://www.youtube.com/watch?v=video_id1 https://www.youtube.com/watch?v=video_id2 Leonardo Grigorio | 2 months ago | Free +2", "completion": "{\"id\":\"XSyVFC1tsGSxNwX9\",\"meta\":{\"instanceId\":\"60ad864624415060d2d0a0e71acd8b3b40e4ee2e9ef4b439d9937d3d33537a96\"},\"name\":\"Complete Youtube\",\"tags\":[],\"nodes\":[{\"id\":\"fd74706b-609b-4723-b4a4-067e1b064194\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[300,60],\"parameters\":{\"options\":{\"systemMessage\":\"=You help youtube creators find trending videos based on a specific niche.\\n\\nVerify if the user informed a niche before doing anything. If not, then ask him for one by giving him suggestions for him to select from.\\n\\nAfter you know what type of content the user might produce, use the \\\"youtube_search\\\" tool up to 3 times with different search terms based on the user's content type and niche.\\n\\nThe tool will answer with a list of videos from the last 2 days that had the most amount of relevancy. It returns a list of json's covering each video's id, view count, like count, comment count, description, channel title, tags and channel id. Each video is separated by \\\"### NEXT VIDEO FOUND: ###\\\".\\n\\nYou should then proceed to understand the data received then provide the user with insightful data of what could be trending from the past 2 days. Provide the user links to the trending videos which should be in this structure:\\n\\nhttps://www.youtube.com/watch?v={video_id}\\n\\nto reach the channel's link you should use:\\n\\nhttps://www.youtube.com/channel/{channel_id}\\n\\nFind patterns in the tags, titles and especially in the related content for the videos found.\\n\\nYour mission isn't to find the trending videos. It's to provide the user with valuable information of what is trending in that niche in terms of content news. Remember to provide the user with the numbers of views, likes and comments while commenting about any video. So you should not talk about any particular video, focus rather in explaining the overall senario of all that was found.\\n\\nExample of response:\\n"}
{"prompt": "Create an n8n workflow for: Slack slash commands AI Chat Bot This is a response chatbot in public channels through slash commands. I explain more in detail through the YouTube video, but it's only available in Korean. How it works? When you request the created slash command in Slack, the request comes to the webhook. Then, the Switch Node branches appropriately according to each slash command request. Here, a slash command called /ask is connected to the chatbot, and the chatbot generates answers to the questions asked. The final node responds to the channel. Set up steps Create a Slack app. Add chat:write permission in Slack OAuth&Permissions&gt;Scopes. Create a Command in Slack Slash Commands menu and enter the n8n Webhook node's URL. Complete creating the Slash Commands. Enter the created command in the Switch node. \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\ub97c \ud1b5\ud55c \uacf5\uac1c \ucc44\ub110\uc5d0\uc11c\uc758 \uc751\ub2f5 \ucc57\ubd07 \uc785\ub2c8\ub2e4. \uc720\ud29c\ube0c \uc601\uc0c1\uc5d0 \ub354 \uc790\uc138\ud558\uac8c \uc124\uba85 \ub4dc\ub9bd\ub2c8\ub2e4. \uc124\uba85 \uc2ac\ub799\uc5d0 \uc0dd\uc131\ud55c \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\ub97c \uc2ac\ub799\uc5d0\uc11c \uc694\uccad\ud558\uba74 \uc6f9\ud6c5\uc5d0 \uc694\uccad\uc774 \ub4e4\uc5b4\uc635\ub2c8\ub2e4. \uc774\ud6c4 Switch Node\uc5d0\uc11c \uac01 \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\uc758 \uc694\uccad\uc5d0 \ub530\ub77c \uc54c\ub9de\uac8c \ubd84\uae30\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 /ask\ub77c\ub294 \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\uac00 \ucc57\ubd07\uc73c\ub85c \uc5f0\uacb0\ub418\uc5b4 \uc788\uace0, \ucc57\ubd07\uc5d0\uc11c \uc9c8\ubb38\ud55c \ub0b4\uc6a9\uc758 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \ub178\ub4dc\uc5d0\uc11c \ucc44\ub110\ub85c \uc751\ub2f5\uc744 \ud569\ub2c8\ub2e4. \uc124\uc815 \ubc29\ubc95 Slack \uc571\uc744 \ub9cc\ub4dc\uc138\uc694. Slack OAuth&Permissions&gt;Scopes \uc5d0\uc11c chat:write \uad8c\ud55c\uc744 \ucd94\uac00\ud558\uc138\uc694. Slack Slash Commands \uba54\ub274\uc5d0\uc11c Command\ub97c \uc0dd\uc131\ud558\uace0, n8n Webhook \ub178\ub4dc\uc758 url\uc744 \uc785\ub825\ud558\uc138\uc694. Slash Slash Commands \uc0dd\uc131\uc744 \uc644\ub8cc\ud558\uc138\uc694. Switch \ub178\ub4dc\uc5d0 \uc0dd\uc131\ud55c \ucee4\ub9e8\ub4dc\ub97c \uc785\ub825\ud558\uc138\uc694. InfoGrab | 18 days ago | Free", "completion": "{\"id\":\"PGLFPj5y01s26rE1\",\"meta\":{\"instanceId\":\"b68f2515130d1ee83f4af1a6f2ca359fc9bb8cdbe875ca10b6f944f99aa931b5\",\"templateCredsSetupCompleted\":true},\"name\":\"My workflow 6\",\"tags\":[],\"nodes\":[{\"id\":\"82670f40-2e3b-4e02-ae52-f2c918c3aa1c\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-80,-600],\"parameters\":{\"color\":7,\"width\":280,\"height\":380,\"content\":\"## Command Trigger\\n\\nCopy the webhook URL, paste it into the Request URL of the Slack slash command, and complete the creation.\\n\\n\\n\uc6f9\ud6c5 URL\uc744 \ubcf5\uc0ac\ud558\uc5ec \uc2ac\ub799 \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\uc758 Request URL\uc5d0 \ubd99\uc774\uace0 \uc0dd\uc131\uc744 \uc644\ub8cc\ud558\uc138\uc694.\"},\"typeVersion\":1},{\"id\":\"28f56691-0ad5-47b1-974b-1ece4890933b\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[260,-600],\"parameters\":{\"color\":7,\"height\":380,\"content\":\"## Command Switch\\n\\nSwitch each slash command.\\n\\n\uac01 \uc2ac\ub798\uc2dc \ucee4\ub9e8\ub4dc\ub97c \ubd84\uae30\ud558\uc138\uc694.\"},\"typeVersion\":1},{\"id\":\"9dc9ca95-e29d-44d9-9e09-b2a72d9ad23a\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[600,-600],\"parameters\":{\"color\":7,\"width\":360,\"height\":380,\"content\":\"## Create AI Messages\"},\"typeVersion\":1},{\"id\":\"025c5a59-06b6-4b"}
{"prompt": "Create an n8n workflow for: AI agent for Instagram DM/inbox. Manychat + Open AI integration Automate Instagram DMs with OpenAI GPT and ManyChat How It Works: Once connected, GPT will automatically initiate conversations with messages from new recipients in Intagram. Who Is This For? This workflow is ideal for marketers, business owners content creators who want to automatically respond to Instagram direct messages using OpenAI GPT. By integrating ManyChat, you can manage conversations, nurture leads, and provide instant replies at scale. What This Workflow Does Captures** incoming Instagram DMs through ManyChat\u2019s integration. Processes** messages with GPT to generate a relevant response. Delivers** instant replies back to Instagram users, creating efficient, AI-driven communication. Setup Import the Template: Copy the n8n workflow into your workspace. OpenAI Credentials: Add your OpenAI API key in n8n so GPT can generate responses. ManyChat Account: Create (or log in to) your ManyChat account. Connect Instagram: Link your Instagram profile as a channel in ManyChat. ManyChat Custom Field: Create a custom field for storing user input or conversation context. Configure Default Reply: In ManyChat, set up the default Instagram reply flow to point to your n8n webhook. Add External Request: Create an external request step in ManyChat to send messages to n8n. Test the Flow: Send yourself a DM on Instagram to confirm the workflow triggers and GPT responds correctly. Instructions and links: Notion instruction Register in ManyChat Alex Hi no code | 1 month ago | Free", "completion": "{\"id\":\"qww129cm4TM9N8Ru\",\"meta\":{\"instanceId\":\"038da3428bba4563b42be267feeca21b4922693db254331ac640a5c56ee7cadf\",\"templateCredsSetupCompleted\":true},\"name\":\"InstaTest\",\"tags\":[{\"id\":\"8PlqXsDyqVlHJ7RC\",\"name\":\"AI\",\"createdAt\":\"2024-07-10T14:12:10.657Z\",\"updatedAt\":\"2024-07-10T14:12:10.657Z\"}],\"nodes\":[{\"id\":\"51dcaa84-d1f9-4abc-aebc-24a06801e42d\",\"name\":\"Set your system promt for AI\",\"type\":\"n8n-nodes-base.set\",\"notes\":\"In this node in \\\"prompt\\\" variable you can set your system prompt\",\"position\":[1120,620],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"0b3c3d71-5627-4b8c-91f0-ac44eaedf196\",\"name\":\"prompt\",\"type\":\"string\",\"value\":\"=Persona: You are a instagram influencer.\\nContext: You receive a messages from your subscribers\\nTask: Answer questions in your writing style and patterns according to your previous posts text. Use your post only for style and patterns reference.\\nStyle rules:\\nsimple answers\"},{\"id\":\"c2a9e272-5c0d-4685-ad0e-ce6995f92a1c\",\"name\":\"sessionId\",\"type\":\"string\",\"value\":\"={{ $json.body.session_id }}\"},{\"id\":\"b3c20ee3-07a1-4584-b0d9-7310a2c6b723\",\"name\":\"chatInput\",\"type\":\"string\",\"value\":\"={{ $json.body.text }}\"}]}},\"typeVersion\":3.3},{\"id\":\"0fb36573-d632-4403-8809-3973f9caa32a\",\"name\":\"Local n8n memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1500,780],\"parameters\":{\"sessionKey\":\"={{ $('"}
{"prompt": "Create an n8n workflow for: Detect hallucinations using specialised Ollama model bespoke-minicheck Fact-Checking Workflow Documentation Overview This workflow is designed for automated fact-checking of texts. It uses AI models to compare a given text with a list of facts and identify potential discrepancies or hallucinations. Components 1. Input The workflow can be initiated in two ways: a) Manually via the \"When clicking 'Test workflow'\" trigger b) By calling from another workflow via the \"When Executed by Another Workflow\" trigger Required inputs: facts: A list of verified facts text: The text to be checked 2. Text Preparation The \"Code\" node splits the input text into individual sentences Takes into account date specifications and list elements 3. Fact Checking Each sentence is individually compared with the given facts Uses the \"bespoke-minicheck\" Ollama model for verification The model responds with \"Yes\" or \"No\" for each sentence 4. Filtering and Aggregation Sentences marked as \"No\" (not fact-based) are filtered The filtered results are aggregated 5. Summary A larger language model (Qwen2.5) creates a summary of the results The summary contains: Number of incorrect factual statements List of incorrect statements Final assessment of the article's accuracy Usage Ensure the \"bespoke-minicheck\" model is installed in Ollama (ollama pull bespoke-minicheck) Prepare a list of verified facts Enter the text to be checked Start the workflow The results are output as a structured summary Notes The workflow ignores small talk and focuses on verifiable factual statements Accuracy depends on the quality of the provided facts and the performance of the AI models Customization Options The summarization function can be adjusted or removed to return only the raw data of the issues found The AI models used can be exchanged if needed This workflow provides an efficient method for automated fact-checking and can be easily integrated into larger systems or editorial workflows. Guido Zockoll | 15 days ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6e361bfcd1e8378c9b07774b22409c7eaea7080f01d5248da45077c0c6108b99\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"cbc036f7-b0e1-4eb4-94c3-7571c67a1efe\",\"name\":\"Code\",\"type\":\"n8n-nodes-base.code\",\"position\":[-120,40],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"jsCode\":\"// Get the input text\\nconst text = $input.item.json.text;\\n\\n// Ensure text is not null or undefined\\nif (!text) {\\n throw new Error('Input text is empty');\\n}\\n\\n// Function to split text into sentences while preserving dates and list items\\nfunction splitIntoSentences(text) {\\n const monthNames = '(?:Januar|Februar|M\u00e4rz|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)';\\n const datePattern = `(?:\\\\\\\\d{1,2}\\\\\\\\.\\\\\\\\s*(?:${monthNames}|\\\\\\\\d{1,2}\\\\\\\\.)\\\\\\\\s*\\\\\\\\d{2,4})`;\\n \\n // Split by sentence-ending punctuation, but not within dates or list items\\n const regex = new RegExp(`(?<=[.!?])\\\\\\\\s+(?=[A-Z\u00c4\u00d6\u00dc]|$)(?!${datePattern}|\\\\\\\\s*[-\u2022]\\\\\\\\s)`, 'g');\\n \\n return text.split(regex)\\n .map(sentence => sentence.trim())\\n .filter(sentence => sentence !== '');\\n}\\n\\n// Split the text into sentences\\nconst sentences = splitIntoSentences(text);\\n\\n// Output a single object with an array of sentences\\nreturn { json: { sentences: sentences } };\"},\"typeVersion\":2},{\"id\":\"faae4740-a529-4275-be0e-b079c3bfde58\",\"name\":\"Split Out1\",\"type\":\"n8n-nodes-base.splitOut"}
{"prompt": "Create an n8n workflow for: Arxiv Paper summarization with ChatGPT Webhook | Paper Summarization Who is this for? This workflow is designed for researchers, students, and professionals who frequently read academic papers and need concise summaries. It is useful for anyone who wants to quickly extract key information from research papers hosted on arXiv. What problem is this workflow solving? Academic papers are often lengthy and complex, making it time-consuming to extract essential insights. This workflow automates the process of retrieving, processing, and summarizing research papers, allowing users to focus on key findings without manually reading the entire paper. What this workflow does This workflow extracts the content of an arXiv research paper, processes its abstract and main sections, and generates a structured summary. It provides a well-organized output containing the Abstract Overview, Introduction, Results, and Conclusion, ensuring that users receive critical information in a concise format. Setup Ensure you have n8n installed and configured. Import this workflow into your n8n instance. Configure an external trigger using the Webhook node to accept paper IDs. Test the workflow by providing an arXiv paper ID. (Optional) Modify the summarization model or output format according to your preferences. How to customize this workflow to your needs Adjust the HTTPRequest node to fetch papers from other sources beyond arXiv. Modify the Summarization Chain node to refine the summary output. Enhance the Reorganize Paper Summary step by integrating additional language models. Add an email or Slack notification step to receive summaries directly. Workflow Steps Webhook receives a request with an arXiv paper ID. Send an HTTP request using \"Request to Paper Page\" to fetch the HTML content of the paper. Extract the abstract and sections using \"Extract Contents\". Split out all sections using \"Split out All Sections\" to process individual paragraphs. Clean up text using \"Remove useless links\" to remove unnecessary elements. Summarize extracted content using \"Summarization Chain\". Aggregate summarized content using \"Aggregate summarized content\". Reorganize the paper summary into structured sections using \"Reorganize Paper Summary\". Extract key information using \"Content Extractor\" to classify data into Abstract Overview, Introduction, Results, and Conclusion. Respond to the webhook with the structured summary. Note: This workflow is designed for use with arXiv research papers but can be adapted to process papers from other sources. Teddy | 18 days ago | Free", "completion": "{\"id\":\"5IAbyLhZX99QS1ff\",\"meta\":{\"instanceId\":\"0b0f5302e78710cf1b1457ee15a129d8e5d83d4e366bd96d14cc37da6693e692\"},\"name\":\"Webhook | Paper Summarization\",\"tags\":[],\"nodes\":[{\"id\":\"cf2bfb6d-c5ae-46e8-9382-274f37129291\",\"name\":\"Summarization Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainSummarization\",\"position\":[1000,0],\"parameters\":{\"options\":{}},\"typeVersion\":2},{\"id\":\"b1ab5c2c-f7df-4f2b-bf2d-e3f11a76b691\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1000,140],\"parameters\":{\"model\":\"gpt-3.5-turbo\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"kfEFSW14uz5PPu9A\",\"name\":\"OpenAi account(n8n_)\"}},\"typeVersion\":1},{\"id\":\"da17d97e-218a-466d-b356-ccdb63120626\",\"name\":\"Request to Paper Page\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[340,0],\"parameters\":{\"url\":\"=https://arxiv.org/html/{{ $json.query.id }}\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"034a05a8-78ef-4037-a41d-e2ae7e747378\",\"name\":\"Extract Contents\",\"type\":\"n8n-nodes-base.html\",\"position\":[500,0],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"abstract\",\"cssSelector\":\"div.ltx_abstract\"},{\"key\":\"sections\",\"cssSelector\":\"div.ltx_para\",\"returnArray\":true}]}},\"typeVersion\":1.2},{\"id\":\"88cae164-4e9c-40ab-bbd7-b07086"}
{"prompt": "Create an n8n workflow for: AI-Powered RAG Workflow For Stock Earnings Report Analysis This n8n workflow creates a financial analysis tool that generates reports on a company's quarterly earnings using the capabilities of OpenAI GPT-4o-mini, Google's Gemini AI and Pinecone's vector search. By analyzing PDFs of any company's earnings reports from their Investor Relations page, this workflow can answer complex financial questions and automatically compile findings into a structured Google Doc. How it works: Data loading and indexing Fetches links to PDF earnings document from a Google Sheet containing a list of file links. Downloads the PDFs from Google Drive. Parses the PDFs, splits the text into chunks, and generates embeddings using the Embeddings Google AI node (text-embedding-004 model). Stores the embeddings and corresponding text chunks in a Pinecone vector database for semantic search. Report generation with AI agent Utilizes an AI Agent node with a specifically crafted system prompt. The agent orchestrates the entire process. The agent uses a Vector Store Tool to access and retrieve information from the Pinecone database. Report delivery Saves the generated report as a Google Doc in a specified Google Drive location. Set up steps Google Cloud Project & Vertex AI API: Create a Google Cloud project. Enable the Vertex AI API for your project. Google AI API key: Obtain a Google AI API key from Google AI Studio. Pinecone account and API key: Create a free account on the Pinecone website. Obtain your API key from your Pinecone dashboard. Create an index named company-earnings in your Pinecone project. Google Drive - download and save financial documents: Go to a company you want to analize and download their quarterly earnings PDFs Save the PDFs in Google Drive Create a Google Sheet that stores a list of file URLs pointing to the PDFs you downloaded and saved to Google Drive Configure credentials in your n8n environment for: Google Sheets OAuth2 Google Drive OAuth2 Google Docs OAuth2 Google Gemini(PaLM) Api (using your Google AI API key) Pinecone API (using your Pinecone API key) Import and configure the workflow: Import this workflow into your n8n instance. Update the List Of Files To Load (Google Sheets) node to point to your Google Sheet. Update the Download File From Google Drive to point to the column where the file URLs are Update the Save Report to Google Docs node to point to your Google Doc where you want the report saved. Mihai Farcas | 1 month ago | Free +6", "completion": "{\"id\":\"fqaNojXWrspqjfkY\",\"meta\":{\"instanceId\":\"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"},\"name\":\"RAG Workflow For Stock Earnings Report Analysis\",\"tags\":[],\"nodes\":[{\"id\":\"1a621f76-9636-430d-94dd-d5e7dcd5afdc\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[380,-60],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"company-earnings\",\"cachedResultName\":\"company-earnings\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"e5936e45-0f58-48e9-9ab4-cc69f2ef6578\",\"name\":\"Embeddings Google Gemini\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsGoogleGemini\",\"position\":[300,220],\"parameters\":{\"modelName\":\"models/text-embedding-004\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"jLOqyTR4yTT1nYKi\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"e98dbc8e-6b4a-415d-a044-85e590fcb105\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[520,200],\"parameters\":{\"loader\":\"pdfLoader\",\"options\":{},\"dataType\":\"binary\"},\"typeVersion\":1},{\"id\":\"ae77f5f4-3704-4b66-9c3f-27d6bd3f68c3\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[560,380],\"parameters\":{\"options\":{}"}
{"prompt": "Create an n8n workflow for: Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API This workflow automates the collection, analysis, and summarization of Pinterest Pin data to help marketers optimize content strategy. It gathers Pinterest Pin performance data, analyzes trends using an AI agent, and delivers actionable insights to the Marketing Manager via email. This setup is ideal for content creators and marketing teams who need weekly insights on Pinterest trends to refine their content calendar and audience engagement strategy. Prerequisites Before setting up this workflow, ensure you have the following: Pinterest API Access & Developer Account Sign up at Pinterest Developers and obtain API credentials. Ensure you have access to both Organic and Paid Pin data. Airtable Account & API Key Create an account at Airtable and set up a database. Obtain an API key from Account Settings. AI Agent for Trend Analysis An AI-powered agent (such as OpenAI's GPT or a custom ML model) is required to analyze Pinterest trends. Ensure integration with your workflow automation tool (e.g., Zapier, Make, or a custom Python script). Email Automation Setup Configure an SMTP email service (e.g., Gmail, Outlook, SendGrid) to send the summarized results to the Marketing Manager. Step-by-Step Guide to Automating Pinterest Pin Analysis 1. Scheduled Trigger for Data Collection At 8:00 AM (or your preferred time), an automated trigger starts the workflow. Adjust the timing based on your marketing schedule to optimize trend tracking. 2. Fetch Data from Pinterest API Retrieve recent Pinterest Pin performance data, including impressions, clicks, saves, and engagement rate. Ensure both Organic and Paid Ads data are labeled correctly for clarity. 3. Store Data in Airtable Pins are logged and categorized in an Airtable database for further analysis. Sample Airtable Template for Pinterest Pins | Column Name | Description | |---------------|---------------------------------------| | pin_id | Unique identifier for each Pin | | created_at | Timestamp of when the Pin was created | | title | Title of the Pin | | description| Short description of the Pin | | link | URL linking to the Pin | | type | Type of Pin (e.g., organic, ad) | 4. AI Agent Analyzes Pinterest Trends The AI model reviews the latest Pinterest data and identifies: Trending Topics & Keywords** Engagement Patterns** Audience Interests & Behavior Changes** Optimal Posting Times & Formats** 5. Generate Content Suggestions with AI The AI Agent recommends new Pin ideas and content calendar updates to maximize engagement. Suggestions include creative formats, hashtags, and timing adjustments for better performance. 6. Summary & Insights Generated by AI A concise report is created, summarizing Pinterest trends and actionable insights for content strategy. 7. Email Report Sent to the Marketing Manager The summary is emailed to the Marketing Manager to assist with content planning and execution. The report includes: Performance Overview of Recent Pins Trending Content Ideas Best Performing Pin Formats AI-Generated Recommendations This workflow enables marketing teams to automate Pinterest analysis and optimize their content strategy through AI-driven insights. \ud83d\ude80 Marketing Canopy | 24 days ago | Free +2", "completion": "{\"id\":\"gP9EsxKN5agUGzDS\",\"meta\":{\"instanceId\":\"73d9d5380db181d01f4e26492c771d4cb5c4d6d109f18e2621cf49cac4c50763\",\"templateCredsSetupCompleted\":true},\"name\":\"Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API\",\"tags\":[],\"nodes\":[{\"id\":\"7f582bb4-97cd-458e-a7b7-b518c5b8a4ca\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[540,-260],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"95QGJD3XSz0piaNU\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"c6772882-468c-4391-a259-93e52daf49d4\",\"name\":\"Airtable2\",\"type\":\"n8n-nodes-base.airtableTool\",\"position\":[700,-260],\"parameters\":{\"id\":\"=\",\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appfsNi1QEhw6WvXK\",\"cachedResultUrl\":\"https://airtable.com/appfsNi1QEhw6WvXK\",\"cachedResultName\":\"Pinterest_Metrics\"},\"table\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"tbl9Dxdrwx5QZGFnp\",\"cachedResultUrl\":\"https://airtable.com/appfsNi1QEhw6WvXK/tbl9Dxdrwx5QZGFnp\",\"cachedResultName\":\"Pinterest_Organic_Data\"},\"options\":{}},\"credentials\":{\"airtableTokenApi\":{\"id\":\"0ApVmNsLu7aFzQD6\",\"name\":\"Airtable Personal Access Token account\"}},\"typeVersion\":2.1},{\"id\":\"85ea8bec-14c8-4277-b2e3-eb145db0713a\",\"name\":\"OpenAI Chat Model1\",\"type\":\""}
{"prompt": "Create an n8n workflow for: Build an OpenAI Assistant with Google Drive Integration Workflow Overview This workflow automates the creation and management of a custom OpenAI Assistant for a travel agency (\"Travel with us\"), leveraging Google Drive for document storage. How It Works 1. Create the OpenAI Assistant Node**: OpenAI Action: Creates a custom assistant named \"Travel with us\" Assistant using the gpt-4o-mini model. Instructions: Respond only using the provided document (e.g., agency-specific info). Stay friendly, brief, and focused on travel-related queries. Ignore irrelevant questions politely. Credentials: Requires OpenAI API key. 2. Upload Agency Document Google Drive Node**: Action: Downloads a Google Doc as a PDF. OpenAI2 Node**: Action: Uploads the PDF to OpenAI with purpose: \"assistants\". Output: Generates a file_id. 3. Update the Assistant with the Document OpenAI Node**: Action: Updates the assistant to include the uploaded file. 4. Chat Interaction Chat Trigger**: Activates when a message is received (\"When chat message received\"). OpenAI Assistant Node**: Action: Uses the updated assistant to respond to user queries. Memory: Window Buffer Memory retains chat context for coherent conversations. Set Up Steps Prepare the Document: Store your travel agency guide in Google Drive (e.g., as a Google Doc). Update the Google Drive node with your document\u2019s ID. Configure Credentials: Google Drive: Connect via OAuth2 (googleDriveOAuth2Api). OpenAI: Add your API key to all OpenAI nodes. Customize the Assistant: Modify the instructions in the OpenAI node to reflect your agency\u2019s needs. Ensure the document includes FAQs, policies, and travel info. Test the Workflow: Trigger manually (\"Test workflow\") to create the assistant and upload the file. Send a chat message (e.g., \"What are your travel packages?\") to test responses. Dependencies Google Drive Account**: To store and retrieve the agency document. OpenAI API Access**: For assistant creation and file uploads. n3w Italia | 1 month ago | Free", "completion": "{\"id\":\"AjJ7O98qjw8XVirk\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Build an OpenAI Assistant with Google Drive Integration\",\"tags\":[{\"id\":\"2VG6RbmUdJ2VZbrj\",\"name\":\"Google Drive\",\"createdAt\":\"2024-12-04T16:50:56.177Z\",\"updatedAt\":\"2024-12-04T16:50:56.177Z\"},{\"id\":\"paTcf5QZDJsC2vKY\",\"name\":\"OpenAI\",\"createdAt\":\"2024-12-04T16:52:10.768Z\",\"updatedAt\":\"2024-12-04T16:52:10.768Z\"}],\"nodes\":[{\"id\":\"8a00e7b2-8348-47d2-87db-fe40b41a44f1\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[180,260],\"parameters\":{},\"typeVersion\":1},{\"id\":\"1d8fe39a-c7b9-4c38-9dc6-0fbce63151ba\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[480,380],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"1JG7ru_jBcWu5fvgG3ayKjXVXHVy67CTqLwNITqsSwh8\",\"cachedResultUrl\":\"https://docs.google.com/document/d/1JG7ru_jBcWu5fvgG3ayKjXVXHVy67CTqLwNITqsSwh8/edit?usp=drivesdk\",\"cachedResultName\":\"[TEST] Assistente Agenzia viaggi\"},\"options\":{\"binaryPropertyName\":\"data.pdf\",\"googleFileConversion\":{\"conversion\":{\"docsToFormat\":\"application/pdf\"}}},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"HEy5Eu"}
{"prompt": "Create an n8n workflow for: Bitrix24 Chatbot Application Workflow example with Webhook Integration Use Case Automate chat interactions in Bitrix24 with a customizable bot that can handle various events and respond to user messages. What This Workflow Does Processes incoming webhook requests from Bitrix24 Handles authentication and token validation Routes different event types (messages, joins, installations) Provides automated responses and bot registration Manages secure communication between Bitrix24 and external services Setup Instructions Configure Bitrix24 webhook endpoints Set up authentication credentials Customize bot responses and behavior Deploy and test the workflow Ferenc Erb | 15 days ago | Free", "completion": "{\"id\":\"cmGsNvW9bEORABdo\",\"meta\":{\"instanceId\":\"15c09ee9508dd818e298e675375571ba4b871bbb8c420fd01ac9ed7c58622669\"},\"name\":\"Bitrix24 Chatbot Application Workflow example with Webhook Integration\",\"tags\":[{\"id\":\"5YZ9E6AmGZn6WTMa\",\"name\":\"Tech demo\",\"createdAt\":\"2024-12-28T09:13:02.965Z\",\"updatedAt\":\"2024-12-28T09:13:02.965Z\"},{\"id\":\"hEvnK1kMYTPrL3vs\",\"name\":\"Bitrix24\",\"createdAt\":\"2025-01-04T16:12:36.741Z\",\"updatedAt\":\"2025-01-04T16:12:36.741Z\"},{\"id\":\"yKS9RGKLuFUhYFIE\",\"name\":\"Chatbot\",\"createdAt\":\"2025-01-04T16:12:36.757Z\",\"updatedAt\":\"2025-01-04T16:12:36.757Z\"}],\"nodes\":[{\"id\":\"ddd802bb-0da0-474d-b1e9-74f247e603e0\",\"name\":\"Bitrix24 Handler\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[0,0],\"webhookId\":\"c3ae607d-41f0-42bc-b669-c2c77936d443\",\"parameters\":{\"path\":\"bitrix24/handler.php\",\"options\":{},\"httpMethod\":\"POST\",\"responseMode\":\"responseNode\"},\"typeVersion\":1},{\"id\":\"5676a53e-6758-4ad5-ace6-e494fa10b6c3\",\"name\":\"Credentials\",\"type\":\"n8n-nodes-base.set\",\"position\":[200,0],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"030f8f90-2669-4c20-9eab-c572c4b7c70c\",\"name\":\"CLIENT_ID\",\"type\":\"string\",\"value\":\"local.6779636e712043.37129431\"},{\"id\":\"de9bbb7a-b782-4540-b259"}
{"prompt": "Create an n8n workflow for: Fetch Dynamic Prompts from GitHub and Auto-Populate n8n Expressions in Prompt Who Is This For? This workflow is designed for AI engineers, automation specialists, and content creators who need a scalable system to dynamically manage prompts stored in GitHub. It eliminates manual updates, enforces required variable checks, and ensures that AI interactions always receive fully processed prompts. \ud83d\ude80 What Problem Does This Solve? Manually managing AI prompts can be inefficient and error-prone. This workflow: \u2705 Fetches dynamic prompts from GitHub \u2705 Auto-populates placeholders with values from the setVars node \u2705 Ensures all required variables are present before execution \u2705 Processes the formatted prompt through an AI agent \ud83d\udee0 How This Workflow Works This workflow consists of three key branches, ensuring smooth prompt retrieval, variable validation, and AI processing. 1\ufe0f\u20e3 Retrieve the Prompt from GitHub (HTTP Request \u2192 Extract from File \u2192 SetPrompt) The workflow starts manually or via an external trigger. It fetches a text-based prompt stored in a GitHub repository. The Extract from File Node retrieves the content from the GitHub file. The SetPrompt Node stores the prompt, making it accessible for processing. \ud83d\udccc Note: The prompt must contain n8n expression format variables (e.g., {{ $json.company }}) so they can be dynamically replaced. 2\ufe0f\u20e3 Extract & Auto-Populate Variables (Check All Prompt Vars \u2192 Replace Variables) A Code Node scans the prompt for placeholders in the n8n expression format ({{ $json.variableName }}). The workflow compares required variables against the setVars node: \u2705 If all variables are present, it proceeds to variable replacement. \u274c If any variables are missing, the workflow stops and returns an error listing them. The Replace Variables Node replaces all placeholders with values from setVars. \ud83d\udccc Example of a properly formatted GitHub prompt: Hello {{ $json.company }}, your product {{ $json.features }} launches on {{ $json.launch_date }}. This ensures seamless replacement when processed in n8n. 3\ufe0f\u20e3 AI Processing & Output (AI Agent \u2192 Prompt Output) The Set Completed Prompt Node stores the final, processed prompt. The AI Agent Node (Ollama Chat Model) processes the prompt. The Prompt Output Node returns the fully formatted response. \ud83d\udccc Optional: Modify this to use OpenAI, Claude, or other AI models. \u26a0\ufe0f Error Handling: Missing Variables If a required variable is missing, the workflow stops execution and provides an error message: \u26a0\ufe0f Missing Required Variables: [\"launch_date\"] This ensures no incomplete prompts are sent to AI agents. \u2705 Example Use Case \ud83d\udcdc GitHub Prompt File (Using n8n Expressions) Hello {{ $json.company }}, your product {{ $json.features }} launches on {{ $json.launch_date }}. \ud83d\udd39 Variables in setVars Node { \"company\": \"PropTechPro\", \"features\": \"AI-powered Property Management\", \"launch_date\": \"March 15, 2025\" } \u2705 Successful Output Hello PropTechPro, your product AI-powered Property Management launches on March 15, 2025. \ud83d\udea8 Error Output (If Missing launch_date) \u26a0\ufe0f Missing Required Variables: [\"launch_date\"] \ud83d\udd27 Setup Instructions 1\ufe0f\u20e3 Connect Your GitHub Repository Store your prompt in a public or private GitHub repo. The workflow will fetch the raw file using the GitHub API. 2\ufe0f\u20e3 Configure the SetVars Node Define the required variables in the SetVars Node. Make sure the variable names match those used in the prompt. 3\ufe0f\u20e3 Test & Run Click Test Workflow to execute. If variables are missing, it will show an error. If everything is correct, it will output the fully formatted prompt. \u26a1 How to Customize This Workflow \ud83d\udca1 Need CRM or Database Integration? Connect the setVars node to an Airtable, Google Sheets, or HubSpot API to pull variables dynamically. \ud83d\udca1 Want to Modify the AI Model? Replace the Ollama Chat Model with OpenAI, Claude, or a custom LLM endpoint. \ud83d\udccc Why Use This Workflow? \u2705 No Manual Updates Required \u2013 Fetches prompts dynamically from GitHub. \u2705 Prevents Broken Prompts \u2013 Ensures required variables exist before execution. \u2705 Works for Any Use Case \u2013 Handles AI chat prompts, marketing messages, and chatbot scripts. \u2705 Compatible with All n8n Deployments \u2013 Works on Cloud, Self-Hosted, and Desktop versions. RealSimple Solutions | 19 days ago | Free", "completion": "{\"id\":\"QyMyf3zraY0wxXDf\",\"meta\":{\"instanceId\":\"ba3fa76a571c35110ef5f67e5099c9a5c1768ef125c2f3b804ba20de75248c0b\",\"templateCredsSetupCompleted\":true},\"name\":\"Load Prompts from Github Repo and auto populate n8n expressions\",\"tags\":[],\"nodes\":[{\"id\":\"34781446-b06e-41eb-83b8-b96bda1a5595\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-80,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c53b7243-7c82-47e0-a5ee-bd82bc51c386\",\"name\":\"GitHub\",\"type\":\"n8n-nodes-base.github\",\"position\":[600,0],\"parameters\":{\"owner\":{\"__rl\":true,\"mode\":\"name\",\"value\":\"={{ $json.Account }}\"},\"filePath\":\"={{ $json.path }}{{ $json.prompt }}\",\"resource\":\"file\",\"operation\":\"get\",\"repository\":{\"__rl\":true,\"mode\":\"name\",\"value\":\"={{ $json.repo }}\"},\"additionalParameters\":{}},\"credentials\":{\"githubApi\":{\"id\":\"ostHZNoe8GSsbaQM\",\"name\":\"The GitHub account\"}},\"typeVersion\":1},{\"id\":\"9976b199-b744-47a7-9d75-4b831274c01b\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[840,0],\"parameters\":{\"options\":{},\"operation\":\"text\"},\"typeVersion\":1},{\"id\":\"26aa4e6a-c487-4cdf-91d5-df660cf826a6\",\"name\":\"setVars\",\"type\":\"n8n-nodes-base.set\",\"position\":[180,0],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"150618c5-09b1-4f8b-a7b4-984662bf3381\",\"name\":\"Account\",\"type\":\"string\",\"value\":\"TPG"}
{"prompt": "Create an n8n workflow for: Extract and process information directly from PDF using Claude and Gemini Overview This workflow helps you compare Claude 3.5 Sonnet and Gemini 2.0 Flash when extracting data from a PDF This workflow extracts and processes the data within a PDF in one single step, instead of calling an OCR and then an LLM\u201d How it works The initial 2 steps download the PDF and convert it to base64. This base64 string is then sent to both Claude 3.5 Sonnet and Gemini 2.0 Flash to extract information. This workflow is made to let you compare results, latency, and cost (in their dedicated dashboard). How to use it Set up your Google Drive if not already done Select a document on your Google Drive Modify the prompt in \"Define Prompt\" to extract the information you need and transform it as wanted. Get a Claude API key and/or Gemini API key Note that you can deactivate one of the 2 API calls if you don't want to try both Test the Workflow Agent Studio | 1 month ago | Free", "completion": "{\"meta\":{\"instanceId\":\"f4f5d195bb2162a0972f737368404b18be694648d365d6c6771d7b4909d28167\"},\"nodes\":[{\"id\":\"b6cd232e-e82e-457b-9f03-c010b3eba148\",\"name\":\"When clicking 'Test workflow'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-40,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"2b734806-e3c0-4552-a491-54ca846ed3ac\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[620,0],\"parameters\":{\"options\":{},\"operation\":\"binaryToPropery\"},\"typeVersion\":1},{\"id\":\"2c199499-cc4f-405c-8560-765500b7acba\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[420,0],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"18Ac2xorxirIBm9FNFDDB5aVUSPBCCg1U\",\"cachedResultUrl\":\"https://drive.google.com/file/d/18Ac2xorxirIBm9FNFDDB5aVUSPBCCg1U/view?usp=drivesdk\",\"cachedResultName\":\"Invoice-798FE2FA-0004.pdf\"},\"options\":{},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"AUEpxwlqBJghNMtb\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"e3031c0c-f059-4f30-9684-10014a277d55\",\"name\":\"Call Gemini 2.0 Flash with PDF Capabilities\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[880,220],\"parameters\":{\"url\":\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"contents\\\":"}
{"prompt": "Create an n8n workflow for: Automate SIEM Alert Enrichment with MITRE ATT&CK, Qdrant & Zendesk in n8n n8n Workflow: Automate SIEM Alert Enrichment with MITRE ATT&CK & Qdrant Who is this for? This workflow is ideal for: Cybersecurity teams & SOC analysts* who want to automate *SIEM alert enrichment**. IT security professionals* looking to integrate *MITRE ATT&CK intelligence** into their ticketing system. Organizations using Zendesk for security incidents* who need enhanced *contextual threat data**. Anyone using n8n and Qdrant* to build *AI-powered security workflows**. What problem does this workflow solve? Security teams receive large volumes of raw SIEM alerts that lack actionable context. Investigating every alert manually is time-consuming and can lead to delayed response times. This workflow solves this problem by: \u2714 Automatically enriching SIEM alerts with MITRE ATT&CK TTPs. \u2714 Tagging & classifying alerts based on known attack techniques. \u2714 Providing remediation steps to guide the response team. \u2714 Enhancing security tickets in Zendesk with relevant threat intelligence. What this workflow does 1\ufe0f\u20e3 Ingests SIEM alerts (via chatbot or ticketing system like Zendesk). 2\ufe0f\u20e3 Queries a Qdrant vector store containing MITRE ATT&CK techniques. 3\ufe0f\u20e3 Extracts relevant TTPs (Tactics, Techniques, & Procedures) from the alert. 4\ufe0f\u20e3 Generates remediation steps using AI-powered enrichment. 5\ufe0f\u20e3 Updates Zendesk tickets with threat intelligence & recommended actions. 6\ufe0f\u20e3 Provides structured alert data for further automation or reporting. Setup Guide Prerequisites n8n instance** (Cloud or Self-hosted). Qdrant vector store** with MITRE ATT&CK data embedded. OpenAI API key** (for AI-based threat processing). Zendesk account** (for ticket enrichment, if applicable). Clean Mitre Data Python Script Cleaned Mitre Data Full Mitre Data Steps to Set Up 1\ufe0f\u20e3 Embed MITRE ATT&CK data into Qdrant This workflow pulls MITRE ATT&CK data from Google Drive and loads it into Qdrant. The data is vectorized using OpenAI embeddings for fast retrieval. 2\ufe0f\u20e3 Deploy the n8n Chatbot The chatbot listens for SIEM alerts and sends them to the AI processing pipeline. Alerts are analyzed using an AI agent trained on MITRE ATT&CK. 3\ufe0f\u20e3 Enrich Zendesk Tickets The workflow extracts MITRE ATT&CK techniques from alerts. It updates Zendesk tickets with contextual threat intelligence. The remediation steps are included as internal notes for SOC teams. How to Customize This Workflow \ud83d\udd27 Modify the chatbot trigger: Adapt the chatbot node to receive alerts from Slack, Microsoft Teams, or any other tool. \ud83d\udd27 Change the SIEM input source: Connect your workflow to Splunk, Elastic SIEM, or Chronicle Security. \ud83d\udd27 Customize remediation steps: Use a custom AI model to tailor remediation responses based on organization-specific security policies. \ud83d\udd27 Extend ticketing integration: Modify the Zendesk node to also work with Jira, ServiceNow, or another ITSM platform. Why This Workflow is Powerful \u2705 Saves time: Automates alert triage & classification. \u2705 Improves security posture: Helps SOC teams act faster on threats. \u2705 Leverages AI & vector search: Uses LLM-powered enrichment for real-time context. \u2705 Works across platforms: Supports n8n Cloud, Self-hosted, and Qdrant. \ud83d\ude80 Get Started Now! \ud83d\udcd6 Watch the Setup Video \ud83d\udcac Have Questions? Join the Discussion in the YouTube Comments! Angel Menendez | 29 days ago | Free +5", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"86ddd018-3d6b-46b9-aa93-dedd6c6b5076\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-880,360],\"webhookId\":\"a9668bb8-bbe8-418a-b5c9-ff7dd431244f\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"a5ba5090-8e3b-4408-82df-92d2c524039e\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[-680,360],\"parameters\":{\"options\":{\"systemMessage\":\"You are a cybersecurity expert trained on MITRE ATT&CK and enterprise incident response. Your job is to:\\n1. Extract TTP information from SIEM data.\\n2. Provide actionable remediation steps tailored to the alert.\\n3. Cross-reference historical patterns and related alerts.\\n4. Recommend external resources for deeper understanding.\\n\\nEnsure that:\\n- TTPs are tagged with the tactic, technique name, and technique ID.\\n- Remediation steps are specific and actionable.\\n- Historical data includes related alerts and notable trends.\\n- External links are relevant to the observed behavior.\\n\"}},\"typeVersion\":1.7},{\"id\":\"67c52944-b616-4ea6-9507-e9fb6fcdbe2b\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-740,580],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"QpFZ2EiM3WGl6Zr3\",\"name\":\"Marketing OpenAI\"}},\"typeVersion\":1},{\"id\":\"55f6c16a-51ed-45e4"}
{"prompt": "Create an n8n workflow for: Basic Automatic Gmail Email Labelling with OpenAI and Gmail API Description This workflow automates email categorization in Gmail using the Gmail API and OpenAI's language model. It periodically checks for new emails, reads their content, and categorizes them based on existing Gmail labels. If no matching label is found, the workflow creates a new label and assigns it to the email. Key Features Polling for Emails**: The workflow triggers every 5 minutes to check for new emails using the Gmail Trigger node. Reading Labels**: Existing Gmail labels are fetched to determine the most relevant match for email categorization. Dynamic Labeling**: If no existing label matches, a new label is created dynamically based on the email's content. OpenAI Integration**: The workflow uses OpenAI's Chat model to analyze email content and suggest or create appropriate labels. Email Categorization**: Labels are applied to emails, ensuring they are organized in Gmail's structure. The workflow also removes less relevant emails (e.g., ads) from the inbox. Nodes in Use Gmail Trigger: Polls Gmail every 5 minutes for new emails. Gmail - Read Labels: Fetches all existing Gmail labels. Gmail - Get Message: Retrieves the full content of a specific email. Gmail - Add Label to Message: Assigns a chosen label to the email. Gmail - Create Label: Creates a new label if necessary. OpenAI Chat Model: Analyzes email content for categorization. Memory Buffer: Retains context for email analysis across multiple iterations. Wait Node: Adds a buffer period to manage email processing. Prerequisites Gmail API Setup**: Ensure Gmail OAuth2 credentials are configured in n8n. OpenAI API Key**: Configure OpenAI credentials for email analysis. Labeling Standards**: Maintain a consistent Gmail label structure for better organization. Instructions Add your Gmail API credentials to the Gmail nodes. Add your OpenAI API credentials to the OpenAI Chat Model node. Activate the workflow. It will start polling for new emails every 5 minutes. Monitor and refine the categorization logic if necessary to ensure alignment with Gmail's organizational needs. Use Case Ideal for individuals or teams handling high email volumes who want to maintain an organized inbox and automate repetitive categorization tasks. Note: You can improve the prompt to get better results from the agent by giving it more personal rules on how to categorize. Martijn Kerver | 1 month ago | Free", "completion": "{\"nodes\":[{\"id\":\"2a41e2da-19f7-4c31-ab93-3a534db3179e\",\"name\":\"Gmail Trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-360,-260],\"parameters\":{\"filters\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyX\",\"unit\":\"minutes\",\"value\":5}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"10LJ3tXKoUfexiKU\",\"name\":\"Gmail account\"}},\"typeVersion\":1.2},{\"id\":\"a25e0e42-8eab-49c5-a553-797da40eb623\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-220,-60],\"parameters\":{\"options\":{\"maxTokens\":4096}},\"credentials\":{\"openAiApi\":{\"id\":\"qR44iMsUYcLrhdR0\",\"name\":\"OpenAi account\"}},\"notesInFlow\":false,\"typeVersion\":1},{\"id\":\"cf437748-a0df-42a2-b1ca-f93162d85bfe\",\"name\":\"Gmail - read labels\",\"type\":\"n8n-nodes-base.gmailTool\",\"position\":[80,-40],\"webhookId\":\"d8ec9401-a9ff-4fe2-9c1e-5a8036cd96c9\",\"parameters\":{\"resource\":\"label\",\"returnAll\":true,\"descriptionType\":\"manual\",\"toolDescription\":\"Tool to read all existing gmail labels\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"10LJ3tXKoUfexiKU\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"152f1970-7a1f-4977-9c21-64b69242d3a9\",\"name\":\"Gmail - get message\",\"type\":\"n8n-nodes-base.gmailTool\",\"position\":[260,-40],\"webhookId\":\"d8ec9401-a9ff-4fe2-9c1e-5a8036cd96c9\",\"parameters\":{\"messageId\":\"={{ $fromAI('gmail_message_id', '"}
{"prompt": "Create an n8n workflow for: Analyze tradingview.com charts with Chrome extension, N8N and OpenAI This flow is supported by a Chrome plugin created with Cursor AI. The idea was to create a Chrome plugin and a backend service in N8N to do chart analytics with OpenAI. It's a good sample on how to submit a screenshot from the browser to N8N. Who is it for? N8N developers who want to learn about using a Chrome plugin, an N8N webhook and OpenAI. What opportunity does it present? This sample opens up a whole range of N8N connected Chrome extensions that can analyze screenshots by using OpenAI. What this workflow does? The workflow contains: a webhook trigger an OpenAI node with GPT-4O-MINI and Analyze Image selected a response node to send back the Text that was created after analysing the screenshot. All this is needed to talk to the Chrome extension which is created with Cursor AI. The idea is to visit the tradingview.com crypto charts, click the Chrome plugin and get back analytics about the shown chart in understandable language. This is driven by the N8N flow. With the new image analytics capabilities of OpenAI this opens up a world of opportunities. Requirements/setup OpenAI API key Cursor AI installed The Chrome extension. Download The N8N JSON code. Download How to customize it to your needs? Both the Chrome extension and N8N flow can be adapted to use on other websites. You can consider: analyzing a financial screen and ask questions about the data shown analyzing other charts extending the N8N workflow with other AI nodes With AI and image analytics the sky is the limit and in some cases it saves you from creating complex API integrations. Download Chrome extension Hans Blaauw | 2 months ago | Free", "completion": "{\"id\":\"Q8On8rR6BkmPzDUd\",\"meta\":{\"instanceId\":\"f57770b08f6a574802832e927ed1b0063c627ffc5b95965abf0d4a7396150138\"},\"name\":\"chrome extension backend with AI\",\"tags\":[],\"nodes\":[{\"id\":\"0f38fe62-36d9-43da-a992-a3981377e89e\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[-220,-20],\"webhookId\":\"e9a97dd5-f1e7-4d5b-a6f1-be5f0c9eb96c\",\"parameters\":{\"path\":\"e9a97dd5-f1e7-4d5b-a6f1-be5f0c9eb96c\",\"options\":{},\"httpMethod\":\"POST\",\"responseMode\":\"responseNode\"},\"typeVersion\":2},{\"id\":\"83959562-edf5-4d37-bd11-47186c6a31c7\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[-40,-20],\"parameters\":{\"text\":\"You are an expert financial analyst tasked with providing an advanced technical analyses of a stock or crypto currency chart provided. Your analysis will be based on various technical indicators and will provide simple insights for novice traders. Just explain to traders were you expect the market is moving. Also warn them this is not a binding advice. Make sure to explain everything in infant language.\",\"modelId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\",\"cachedResultName\":\"GPT-4O-MINI\"},\"options\":{},\"resource\":\"image\",\"inputType\":\"base64\",\"operation\":\"analyze\"},\"credentials\":{\"openAiApi\":{\"id\":\"8MS1muoK4z86fxUs\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.7},{\"id\":\"c6f1f833-7ba3-49c5-86df-f586e6bb5975\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[140,-20],\"param"}
{"prompt": "Create an n8n workflow for: Modular & Customizable AI-Powered Email Routing: Text Classifier for eCommerce How It Works Form Submission: The workflow starts with the On form submission node, which triggers when a user submits a contact form. The form collects the user's name, email, and message. Text Classification: The Text Classifier node uses an AI model (GPT-4) to classify the submitted message into one of the predefined categories: Request Quote: For quote requests. Product info: For general product inquiries. General problem: For issues or problems related to products. Order: For questions about placed orders. Other: For any messages that don\u2019t fit the above categories. Email Routing: Based on the classification, the workflow routes the message to the appropriate department via email: Prod. Dep.: For product-related inquiries. Quote Dep.: For quote requests. Gen. Dep.: For general problems. Order Dep.: For order-related questions. Other Dep.: For all other inquiries. Each email includes the user's name, email, message, and the classified category. Data Logging: The workflow logs the form submission and classification results into a Google Sheets document. Each department has its own sheet where the data is appended, including: User\u2019s name, email, and message. Submission date and time. Assigned category. Email recipient details. AI Model Integration: The OpenAI node provides the AI model (GPT-4) used by the Text Classifier to classify the messages. The model is instructed to classify the text into one of the predefined categories without additional explanations. Set Up Steps Configure the Form Trigger: Set up the On form submission node to collect user inputs (name, email, and message) and trigger the workflow. Set Up the Text Classifier: Configure the Text Classifier node to use the OpenAI model (GPT-4) for text classification. Define the categories and their descriptions (e.g., \"Request Quote\", \"Product info\", etc.). Set the fallback category to \"Other\" for unclassifiable messages. Configure Email Sending: Set up the Email Send nodes for each department (Prod. Dep., Quote Dep., Gen. Dep., Order Dep., Other Dep.). Configure the email subject, body, and reply-to address using the form data and classification results. Ensure SMTP credentials are correctly configured for sending emails. Set Up Google Sheets Integration: Configure the Google Sheets nodes to append data to the appropriate sheets for each department. Map the form data (name, email, message, date, category, and recipient) to the corresponding columns in the Google Sheets document. Test the Workflow: Submit a test form to ensure the workflow correctly classifies the message, sends the email to the right department, and logs the data in Google Sheets. Verify that the OpenAI model is classifying messages accurately. Activate the Workflow: Once tested, activate the workflow to automate the process of handling contact form submissions. Key Features Automated Classification**: Uses AI to classify messages into relevant categories, reducing manual effort. Email Routing**: Sends emails to the appropriate department based on the classification. Data Logging**: Logs all form submissions and classification results in Google Sheets for tracking and analysis. Scalability**: Easily adaptable to additional categories or departments by modifying the workflow. This workflow is ideal for eCommerce businesses or customer support teams looking to automate and streamline the handling of contact form submissions. n3w Italia | 27 days ago | Free", "completion": "{\"id\":\"LGpVLWPpNZSt9ISM\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Contact Form Text Classifier for eCommerce\",\"tags\":[],\"nodes\":[{\"id\":\"13175d48-c3a6-4ca6-afed-b70f40289f38\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-480,-320],\"webhookId\":\"8e10c8ca-895c-4274-ba95-0d646b8bda4e\",\"parameters\":{\"options\":{},\"formTitle\":\"Contacts\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Name\",\"placeholder\":\"Name\",\"requiredField\":true},{\"fieldLabel\":\"Email\",\"placeholder\":\"Email\",\"requiredField\":true},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Message\",\"placeholder\":\"Message\",\"requiredField\":true}]},\"responseMode\":\"lastNode\",\"formDescription\":\"Basic Contact Form\"},\"typeVersion\":2.2},{\"id\":\"7b352c9f-5d2e-46ca-9499-594063167e9a\",\"name\":\"Text Classifier\",\"type\":\"@n8n/n8n-nodes-langchain.textClassifier\",\"position\":[-160,-320],\"parameters\":{\"options\":{\"fallback\":\"other\",\"systemPromptTemplate\":\"=Please classify the text provided by the user into one of the following categories: {categories}, and use the provided formatting instructions below. Don't explain, and only output the json with the selected {categories}.\"},\"inputText\":\"={{ $json.Message }}\",\"categories\":{\"categories\":[{\"category\":\"Request Quote\",\"description\":\"Request for quote\"},{\"category\":\"Product info\",\"description\":\"General information about a product\"},{\"category\":\"General problem\",\"description\":\"General problems about a product\"},{\"category\":\"Order\",\"description\":\"Information about an order placed\"}]}},\"typeVersion\":1},{\"id\":\"efef4c71-5f56-44b0-a613-9fa888e495b8\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-220"}
{"prompt": "Create an n8n workflow for: \ud83c\udfa8 Interactive Image Editor with FLUX.1 Fill Tool for Inpainting &gt; Like this template? Connect with Eduard via LinkedIn. This workflow is a prototype of an AI-powered image editing interface, similar to Photoshop's Generative Fill feature, but running entirely in the browser. It provides a web-based editor that allows users to: Select areas in images using an adjustable brush tool Input text prompts to guide the AI generation Compare original and generated images side by side Iterate on edits with different prompts and settings Save or reuse generated images &gt; \ud83c\udfa8 Perfect for product catalog management, seasonal content updates, and creative image editing tasks! \ud83d\udccb Requirements FLUX API Access: You'll need API credentials from FLUX to use this workflow. Configure the HTTP Header Auth credential in n8n with your FLUX API key \ud83d\udd27 Key Components FLUX Fill API for AI-powered image generation Konva.js for canvas manipulation img-comparison-slider for result visualization Custom CSS/JS for editor functionality Simple Editor Interface HTML page with an editor is served on the Webhook call Adjustable brush selection tool Provides several mock examples and allows uploading custom images Basic prompt and FLUX model parameter controls Image Processing Pipeline Handles image and mask separately Processes FLUX Fill API requests Delivers results back to the editor Result Viewer Split-screen comparison of original and generated images Interactive slider for before/after comparison Options to save or continue editing Support for multiple iteration cycles \ud83c\udfaf Use Cases This prototype is particularly useful for: Testing AI-powered image editing concepts Quick product visualization experiments Exploring creative image variations Demonstrating inpainting capabilities &gt; \ud83d\udca1 Pro Tip: Save masks for frequently edited areas to quickly generate variations with different prompts! The workflow can be extended to integrate with various data sources and can be customized for specific business needs. Eduard | 1 month ago | Free", "completion": "{\"id\":\"OvuZIXwt9mdU2JGK\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"FLUX-fill standalone\",\"tags\":[],\"nodes\":[{\"id\":\"9f051c89-0243-48fb-baa4-666af3fe54b3\",\"name\":\"Merge\",\"type\":\"n8n-nodes-base.merge\",\"position\":[940,120],\"parameters\":{\"mode\":\"combine\",\"options\":{},\"combineBy\":\"combineByPosition\"},\"typeVersion\":3},{\"id\":\"5da963f7-4320-4359-aefa-bf8f6d6ef815\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1520,120],\"parameters\":{\"options\":{},\"respondWith\":\"text\",\"responseBody\":\"={{ $json.html }}\"},\"typeVersion\":1.1},{\"id\":\"05d877bc-b591-478c-b112-32b7efe1ca3f\",\"name\":\"Wait 3 sec\",\"type\":\"n8n-nodes-base.wait\",\"position\":[920,680],\"webhookId\":\"90f31c1f-6707-4f2f-b525-d3961432cd81\",\"parameters\":{\"amount\":3},\"typeVersion\":1.1},{\"id\":\"a3cc4a50-4218-4a01-ab20-151fd707dd66\",\"name\":\"Is Ready?\",\"type\":\"n8n-nodes-base.if\",\"position\":[1340,680],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"3cf5b451-9ff5-4c2a-864f-9aa7d286871a\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.status }}\",\"rightValue\":\"Ready\"}]}},\"typeVersion\":2.2},{\""}
{"prompt": "Create an n8n workflow for: AI-Powered Social Media Amplifier &gt; Reach out to me for any setup help/consulting. Automate the curation and sharing of trending GitHub discussions from Hacker News to Twitter and LinkedIn. This workflow leverages AI to generate engaging posts, streamlining your social media content creation and distribution. How it Works Crawl Hacker News for GitHub Posts: The workflow fetches trending GitHub-related discussions from Hacker News. Extract Key Information: Relevant data such as post titles, URLs, and metadata are extracted and filtered to focus only on unposted content. Fetch Additional Details: For each GitHub post, the workflow retrieves extra information from the GitHub repository page to enrich the post content. Generate Social Media Posts: Using AI, the workflow automatically generates tailored posts for Twitter and LinkedIn based on the collected data. Post to Twitter & LinkedIn: The generated content is posted to your Twitter and LinkedIn accounts. Track and Log Posts: Each post is logged in Airtable for tracking, and its status is updated to ensure no duplicate posts are made. Telegram Notification: After posting, a summary of the posts is sent to your Telegram chat for real-time updates. Requirements n8n Account**: Set up and configured. Sign up here. API Credentials**: Valid keys for LinkedIn, Twitter, Airtable, OpenAI, and Telegram. Airtable Base**: Configured with fields such as Title, URL, Post Content, Status, and Timestamp. Get started with Airtable. Telegram Chat ID**: For receiving real-time notifications. Set Up Steps Clone the Workflow: Import the workflow into your n8n environment using the provided JSON. Configure API Credentials: Enter your API keys for LinkedIn, Twitter, Airtable, OpenAI, and Telegram into the respective nodes. Set Up Airtable Base: Create an Airtable base with fields such as Title, URL, Post Content, Status, and Timestamp. Customize Telegram Chat ID: Modify the 'Ping me' node with your Telegram chat ID to receive notifications. Run the Workflow: Activate the workflow to start the automated content curation and posting process. Additional Resources n8n AI Agentic Workflows Guide n8n AI Workflow Tutorial n8n Community Tutorial on Building an AI-Powered Telegram Bot &gt; Note: Chat GPT prompt should/can be tweaked in the step to give the desired behaviour. Sample Posts from my X and LinkedIn -** Mudit Juneja | 2 months ago | Free +2", "completion": "{\"id\":\"ZeSJSbwXI593H1Qj\",\"meta\":{\"instanceId\":\"8e1a7e3413df437923cda0e92c098469371d84f7001856e525beaff17be8b941\",\"templateCredsSetupCompleted\":true},\"name\":\"Social Media AI Agent - Telegram\",\"tags\":[],\"nodes\":[{\"id\":\"814303e0-5fe9-474e-a4ed-e4a728fd4acf\",\"name\":\"Crawl HN Home\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1540,1640],\"parameters\":{\"url\":\"https://news.ycombinator.com/\",\"options\":{\"response\":{\"response\":{\"neverError\":true,\"fullResponse\":true}}}},\"executeOnce\":true,\"typeVersion\":4.2,\"alwaysOutputData\":true},{\"id\":\"32e20b1d-b3f1-4ed2-acbf-4d5bd56b0d8b\",\"name\":\"Extract Meta\",\"type\":\"n8n-nodes-base.code\",\"position\":[-1260,1720],\"parameters\":{\"language\":\"python\",\"pythonCode\":\"# Import necessary modules\\nimport asyncio\\nimport micropip\\n\\n# Define an asynchronous function to install packages\\nasync def install_packages():\\n await micropip.install(\\\"beautifulsoup4\\\")\\n await micropip.install(\\\"simplejson\\\")\\n\\n# Run the asynchronous package installation\\nasyncio.get_event_loop().run_until_complete(install_packages())\\n\\n# Now, import the installed packages\\nimport simplejson as json\\nfrom bs4 import BeautifulSoup\\n\\n# Retrieve the HTML content from the first item in the input\\n# Assuming n8n passes data as a list of items, each with a 'json' key\\nhtml_content = items[0].get('json', {}).get('data', '')\\n\\n# Initialize BeautifulSoup with the HTML content\\nsoup = BeautifulSoup(html_content, 'html.parser')\\n\\n# Initialize a list to store metadata of GitHub posts\\ngithub_posts = []\\n\\n# Find all '"}
{"prompt": "Create an n8n workflow for: HR Job Posting and Evaluation with AI Workflow Documentation: HR Job Posting and Evaluation with AI Detailed Description The HR Job Posting and Evaluation with AI workflow is designed to streamline and enhance recruitment for technical roles, such as Automation Specialists. By automating key stages in the hiring process, this workflow ensures a seamless experience for both candidates and HR teams. From collecting applications to evaluating candidates using AI and scheduling interviews, this workflow provides an end-to-end solution for recruitment challenges. Who is this for? This workflow is ideal for: HR Professionals**: Managing multiple job postings and candidates efficiently. Recruitment Teams**: Handling large volumes of applications for technical positions. Hiring Managers**: Ensuring structured and objective candidate evaluations. What problem does this workflow solve? Time-Consuming Processes**: Automates repetitive tasks like data entry, CV management, and scheduling. Fair Candidate Evaluation**: Leverages AI to provide objective insights based on resumes and job descriptions. Streamlined Communication**: Ensures timely and personalized candidate interactions, improving their experience. What this workflow does This workflow automates the following steps: Form Submission: Collects candidate information via a structured application form. Data Storage: Stores applicant details in Airtable for centralized tracking. CV Management: Automatically uploads resumes to Google Drive for easy access and organization. AI-Powered Candidate Evaluation: Scores candidates based on their resumes and job descriptions using OpenAI, providing actionable insights. Interview Scheduling: Automates scheduling based on candidate and interviewer availability. Communication: Sends customized emails to candidates for interview invitations and feedback. Setup Prerequisites To use this workflow, you\u2019ll need: n8n Account**: To create and run the workflow. Airtable Account**: For managing applicant data. Google Drive Account**: For storing candidate CVs. OpenAI API Key**: For AI-powered candidate scoring. SMTP Email Account**: For sending candidate communications. Setup Process Airtable Configuration: Create a base in Airtable with tables for Applicants and Job Positions. Google Drive Setup: Create a folder for CV storage and ensure you have write permissions. Integrate Airtable in n8n: Use the Airtable API key to connect Airtable to n8n. Integrate Google Drive in n8n: Authorize Google Drive to enable CV storage automation. OpenAI Integration: Add your OpenAI API key to n8n for candidate scoring. Email Configuration: Set up your SMTP email account in n8n for sending notifications and invitations. How to customize this workflow Tailor the workflow to fit your unique recruitment needs: Edit Job Descriptions: Adjust the form parameters to match the specific role and qualifications. Refine AI Evaluation Criteria: Modify OpenAI prompts to reflect the skills and competencies for the desired position. Personalize Email Templates: Update email content to match your organization\u2019s tone and branding. Add New Features: Incorporate additional steps like feedback collection or integration with other HR tools. Conclusion The HR Job Posting and Evaluation with AI workflow simplifies and automates the recruitment process, enabling HR teams to focus on engaging with candidates rather than handling administrative tasks. With its powerful integrations and customization options, this workflow helps organizations hire efficiently while improving the candidate experience. Francis Njenga | 1 month ago | Free +2", "completion": "{\"id\":\"eMxH0GjgfWEvBDic\",\"meta\":{\"instanceId\":\"be27b2af86ae3a5dc19ef2a1947644c0aec45fd8c88f29daa7dea6f0ce537691\"},\"name\":\"HR Job Posting and Evaluation with AI\",\"tags\":[{\"id\":\"9ZApRtWeNXlymyQ6\",\"name\":\"HR\",\"createdAt\":\"2025-01-08T08:47:43.054Z\",\"updatedAt\":\"2025-01-08T08:47:43.054Z\"}],\"nodes\":[{\"id\":\"450e15b2-bddf-4853-b44e-822facaac14d\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-700,-80],\"webhookId\":\"18f7428c-9990-413f-aff3-bdcca1bbbe2d\",\"parameters\":{\"options\":{\"path\":\"automation-specialist-application\",\"ignoreBots\":false,\"buttonLabel\":\"Submit\",\"appendAttribution\":false,\"useWorkflowTimezone\":true},\"formTitle\":\"Job Application\",\"formFields\":{\"values\":[{\"fieldLabel\":\"First Name\",\"requiredField\":true},{\"fieldLabel\":\"Last Name\",\"requiredField\":true},{\"fieldType\":\"email\",\"fieldLabel\":\"Email\",\"requiredField\":true},{\"fieldType\":\"number\",\"fieldLabel\":\"Phone\",\"requiredField\":true},{\"fieldType\":\"number\",\"fieldLabel\":\"Years of experience\",\"requiredField\":true},{\"fieldType\":\"file\",\"fieldLabel\":\"Upload your CV\",\"requiredField\":true,\"acceptFileTypes\":\".pdf\"}]},\"formDescription\":\"=Fill this for to apply for the role Automation Specialist:\\n\\nLocation: Remote\\nExperience: Minimum 3 years\\nEmployment Type: Full-time\\n\\nJob Description:\\nWe are seeking a highly skilled Automation Specialist with at least 3 years of experience in designing and implementing workflow automation solutions. The ideal candidate will have expertise in tools such as n8n, Zapier, Make.com, or similar platforms, and a strong background in integrating APIs, streamlining processes, and enhancing operational efficiency.\\n\\nKey Responsibilities:\\n\\n Develop and implement automated workflows to optimize business processes.\\n Integrate third-party APIs and systems to"}
{"prompt": "Create an n8n workflow for: Auto-Tag Blog Posts in WordPress with AI How it works: This workflow automates tagging for WordPress posts using AI: Fetch blog post content and metadata. Generate contextually relevant tags using AI. Verify existing tags in WordPress and create new ones if necessary. Automatically update posts with accurate and optimized tags. Set up steps: Estimated time: ~15 minutes. Configure the workflow with your WordPress API credentials. Connect your content source (e.g., RSS feed or manual input). Adjust tag formatting preferences in the workflow settings. Run the workflow to ensure proper tag creation and assignment. This workflow is perfect for marketers and content managers looking to streamline their content categorization and improve SEO efficiency. Ludwig | 1 month ago | Free +2", "completion": "{\"id\":\"siXUnQhJpCJ9rHzu\",\"meta\":{\"instanceId\":\"a9f3b18652ddc96459b459de4fa8fa33252fb820a9e5a1593074f3580352864a\",\"templateCredsSetupCompleted\":true},\"name\":\"Auto-Tag Blog Posts in WordPress with AI\",\"tags\":[{\"id\":\"ijuVOmJpw5mCrzQX\",\"name\":\"marketing\",\"createdAt\":\"2025-01-28T16:42:03.029Z\",\"updatedAt\":\"2025-01-28T16:42:03.029Z\"}],\"nodes\":[{\"id\":\"0561d80b-f360-4a8e-930d-49b778833991\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[3260,480],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"yWpagxp5s8o3dlBp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"d71aec64-299c-4258-8eb4-95821d15b758\",\"name\":\"Auto-fixing Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserAutofixing\",\"position\":[3460,540],\"parameters\":{},\"typeVersion\":1},{\"id\":\"1468a001-ca7b-4726-ae31-02b28d78b07e\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[3360,680],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"yWpagxp5s8o3dlBp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"bb4221ad-94d7-4543-850f-87b83735d2a6\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[3560,760],\"parameters"}
{"prompt": "Create an n8n workflow for: Zoom AI Meeting Assistant creates mail summary, ClickUp tasks and follow-up call What this workflow does This workflow retrieves the Zoom meeting data from the last 24 hours. The transcript of the last meeting is then retrieved, processed, a summary is created using AI and sent to all participants by email. AI is then used to create tasks and follow-up appointments based on the content of the meeting. Important: You need a Zoom Workspace Pro account and must have activated Cloud Recording/Transcripts! This workflow has the following sequence: manual trigger (Can be replaced by a scheduled trigger or a webhook) retrieval of of Zoom meeting data filter the events of the last 24 hours retrieval of transcripts and extract of the text creating a meeting summary, format to html and send per mail create tasks and follow-up call (if discussed in the meeting) in ClickUp/Outlook (can be replaced by Gmail, Airtable, and so forth) via sub workflow Requirements: Zoom Workspace (via API and HTTP Request): Documentation Microsoft Outlook: Documentation ClickUp: Documentation AI API access (e.g. via OpenAI, Anthropic, Google or Ollama) SMTP access data (for sending the mail) You must set up the individual sub-workflows as separate workflows. Then set the \u201cExecute workflow trigger\u201d here. Then select the corresponding sub-workflow in the AI Agent Tools. You can select the number of domains yourself. If the data queries are not required, simply delete the corresponding tool (e.g. \u201cAnalytics_Domain_5). Feel free to contact me via LinkedIn, if you have any questions! Friedemann Schuetz | 1 month ago | Free +4", "completion": "{\"id\":\"jhNsy4dPQYw9QDaa\",\"meta\":{\"instanceId\":\"1acdaec6c8e84424b4715cf41a9f7ec057947452db21cd2e22afbc454c8711cd\",\"templateId\":\"2683\",\"templateCredsSetupCompleted\":true},\"name\":\"Zoom AI Meeting Assistant\",\"tags\":[],\"nodes\":[{\"id\":\"9b4b21aa-c746-4b94-a4dd-12736a7d4098\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2160,1040],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"EjchNb5GBqYh0Cqn\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"536e360c-d668-4f58-8670-4e78ef579dbe\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[160,460],\"parameters\":{},\"typeVersion\":1},{\"id\":\"eb2b6b98-ca3c-46a9-9d5f-9b5297441224\",\"name\":\"No Recording/Transcript available\",\"type\":\"n8n-nodes-base.stopAndError\",\"position\":[880,660],\"parameters\":{\"errorMessage\":\"={{ $json.error.cause.message }}\"},\"typeVersion\":1},{\"id\":\"33ee5d8b-a373-44a8-9777-9386cf8cf008\",\"name\":\"Zoom: Get data of last meeting\",\"type\":\"n8n-nodes-base.zoom\",\"position\":[340,460],\"parameters\":{\"filters\":{\"type\":\"scheduled\"},\"operation\":\"getAll\",\"returnAll\":true,\"authentication\":\"oAuth2\"},\"credentials\":{\"zoomOAuth2Api\":{\"id\":\"MmccxSST1g202tG2\",\"name\":\"Zoom account\"}},\"typeVersion\":1},{\"id\":\"d67d1fcb-78d1-47e5-bc0e-57"}
{"prompt": "Create an n8n workflow for: AI Fitness Coach Strava Data Analysis and Personalized Training Insights Detailed Title \"Triathlon Coach AI Workflow: Strava Data Analysis and Personalized Training Insights using n8n\" Description This n8n workflow enables you to build an AI-driven virtual triathlon coach that seamlessly integrates with Strava to analyze activity data and provide athletes with actionable training insights. The workflow processes data from activities like swimming, cycling, and running, delivers personalized feedback, and sends motivational and performance improvement advice via email or WhatsApp. Workflow Details Trigger: Strava Activity Updates Node:** Strava Trigger Purpose:** Captures updates from Strava whenever an activity is recorded or modified. The data includes metrics like distance, pace, elevation, heart rate, and more. Integration:** Uses Strava API for real-time synchronization. Step 1: Data Preprocessing Node:** Code Purpose:** Combines and flattens the raw Strava activity data into a structured format for easier processing in subsequent nodes. Logic:** A recursive function flattens JSON input to create a clean and readable structure. Step 2: AI Analysis with Google Gemini Node:** Google Gemini Chat Model Purpose:** Leverages Google Gemini's advanced language model to analyze the activity data. Functionality:** Identifies key performance metrics. Provides feedback and insights specific to the type of activity (e.g., running, swimming, or cycling). Offers tailored recommendations and motivational advice. Step 3: Generate Structured Output Node:** Structure Output Purpose:** Processes the AI-generated response to create a structured format, such as headings, paragraphs, and bullet lists. Output:** Formats the response for clear communication. Step 4: Convert to HTML Node:** Convert to HTML Purpose:** Converts the structured output into an HTML format suitable for email or other presentation methods. Output:** Ensures the response is visually appealing and easy to understand. Step 5: Send Email with Training Insights Node:** Send Email Purpose:** Sends a detailed email to the athlete with performance insights, training recommendations, and motivational messages. Integration:** Utilizes Gmail or SMTP for secure and efficient email delivery. Optional Step: WhatsApp Notifications Node:** WhatsApp Business Cloud Purpose:** Sends a summary of the activity analysis and key recommendations via WhatsApp for instant access. Integration:** Connects to WhatsApp Business Cloud for automated messaging. Additional Notes Customization: You can modify the AI prompt to adapt the recommendations to the athlete's specific goals or fitness levels. The workflow is flexible and can accommodate additional nodes for more advanced analysis or output formats. Scalability: Ideal for individual athletes or coaches managing multiple athletes. Can be expanded to include additional metrics or insights based on user preferences. Performance Metrics Handled: Swimming: SWOLF, stroke count, pace. Cycling: Cadence, power zones, elevation. Running: Pacing, stride length, heart rate zones. Implementation Steps Set Up Strava API Key: Log in to Strava Developers to generate your API key. Integrate the API key into the Strava Trigger node. Configure Google Gemini Integration: Use your Google Gemini (PaLM) API credentials in the Google Gemini Chat Model node. Customize Email and WhatsApp Messaging: Update the Send Email and WhatsApp Business Cloud nodes with the recipient\u2019s details. Automate Execution: Deploy the workflow and use n8n's scheduling features or cron jobs for periodic execution. Developer Notes Author:** Amjid Ali improvements. Resources:** See in Action: Syncbricks Youtube PayPal: Support the Developer Courses : SyncBricks LMS By using this workflow, triathletes and coaches can elevate training to the next level with AI-powered insights and actionable recommendations. Amjid Ali | 1 month ago | Free", "completion": "{\"meta\":{\"instanceId\":\"32d80f55a35a7b57f8e47a2ac19558d9f5bcec983a5519d9c29ba713ff4f12c7\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"d9e3e2af-1db4-4ef1-a12a-c56df545e09e\",\"name\":\"Strava Trigger\",\"type\":\"n8n-nodes-base.stravaTrigger\",\"position\":[-60,0],\"webhookId\":\"c656f7eb-6176-48b1-a68f-7e169699cecb\",\"parameters\":{\"event\":\"update\",\"object\":\"activity\",\"options\":{}},\"credentials\":{\"stravaOAuth2Api\":{\"id\":\"lI69z0e9sP9DBcrp\",\"name\":\"Strava account\"}},\"typeVersion\":1},{\"id\":\"344106a7-f1ce-4ef0-be60-8b0dc6c92fe4\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[560,180],\"parameters\":{\"options\":{},\"modelName\":\"models/gemini-2.0-flash-exp\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"MqxJQHgdP5sIvdos\",\"name\":\"Google Gemini(PaLM) - ali@amjid\"}},\"typeVersion\":1},{\"id\":\"5ea7c2b8-0ddc-414e-b90c-d1269e074d16\",\"name\":\"Gmail\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[1420,-200],\"webhookId\":\"70ab1218-b5a1-47e7-9e9e-89c5c4f84c15\",\"parameters\":{\"sendTo\":\"amjid@amjidali.com\",\"message\":\"={{ $json.html }}\",\"options\":{\"appendAttribution\":false},\"subject\":\"=\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"dYWFonU1YWbQ9MHf\",\"name\":\"Gmail account ali@amjidali\"}},\"typeVersion"}
{"prompt": "Create an n8n workflow for: Angie, Personal AI Assistant with Telegram Voice and Text How it works: This project creates a personal AI assistant named Angie that operates through Telegram. Angie can summarize daily emails, look up calendar entries, remind users of upcoming tasks, and retrieve contact information. The assistant can interact with users via both voice and text inputs. Step-by-step: Telegram Trigger: The workflow starts with a Telegram trigger that listens for incoming message events. The system determines if the incoming message is voice or text. If voice, the voice file is retrieved and transcribed to text using OpenAI's API Speech to Text AI Assistant: The telegram request is passed to the AI assistant (Angie). Tools Integration: The AI assistant is equipped with several tools: Get Email: Uses Gmail API to fetch recent emails, filtering by date. Get Calendar: Retrieves calendar entries for specified dates. Get Tasks: Connects to a Baserow (open-source Airtable alternative) database to fetch to-do list items. Get Contacts: Also uses Baserow to retrieve contact information. Response Generation: The AI formulates a response based on the gathered information and sends back to the user on Telegram Derek Cheung | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"2723a3a635131edfcb16103f3d4dbaadf3658e386b4762989cbf49528dccbdbd\"},\"nodes\":[{\"id\":\"c70236ea-91ab-4e47-b6f6-63a70ede5d3c\",\"name\":\"Google Calendar\",\"type\":\"n8n-nodes-base.googleCalendarTool\",\"position\":[1000,680],\"parameters\":{\"options\":{\"fields\":\"=items(summary, start(dateTime))\",\"timeMin\":\"={{$fromAI(\\\"date\\\",\\\"the date after which to fetch the messages in format YYYY-MM-DDTHH:MM:SS\\\")}}\"},\"calendar\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"derekcheungsa@gmail.com\",\"cachedResultName\":\"derekcheungsa@gmail.com\"},\"operation\":\"getAll\"},\"credentials\":{\"googleCalendarOAuth2Api\":{\"id\":\"qx8JdPX4I5Xk9c46\",\"name\":\"Google Calendar account\"}},\"typeVersion\":1.1},{\"id\":\"d2287bea-de47-4180-8ee6-55d4ab1a89da\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[760,680],\"parameters\":{\"sessionKey\":\"={{ $('Listen for incoming events').first().json.message.from.id }}\",\"sessionIdType\":\"customKey\"},\"typeVersion\":1.2},{\"id\":\"fa955731-86f6-4e4d-8604-dab5f52dee87\",\"name\":\"Get Email\",\"type\":\"n8n-nodes-base.gmailTool\",\"position\":[880,680],\"parameters\":{\"filters\":{\"labelIds\":[\"INBOX\",\"UNREAD\"],\"readStatus\":\"unread\",\"receivedAfter\":\"={{$fromAI(\\\"date\\\",\\\"the date after which to fetch the messages in format YYYY-MM-DDTHH:MM:SS\\\")}}\"},\"operation\":\"getAll\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"tojOpzEqFprdxS46\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"46511f47"}
{"prompt": "Create an n8n workflow for: AI Agent to chat with Airtable and analyze data Video Guide I prepared a detailed guide that shows the entire process of building an AI agent that integrates with Airtable data in n8n. This template covers everything from data preparation to advanced configurations. Youtube Link Who is this for? This workflow is designed for developers, data analysts, and business owners who want to create an AI-powered conversational agent integrated with Airtable datasets. It is particularly useful for users looking to enhance data interaction through chat interfaces. What problem does this workflow solve? Engaging with data stored in Airtable often requires manual navigation and time-consuming searches. This workflow allows users to interact conversationally with their datasets, retrieving essential information quickly while minimizing the need for complex queries. What this workflow does This workflow enables an AI agent to facilitate chat interactions over Airtable data. The agent can: Retrieve order records, product details, and other relevant data. Execute mathematical functions to analyze data such as calculating averages and totals. Optionally generate maps for geographic data visualization. Dynamic Data Retrieval: The agent uses user prompts to dynamically query the dataset. Memory Management: It retains context during conversations, allowing users to engage in a more natural dialogue. Search and Filter Capabilities: Users can perform tailored searches with specific parameters or filters to refine their results. Set up steps Separate workflows: Create additional workflow and move there Workflow 2. Replace credentials: Replace connections and credentials in all nodes. Start chat: Ask questions and don't forget to mention required base name. Mark Shcherbakov | 1 month ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"799d2e0c-29b9-494c-b11a-d79c7ed4a06d\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[920,480],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"zJhr5piyEwVnWtaI\",\"name\":\"OpenAi club\"}},\"typeVersion\":1},{\"id\":\"6254ef4e-9699-404e-96a4-569326cce48d\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[1160,200],\"parameters\":{\"text\":\"={{ $('When chat message received').item.json.chatInput }}\",\"agent\":\"openAiFunctionsAgent\",\"options\":{\"maxIterations\":10,\"systemMessage\":\"You are Airtable assistant. \\nYou need to process user's requests and run relevant tools for that. \\n\\nPlan and execute in right order runs of tools to get data for user's request.\\n\\nFeel free to ask questions before do actions - especially if you noticed some inconcistency in user requests that might be error/misspelling. \\n\\nIMPORTANT Always check right table and base ids before doing queries.\\n\\nIMPORTANT Use Code function to do aggregation functions that requires math like - count, sum, average and etc. Aggegation function could be recognized by words like \\\"how many\\\",\\\"count\\\",\\\"what number\\\" and etc.\\nUse Code function to generate graph and images.\\n\\nIMPORTANT If search with filter failed - try to fetch records without filter\\n\\nIMPORTANT Ask yourself before answering - am I did everything is possible? Is the answer is right? Is the answer related to user request?\\n\\nIMPORTANT Always return in response name of Base and Table where records from. \"},\"promptType\":\"define\"},\"typeVersion\":1.6},{\"id\":\"227a5427-c270-47dc-bc08-4bb321314926\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1740,"}
{"prompt": "Create an n8n workflow for: Respond to WhatsApp Messages with AI Like a Pro! This n8n template demonstrates the beginnings of building your own n8n-powered WhatsApp chatbot! Under the hood, utilise n8n's powerful AI features to handle different message types and use an AI agent to respond to the user. A powerful tool for any use-case! How it works Incoming WhatsApp Trigger provides a way to get messages into the workflow. The message received is extracted and sent through 1 of 4 branches for processing. Each processing branch uses AI to analyse, summarize or transcribe the message so that the AI agent can understand it. The supported types are text, image, audio (voice notes) and video. The AI Agent is used to generate a response generally and uses a wikipedia tool for more complex queries. Finally, the response message is sent back to the WhatsApp user using the WhatsApp node. How to use Once you have setup and configured your WhatsApp account, you'll need to activate your workflow to start processing messages. Good to know: Large media files may negatively impact workflow performance. Requirements WhatsApp Buisness account Google Gemini for LLM. Gemini is used specifically because it can accept audio and video files whereas at time of writing, many other providers like OpenAI's GPT, do not. Customising this workflow For performance reasons, consider detecting large audio and video before sending to the LLM. Pre-processing such files may allow your agent to perform better. Go beyond and create rich and engagement customer experiences by responding using images, audio and video instead of just text! Jimleuk | 4 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"38ffe41a-ecdf-4bb4-bd55-51998abab0f5\",\"name\":\"WhatsApp Trigger\",\"type\":\"n8n-nodes-base.whatsAppTrigger\",\"position\":[220,300],\"webhookId\":\"0b1b3a9b-2f6a-4f5a-8385-6365d96f4802\",\"parameters\":{\"updates\":[\"messages\"]},\"credentials\":{\"whatsAppTriggerApi\":{\"id\":\"H3uYNtpeczKMqtYm\",\"name\":\"WhatsApp OAuth account\"}},\"typeVersion\":1},{\"id\":\"a35ac268-eff0-46cd-ac4e-c9b047a3f893\",\"name\":\"Get Audio URL\",\"type\":\"n8n-nodes-base.whatsApp\",\"position\":[1020,-160],\"parameters\":{\"resource\":\"media\",\"operation\":\"mediaUrlGet\",\"mediaGetId\":\"={{ $json.audio.id }}\",\"requestOptions\":{}},\"credentials\":{\"whatsAppApi\":{\"id\":\"9SFJPeqrpChOkAmw\",\"name\":\"WhatsApp account\"}},\"typeVersion\":1},{\"id\":\"a3be543c-949c-4443-bf82-e0d00419ae23\",\"name\":\"Get Video URL\",\"type\":\"n8n-nodes-base.whatsApp\",\"position\":[1020,200],\"parameters\":{\"resource\":\"media\",\"operation\":\"mediaUrlGet\",\"mediaGetId\":\"={{ $json.video.id }}\",\"requestOptions\":{}},\"credentials\":{\"whatsAppApi\":{\"id\":\"9SFJPeqrpChOkAmw\",\"name\":\"WhatsApp account\"}},\"typeVersion\":1},{\"id\":\"dd3cd0e7-0d1e-40cf-8120-aba0d1646d6d\",\"name\":\"Get Image URL\",\"type\":\"n8n-nodes-base.whatsApp\",\"position\":[1020,540],\"parameters\":{\"resource\":\"media\",\"operation\":\"mediaUrlGet\",\"mediaGetId\":\"={{ $"}
{"prompt": "Create an n8n workflow for: Agentic Telegram AI bot with with LangChain nodes and new tools Create a Telegram bot that combines advanced AI functionalities with LangChain nodes and new tools. Nodes as tools and the HTTP request tool are a new n8n feature that extend custom workflow tool and simplify your setup. We used the workflow tool in the previous Telegram template to call the Dalle-3 model. In the new version, we've achieved similar results using the HTTP Request tool and the Telegram node tool instead. The main difference is that Telegram bot becomes more flexible. The LangChain Agent node can decide which tool to use and when. In the previous version, all steps inside the custom workflow tool were executed sequentially. \u26a0\ufe0f Note that you'd need to select the Tools Agent to work with new tools. Before launching the template, make sure to set up your OpenAI and Telegram credentials. Here\u2019s how the new Telegram bot works: Telegram Trigger listens for new messages in a specified Telegram chat. This node activates the rest of the workflow after receiving a message. AI Tool Agent receives input text, processes it using the OpenAI model and replies to a user. It addresses users by name and sends image links when an image is requested. The OpenAI GPT-4o model generates context-aware responses. You can configure the model parameters or swap this node entirely. Window buffer memory helps maintain context across conversations. It stores the last 10 interactions and ensures that the agent can access previous messages within a session. Conversations from different users are stored in different buffers. The HTTP request tool connects with OpenAI's DALL-E-3 API to generate images based on user prompts. The tool is called when the user asks for an image. Telegram node tool sends generated images back to the user in a Telegram chat. It retrieves the image from the URL returned by the DALL-E-3 model. This does not happen directly, however. The response from the HTTP request tool is first stored in the Agent\u2019s scratchpad (think of it as a short-term memory). In the next iteration, the Agent sends the updated response to the GPT model once again. The GPT model will then create a new tool request to send the image back to the user. To pass the image URL, the tool uses the new $fromAI() expression. Send final reply node sends the final response message created by the agent back to the user on Telegram. Even though the image was already passed to the user, the Agent always stops with the final response that comes from dedicated output. \u26a0\ufe0f Note, that the Agent may not adhere to the same sequence of actions in 100% of situations. For example, sometimes it could skip sending the file via the Telegram node tool and instead just send an URL in the final reply. If you have a longer series of predefined steps, it may be better to use the \u201cold\u201d custom workflow tool. This template is perfect as a starting point for building AI agentic workflow. Take a look at another agentic Telegram AI template that can handle both text and voice messages. Yulia | 3 months ago | Free", "completion": "{\"id\":\"U8EOTtZvmZPMYc6m\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"Agentic Telegram AI bot with LangChain nodes and new tools\",\"tags\":[],\"nodes\":[{\"id\":\"13b3488e-af72-4d89-bef4-e9b895e3bf76\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1640,580],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.7,\"frequencyPenalty\":0.2}},\"credentials\":{\"openAiApi\":{\"id\":\"rveqdSfp7pCRON1T\",\"name\":\"Ted's Tech Talks OpenAi\"}},\"typeVersion\":1},{\"id\":\"864937a1-43f6-4055-bdea-61ab07db9903\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1760,580],\"parameters\":{\"sessionKey\":\"=chat_with_{{ $('Listen for incoming events').first().json.message.chat.id }}\",\"contextWindowLength\":10},\"typeVersion\":1},{\"id\":\"4ef838d4-feaa-4bd3-b2c7-ccd938be4373\",\"name\":\"Listen for incoming events\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[1580,360],\"webhookId\":\"322dce18-f93e-4f86-b9b1-3305519b7834\",\"parameters\":{\"updates\":[\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"9dexJXnlVPA6wt8K\",\"name\":\"Chat & Sound\"}},\"typeVersion\":1},{\"id\":\"fed51c41-2846-4a1a-a5f5-ce121ee7fe88\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes"}
{"prompt": "Create an n8n workflow for: \u2728 Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini Important Notes: Check Legal Regulations: This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry! Workflow Description: \ud83d\ude2e\u200d\ud83d\udca8 Tired of struggling with XPath, CSS selectors, or DOM specificity when scraping ? This AI-powered solution is here to simplify your workflow! With a vision-based AI Agent, you can extract data effortlessly without worrying about how the DOM is structured. This workflow leverages a vision-based AI Agent, integrated with Google Sheets, ScrapingBee, and the Gemini-1.5-Pro model, to extract structured data from webpages. The AI Agent primarily uses screenshots for data extraction but switches to HTML scraping when necessary, ensuring high accuracy. Key Features: Google Sheets Integration**: Manage URLs to scrape and store structured results. ScrapingBee**: Capture full-page screenshots and retrieve HTML data for fallback extraction. AI-Powered Data Parsing**: Use Gemini-1.5-Pro for vision-based scraping and a Structured Output Parser to format extracted data into JSON. Token Efficiency**: HTML is converted to Markdown to optimize processing costs. This template is designed for e-commerce scraping but can be customized for various use cases. Dataki | 3 months ago | Free", "completion": "{\"id\":\"PpFVCrTiYoa35q1m\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\",\"templateCredsSetupCompleted\":true},\"name\":\"Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini\",\"tags\":[],\"nodes\":[{\"id\":\"90ac8845-342e-4fdb-ae09-cb9d169b4119\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[160,460],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7a2bfc41-1527-448d-a52c-794ca4c9e7ee\",\"name\":\"ScrapingBee- Get page HTML\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[2280,1360],\"parameters\":{\"url\":\"https://app.scrapingbee.com/api/v1\",\"options\":{},\"sendQuery\":true,\"queryParameters\":{\"parameters\":[{\"name\":\"api_key\",\"value\":\"<your_scrapingbee_apikey>\"},{\"name\":\"url\",\"value\":\"={{$json.url}}\"}]}},\"typeVersion\":4.2},{\"id\":\"a0ab6dcb-ffad-40bf-8a22-f2e152e69b00\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[2480,880],\"parameters\":{\"jsonSchemaExample\":\"[{\\n \\\"product_title\\\":\\\"The title of the product\\\",\\n \\\"product_price\\\":\\\"The price of the product\\\",\\n \\\"product_brand\\\": \\\"The brand of the product\\\",\\n \\\"promo\\\":\\\"true or false\\\",\\n \\\"promo_percentage\\\":\\\"NUM %\\\"\\n}]\"},\"typeVersion\":1.2},{\"id\":\"34f50603-a969-425d-8a1a-ec8031a5cdfd\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-lang"}
{"prompt": "Create an n8n workflow for: Automate Blog Creation in Brand Voice with AI This n8n template demonstrates a simple approach to using AI to automate the generation of blog content which aligns to your organisation's brand voice and style by using examples of previously published articles. In a way, it's quick and dirty \"training\" which can get your automated content generation strategy up and running for very little effort and cost whilst you evaluate our AI content pipeline. How it works In this demonstration, the n8n.io blog is used as the source of existing published content and 5 of the latest articles are imported via the HTTP node. The HTML node is extract the article bodies which are then converted to markdown for our LLMs. We use LLM nodes to (1) understand the article structure and writing style and (2) identify the brand voice characteristics used in the posts. These are then used as guidelines in our final LLM node when generating new articles. Finally, a draft is saved to Wordpress for human editors to review or use as starting point for their own articles. How to use Update Step 1 to fetch data from your desired blog or change to fetch existing content in a different way. Update Step 5 to provide your new article instruction. For optimal output, theme topics relevant to your brand. Requirements A source of text-heavy content is required to accurately breakdown the brand voice and article style. Don't have your own? Maybe try your competitors? OpenAI for LLM - though I recommend exploring other models which may give subjectively better results. Wordpress for blog but feel free to use other preferred publishing platforms. Customising this workflow Ideally, you'd want to \"train\" your agent on material which is similar to your output ie. your social media post may not get the best results from your blog content due to differing formats. Typically, this brand voice extraction exercise should run once and then be cached somewhere for reuse later. This would save on generation time and overall cost of the workflow. Jimleuk | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"d3159589-dbb7-4cca-91f5-09e8b2e4cba8\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[240,500],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b4b42b3f-ef30-4fc8-829d-59f8974c4168\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2180,700],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"032c3012-ed8d-44eb-94f0-35790f4b616f\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2980,460],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"bf922785-7e8f-4f93-bfff-813c16d93278\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2020,520],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"d8d4b26f-270f-4b39-a4cd-a6e4361da591\",\"name\":\"Extract Voice Characteristics\",\"type\":\"@n8n/n8n-nodes-langchain.informationExtractor\",\"position\":[2160,540],\"parameters\":{\"text\":\"=### Analyse the given content\\n\\n{{ $json.data.map(item => item.replace(/\\\\n/g, '')).join('"}
{"prompt": "Create an n8n workflow for: AI Powered Web Scraping with Jina, Google Sheets and OpenAI : the EASY way Purpose of workflow: The purpose of this workflow is to automate scraping of a website, transforming it into a structured format, and loading it directly into a Google Sheets spreadsheet. How it works: Web Scraping: Uses the Jina AI service to scrape website data and convert it into LLM-friendly text. Information Extraction: Employs an AI node to extract specific book details (title, price, availability, image URL, product URL) from the scraped data. Data Splitting: Splits the extracted information into individual book entries. Google Sheets Integration: Automatically populates a Google Sheets spreadsheet with the structured book data. Step by step setup: Set up Jina AI service: Sign up for a Jina AI account and obtain an API key. Configure the HTTP Request node: Enter the Jina AI URL with the target website. Add the API key to the request headers for authentication. Set up the Information Extractor node: Use Claude AI to generate a JSON schema for data extraction. Upload a screenshot of the target website to Claude AI. Ask Claude AI to suggest a JSON schema for extracting required information. Copy the generated schema into the Information Extractor node. Configure the Split node: Set it up to separate the extracted data into individual book entries. Set up the Google Sheets node: Create a Google Sheets spreadsheet with columns for title, price, availability, image URL, and product URL. Configure the node to map the extracted data to the appropriate columns. Derek Cheung | 3 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"c3ef40df-084e-435c-9a11-3aa0a2f94f36\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[740,520],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e0583472-a450-4582-83bc-84a014bea543\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[1640,520],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"output.results\"},\"typeVersion\":1},{\"id\":\"b8aa573d-5b63-4669-900f-bcc915b6ad41\",\"name\":\"Save to Google Sheets\",\"type\":\"n8n-nodes-base.googleSheets\",\"position\":[1900,520],\"parameters\":{\"columns\":{\"value\":{},\"schema\":[{\"id\":\"name\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"name\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"price\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"price\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"availability\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"availability\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"image\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"image\",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"link\",\"type\":\"string\",\"display\":true,\"removed\":false,\"required\":false,\"displayName\":\"link\",\"defaultMatch\":false,\"canBeUsedToMatch\":true}],\"mappingMode\":\"autoMapInputData\",\"matchingColumns\":[\"Book prices\"]},\"options\":{},\"operation\":\"append\",\"sheetName\":{\"__rl\":true,\"mode\":\"list\",\"value\":258629074,\"cachedResultUrl\":\"https://docs.google.com/spreadsheets/d/1VDbfi2PpeheD2ZlO6feX3RdMeSsm0XukQlNVW8uVcuo/edit#gid"}
{"prompt": "Create an n8n workflow for: API Schema Extractor This workflow automates the process of discovering and extracting APIs from various services, followed by generating custom schemas. It works in three distinct stages: research, extraction, and schema generation, with each stage tracking progress in a Google Sheet. \ud83d\ude4f Jim Le deserves major kudos for helping to build this sophisticated three-stage workflow that cleverly automates API documentation processing using a smart combination of web scraping, vector search, and LLM technologies. How it works Stage 1 - Research: Fetches pending services from a Google Sheet Uses Google search to find API documentation Employs Apify for web scraping to filter relevant pages Stores webpage contents and metadata in Qdrant (vector database) Updates progress status in Google Sheet (pending, ok, or error) Stage 2 - Extraction: Processes services that completed research successfully Queries vector store to identify products and offerings Further queries for relevant API documentation Uses Gemini (LLM) to extract API operations Records extracted operations in Google Sheet Updates progress status (pending, ok, or error) Stage 3 - Generation: Takes services with successful extraction Retrieves all API operations from the database Combines and groups operations into a custom schema Uploads final schema to Google Drive Updates final status in sheet with file location Ideal for: Development teams needing to catalog multiple APIs API documentation initiatives Creating standardized API schema collections Automating API discovery and documentation Accounts required: Google account (for Sheets and Drive access) Apify account (for web scraping) Qdrant database Gemini API access Set up instructions: Prepare your Google Sheets document with the services information. Here's an example of a Google Sheet \u2013 you can copy it and change or remove the values under the columns. Also, make sure to update Google Sheets nodes with the correct Google Sheet ID. Configure Google Sheets OAuth2 credentials, required third-party services (Apify, Qdrant) and Gemini. Ensure proper permissions for Google Drive access. Polina Medvedieva | 2 months ago | Free +6", "completion": "{\"nodes\":[{\"id\":\"2498bb93-176f-458c-acee-f541859df770\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[2460,2820],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c08bcf84-9336-44f9-b452-0c9469f18f48\",\"name\":\"Web Search For API Schema\",\"type\":\"n8n-nodes-base.httpRequest\",\"onError\":\"continueRegularOutput\",\"position\":[3100,3820],\"parameters\":{\"url\":\"https://api.apify.com/v2/acts/serping~fast-google-search-results-scraper/run-sync-get-dataset-items\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"authentication\":\"genericCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"searchTerms\",\"value\":\"={{\\n[\\n `site:${$json.data.url.replace(/^http[s]:\\\\/\\\\//, '').replace(/\\\\/$/, '').replace('www.', '')} \\\"${$json.data.service}\\\" api developer (intext:reference OR intext:resource) (-inurl:support OR -inurl:help) (inurl:api OR intitle:api) -filetype:pdf`\\n]\\n}}\"},{\"name\":\"resultsPerPage\",\"value\":\"={{ 10 }}\"}]},\"genericAuthType\":\"httpHeaderAuth\"},\"typeVersion\":4.2},{\"id\":\"d5b19e3a-acd0-4b06-8d77-42de1f797dba\",\"name\":\"Scrape Webpage Contents\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[3940,3720],\"parameters\":{\"url\":\"https://api.apify.com/v2/acts/apify~web-scraper/run-sync-get-dataset-items\",\"options\":{\"batching\":{\"batch\":{\"batchSize\":2,\"batchInterval\":30000}}},\"jsonBody\":\"={\\n \\\"startUrls\\\": [\\n {\\n \\\"url\\\": \\\"{{ $json.source.link }}\\\",\\n \\\"method\\\": \\\"GET\\\"\\"}
{"prompt": "Create an n8n workflow for: Summarize YouTube Videos from Transcript Who is this template for? This workflow template is designed for content creators, researchers, educators, and professionals who need quick, accurate summaries of YouTube videos. It\u2019s ideal for those looking to save time, extract key insights, or repurpose video content into concise formats for reports, studies, or social media. What does it do? The workflow automates the process of summarizing YouTube videos by extracting the transcript, analyzing the content, and generating a concise summary. It leverages AI tools to ensure accuracy and relevance, making it easier to digest lengthy videos in seconds. Why is it useful? This template saves hours of manual effort by automating video summarization, enabling users to focus on analyzing or sharing insights rather than watching entire videos. It\u2019s particularly useful for staying updated with trends, conducting research, or creating content efficiently. How does it work? The workflow integrates with YouTube\u2019s Transcript API powered by Apify Actor to fetch video transcripts, process the text using AI-powered summarization tools, and deliver a clear, concise summary. Setup Instructions You need an Apify account and an API key to connect with the Actor. Follow the steps below: Create a Free Account. Choose the appropriate Actor from the Apify search. Under the Integration tab, click on \u201cUse API endpoints.\u201d Select the API that best suits your needs. Ahmed Alnaqa | 1 month ago | Free", "completion": "{\"nodes\":[{\"id\":\"6d908a58-8893-48da-8311-8c28ebd8ec62\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-520,-280],\"parameters\":{\"color\":7,\"width\":1160,\"height\":120,\"content\":\"**Summarize YouTube videos**\\n\\nThis project automates the summarization of YouTube videos, transforming lengthy content into concise, actionable insights. By leveraging AI and workflow automation, it extracts video transcripts, analyzes key points, and generates summaries, saving time for content creators, researchers, and professionals. Perfect for staying informed, conducting research, or repurposing video content efficiently.\"},\"typeVersion\":1},{\"id\":\"98de613a-1b1e-4b46-915f-7bebcfd6a931\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-540,120],\"parameters\":{\"width\":230,\"height\":80,\"content\":\"Add the full YouTube URL. \u261d\ufe0f\\nYou can change this input to a webhook or anything else.\"},\"typeVersion\":1},{\"id\":\"064208d4-52c3-46a9-9f9f-d37258189d06\",\"name\":\"Request YouTube Transcript\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-200,-20],\"parameters\":{\"url\":\"Apify API_KEY Here ???\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"startUrls\\\": [\\n \\\"{{ $json['Full URL'] }}\\\"\\n ]\\n}\",\"sendBody\":true,\"specifyBody\":\"json\"},\"typeVersion\":4.2},{\"id\":\"ba5e52fd-18b1-4232-961c-b53b01e21202\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-280,-140],\"parameters\":{\"color\":3,\"width\":280,\"height\":340,\"content\":\"Once you follow the Setup Instructions (mentioned in the template page description), you can insert the full URL endpoint, which includes both the POST Endpoint and API Key. \ud83d\udc47\"},\"typeVersion\":1},{\"id\":\"f3caad55"}
{"prompt": "Create an n8n workflow for: Hacker News to Video Content Hacker News to Video Content Overview This workflow converts trending articles from Hacker News into engaging video content. It integrates AI-based tools to analyze, summarize, and generate multimedia content, making it ideal for content creators, educators, and marketers. Features Article Retrieval: Pulls trending articles from Hacker News. Limits the number of articles to process (configurable). Content Analysis: Uses OpenAI's GPT model to: Summarize articles. Assess their relevance to specific topics like automation or AI. Extract key image URLs. Image and Video Generation: Leonardo.ai: Creates stunning AI-generated images based on extracted prompts. RunwayML: Converts images into high-quality videos. Structured Content Creation: Parses content into structured formats for easy reuse. Generates newsletter-friendly blurbs and social media-ready captions. Cloud Integration: Uploads generated assets to: Dropbox Google Drive Microsoft OneDrive MinIO Social Media Posting (Optional): Supports posting to YouTube, X (Twitter), LinkedIn, and Instagram. Workflow Steps 1. Trigger Initiated manually via the \"Test Workflow\" button. 2. Fetch Articles Retrieves articles from Hacker News. Limits the results to avoid processing overload. 3. Content Filtering Evaluates if articles are related to AI/Automation using OpenAI's language model. 4. Image and Video Generation Generates: AI-driven image prompts via Leonardo.ai. Videos using RunwayML. 5. Asset Management Saves the output to cloud storage services or uploads directly to social media platforms. Prerequisites API Keys**: Hacker News OpenAI Leonardo.ai RunwayML Creatomate n8n Installation**: Ensure n8n is installed and configured locally or on a server. Credentials**: Set up credentials in n8n for all external services used in the workflow. Customization Replace Hacker News with any other data source node if needed. Configure the \"Article Analysis\" node for different topics. Adjust the cloud storage services or add custom storage options. Usage Import this workflow into your n8n instance. Configure your API credentials. Trigger the workflow manually or schedule it as needed. Check the outputs in your preferred cloud storage or social media platform. Notes Extend this workflow further by automating social media posting or newsletter integration. For any questions, refer to the official documentation or reach out to the creator. About the Creator This workflow was built by AlexK1919, an AI-native workflow automation architect. Check out the overview video for a quick demo. Tools Used Leonardo.ai** RunwayML** Creatomate** Hacker News API** OpenAI GPT** Feel free to adapt and extend this workflow to meet your specific needs! \ud83c\udf89 Alex Kim | 3 months ago | Free +9", "completion": "{\"id\":\"744G7emgZe0pXaPB\",\"meta\":{\"instanceId\":\"d868e3d040e7bda892c81b17cf446053ea25d2556fcef89cbe19dd61a3e876e9\"},\"name\":\"Hacker News to Video Template - AlexK1919\",\"tags\":[{\"id\":\"04PL2irdWYmF2Dg3\",\"name\":\"RunwayML\",\"createdAt\":\"2024-11-15T05:55:30.783Z\",\"updatedAt\":\"2024-11-15T05:55:30.783Z\"},{\"id\":\"yrY6updwSCXMsT0z\",\"name\":\"Video\",\"createdAt\":\"2024-11-15T05:55:34.333Z\",\"updatedAt\":\"2024-11-15T05:55:34.333Z\"},{\"id\":\"QsH2EXuw2e7YCv0K\",\"name\":\"OpenAI\",\"createdAt\":\"2024-11-15T04:05:20.872Z\",\"updatedAt\":\"2024-11-15T04:05:20.872Z\"},{\"id\":\"lvPj9rYRsKOHCi4J\",\"name\":\"Creatomate\",\"createdAt\":\"2024-11-19T15:59:16.134Z\",\"updatedAt\":\"2024-11-19T15:59:16.134Z\"},{\"id\":\"9LXACqpQLNtrM6or\",\"name\":\"Leonardo\",\"createdAt\":\"2024-11-19T15:59:21.368Z\",\"updatedAt\":\"2024-11-19T15:59:21.368Z\"}],\"nodes\":[{\"id\":\"c777c41b-842d-4504-a1a0-ccbb034a0fdd\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-320,300],\"parameters\":{},\"typeVersion\":1},{\"id\":\"74fafd7c-55a4-46ec-b4a8-33d46f2b5b54\",\"name\":\"Hacker News\",\"type\":\"n8n-nodes-base.hackerNews\",\"position\":[-20,300],\"parameters\":{\"resource"}
{"prompt": "Create an n8n workflow for: UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports This workflow enables marketers to generate UTM-tagged links, convert them into QR codes, and automate performance tracking in Google Analytics with scheduled reports every 7 days. This solution helps monitor traffic sources from different marketing channels and optimize campaign performance based on analytics data. Prerequisites Before implementing this workflow, ensure you have the following: Google Analytics 4 (GA4) Account & Access Ensure you have a GA4 property set up. Access to the GA4 Data API to schedule performance tracking. Refer to the Google Analytics Data API Overview for more information. Airtable Account & API Key Create an Airtable base to store UTM links, QR codes, and analytics data. Obtain an Airtable API key from your Account Settings. Detailed instructions are available in the Airtable API Authentication Guide. Step-by-Step Guide to Setting Up the Workflow 1. Generate UTM Links Create a form or interface to input: Base URL** (e.g., https://example.com) Campaign Name** (utm_campaign) Source** (utm_source) Medium** (utm_medium) Term** (Optional: utm_term) Content** (Optional: utm_content) Append UTM parameters to generate a trackable URL. 2. Store UTM Links & QR Codes in Airtable Set up an Airtable base with the following columns: UTM Link** QR Code** Campaign Name** Source** Medium** Date Created** Adjust as needed based on your tracking requirements. For guidance on setting up your Airtable base and using the API, refer to the Airtable Web API Documentation. 3. Convert UTM Links to QR Codes Use a QR code generator API (e.g., goqr.me, qrserver.com) to generate QR codes for each UTM link and store them in Airtable. 4. Schedule Google Analytics Performance Reports (Every 7 Days) Use the Google Analytics Data API to pull weekly performance reports based on UTM parameters. Extract key metrics such as: Sessions Users Bounce Rate Conversions Revenue (if applicable) Store the data in Airtable for tracking and analysis. Adjust timeframe as needed For more details on accessing and using the Google Analytics Data API, consult the Google Analytics Data API Overview. Benefits of This Workflow \u2705 Track Marketing Campaigns: Easily monitor which channels drive traffic. \u2705 Automate QR Code Creation: Seamless integration of UTM links with QR codes. \u2705 Scheduled Google Analytics Reports: No manual reporting\u2014everything runs automatically. \u2705 Improve Data-Driven Decisions: Optimize ad spend and marketing strategies based on performance insights. This version ensures proper Markdown structure, includes relevant documentation links, and improves readability. Let me know if you need any further refinements! \ud83d\ude80 Marketing Canopy | 15 days ago | Free +2", "completion": "{\"id\":\"SJrqDqTBIAyaZQkq\",\"meta\":{\"instanceId\":\"73d9d5380db181d01f4e26492c771d4cb5c4d6d109f18e2621cf49cac4c50763\",\"templateCredsSetupCompleted\":true},\"name\":\"UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports\",\"tags\":[],\"nodes\":[{\"id\":\"5efbd956-51b6-4f94-aebc-07e3e691f7eb\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-180,480],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\"},\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"95QGJD3XSz0piaNU\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"a1acd323-ed07-41b4-a51e-614afe361893\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[0,480],\"parameters\":{\"sessionKey\":\"={{ $json.timestamp }}\",\"sessionIdType\":\"customKey\",\"contextWindowLength\":200},\"typeVersion\":1.3},{\"id\":\"c3c2b5fa-c294-4306-a050-dccd592477fa\",\"name\":\"Google Analytics\",\"type\":\"n8n-nodes-base.googleAnalyticsTool\",\"position\":[160,480],\"parameters\":{\"metricsGA4\":{\"metricValues\":[{\"listName\":\"sessions\"}]},\"propertyId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"404306108\",\"cachedResultUrl\":\"https://analytics.google.com/analytics/web/#/p404306108/\",\"cachedResultName\":\"East Coast Concrete Coating\"},\"dimensionsGA4\":{\"dimensionValues\":[{},{\"listName\":\"sourceMedium\"}]},\"additionalFields\":{}},\"credentials\":{\"googleAnalyticsOAuth2\":{\"id\":\"sVZ61SpNfC2D1Z7V\",\"name\":\"Google"}
{"prompt": "Create an n8n workflow for: Microsoft Outlook AI Email Assistant with contact support from Monday and Airtable Microsoft Outlook AI Email Assistant Prerequisites 1. Microsoft 365 Login Credentials Provide your Office 365 credentials to connect Outlook. 2. Monday.com Generate an API token and have a board with your contact details. 3. Airtable Obtain an API key (or personal access token) and set up a base to store: Contacts (populated by the Monday.com sync). Rules & Categories (used by the AI Email Assistant). Use this Airtable base as the template: Airtable AI Email Assistant Template. Define your own rules, categories, and delete rules. 4. OpenAI API Key Sign up for OpenAI if you don\u2019t already have an account. Generate a new API key at OpenAI API Keys. What the System Does 1. Daily Contact Sync (Monday.com \u2192 Airtable) Runs once a day to pull the latest contacts from Monday.com and store or update them in Airtable. 2. AI Email Categorisation & Prioritisation Fetches Outlook emails with filters. Cleans and processes email content. Matches emails with known contacts from Airtable. Uses an AI agent to classify, categorise, and prioritise emails. Updates Outlook categories and importance based on AI results. Runs in parallel with Airtable rules & categories retrieval for real-time decision-making. Workflow 1: Daily Contact Sync (Monday.com \u2192 Airtable) Purpose Keep Airtable\u2019s Contacts table up to date by pulling new or updated contact data from Monday.com daily. Steps Schedule Trigger Runs at a set interval (daily) to initiate contact syncing. Monday.com: Get Contacts Reads the specified board/columns from Monday.com where you store contact details. Airtable - Contacts Upserts (adds or updates) the fetched Monday.com data into Airtable\u2019s Contacts table. Ensures daily updates reflect changes from Monday.com. Result A consolidated contact list in Airtable, ready for AI email categorisation. Workflow 2: Categorise & Prioritise Outlook Emails Purpose Fetches Outlook emails, cleans and processes their content, matches senders with known contacts, and uses AI to categorise and prioritise them. Steps 1. Get Outlook Emails with Filters Trigger**: Either scheduled (Check Mail Schedule Trigger) or manual (Test Workflow). Outlook Filters**: Not flagged (flag/flagStatus == 'notFlagged'). Not categorised (not categories/any()). \ud83d\udd39 Result: A batch of fresh, unprocessed emails ready for processing. 2. Sanitise Email Convert to Markdown: Strips **HTML tags and normalises formatting. Email Messages Processing: Allows manual removal of **signatures, disclaimers, or extra content. \ud83d\udd39 Result: A clean, AI-friendly email for categorisation. 3. Match Contact Loop Over Emails**: Iterates over each email. Contact Lookup: Checks Airtable\u2019s **Contacts table (updated daily). Merge Data: Enriches emails with known **client, supplier, or internal team info. \ud83d\udd39 Result: Enhanced email context for AI processing. 4. AI Agent to Categorise & Prioritise Retrieve Rules & Categories** Reads Rules, Categories, and Delete Rules from Airtable in parallel. AI: Analyse Email (Tools Agent)** Uses email text, sender info, and rules to build a structured AI prompt. OpenAI Chat Model** Processes the AI prompt and outputs: Category Subcategory (optional) Priority level Short rationale Structured Output Parser** Ensures AI response is valid JSON format. \ud83d\udd39 Result: Each email is labelled, categorised, and prioritised with AI-driven logic. 5. Set Outlook Category & Importance Set Category: Updates Outlook with the assigned **category. Check Priority Conditions** (If Node): If Action Required or from a VIP, mark as High Priority. Set Importance: Updates the email's **importance flag in Outlook. \ud83d\udd39 Result: Outlook is updated with categories & importance based on AI recommendations. Parallel Processing: Retrieve Rules & Categories Runs alongside the email categorisation workflow. Ensures Airtable-based rules are available before AI processing. Steps Airtable: Get Rules & Categories Fetches Rules, Categories, and Delete Rules from Airtable. Delete Rules (Optional) If a delete rule matches, the email is removed. \ud83d\udd39 Result: A dynamic, updatable rule system ensuring emails are handled properly. Final Outcome Daily Contact Sync** keeps contacts up to date. AI-driven email workflow** ensures smart categorisation. Outlook automatically updated** with AI-generated categories and importance. This automated system saves time, ensures efficient inbox management, and allows for customisable rules via Airtable. Cognitive Creators | 1 month ago | Free", "completion": "{\"id\":\"reQhibpNwU63Y8sn\",\"meta\":{\"instanceId\":\"2128095e13afd30151f0fb53632960213a789cd45ed0afd3a7fb96a985bb4bcf\",\"templateId\":\"2454\",\"templateCredsSetupCompleted\":true},\"name\":\"Microsoft Outlook AI Email Assistant\",\"tags\":[],\"nodes\":[{\"id\":\"a923cfb0-64fe-499a-8f0e-13fc848731df\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[980,540],\"parameters\":{},\"typeVersion\":1},{\"id\":\"ea865c8e-5c73-4d37-97d1-0349a265b9a4\",\"name\":\"Sticky Note8\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[2880,-600],\"parameters\":{\"color\":5,\"width\":675,\"height\":107,\"content\":\"# Microsoft Outlook AI Email Assistant\"},\"typeVersion\":1},{\"id\":\"c835042f-421b-44a0-8dc4-686ac638b358\",\"name\":\"Sticky Note10\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1300,60],\"parameters\":{\"width\":612,\"height\":401,\"content\":\"## Outlook Business with filters\\nFilters:\\n```\\nflag/flagStatus eq 'notFlagged' and not categories/any()\\n```\\n\\nThese filters ensure we do not process flagged emails or email that already have a category set.\"},\"typeVersion\":1},{\"id\":\"51ae8a4e-2d37-4118-a538-cd0fd4f427f7\",\"name\":\"Microsoft Outlook23\",\"type\":\"n8n-nodes-base.microsoftOutlook\",\"position\":[1540,240],\"parameters\":{\"limit\":10,\"fields\":[\"flag\",\"from\",\"importance\",\"replyTo\",\"sender\",\"subject\",\"toRecipients\",\"body\",\"categories\",\"isRead\"],\"output\":\"fields\",\"options\":{},\"filtersUI\":{\"values\":{\"filters\":{\"custom\":\"flag/flagStatus eq 'notFlagged' and not categories/any()\",\"foldersToInclude\":[\"AAMk"}
{"prompt": "Create an n8n workflow for: Telegram AI bot assistant: ready-made template for voice & text messages Free template for voice & text messages with short-term memory This n8n workflow template is a blueprint for an AI Telegram bot that processes both voice and text messages. Ready to use with minimal setup. The bot remembers the last several messages (10 by default), understands commands and provides responses in HTML. You can easily swap GPT-4 and Whisper for other language and speech-to-text models to suit your needs. Core Features Text: send or forward messages Voice: transcription via Whisper Extend this template by adding LangChain tools. Requirements Telegram Bot API OpenAI API (for GPT-4 and Whisper) \ud83d\udca1 New to Telegram bots? Check our step-by-step guide on creating your first bot and setting up OpenAI access. Use Cases Personal AI assistant Customer support automation Knowledge base interface Integration hub for services that you use: Connect to any API via HTTP Request Tool Trigger other n8n workflows with Workflow Tool Yulia | 3 months ago | Free", "completion": "{\"id\":\"HJwTWtzlhK8Q5SOv\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"Telegram AI multi-format chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"65196267-0d57-4af4-9081-962701478146\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[660,640],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.7,\"frequencyPenalty\":0.2}},\"credentials\":{\"openAiApi\":{\"id\":\"rveqdSfp7pCRON1T\",\"name\":\"Ted's Tech Talks OpenAi\"}},\"typeVersion\":1},{\"id\":\"fc446ef0-2f15-42e7-a993-7960d76d8876\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[800,640],\"parameters\":{\"sessionKey\":\"=chat_with_{{ $('Listen for incoming events').first().json.message.chat.id }}\",\"contextWindowLength\":10},\"typeVersion\":1},{\"id\":\"51c3cddd-fc21-4fff-b615-ea7080c47947\",\"name\":\"Correct errors\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1220,580],\"parameters\":{\"text\":\"={{ $('AI Agent').item.json.output.replace(/&/g, \\\"&amp;\\\").replace(/>/g, \\\"&gt;\\\").replace(/</g, \\\"&lt;\\\").replace(/\\\"/g, \\\"&quot;\\\") }}\",\"chatId\":\"={{ $('Listen for incoming events').first().json.message.from.id }}\",\"additionalFields\":{\"parse_mode\":\"HTML\",\"appendAttribution\":false}},\"credentials\":{\"telegramApi\":{\"id\":\"9dexJXnlVPA6wt8K\",\"name\":\"Chat &"}
{"prompt": "Create an n8n workflow for: Extract personal data with self-hosted LLM Mistral NeMo This workflow shows how to use a self-hosted Large Language Model (LLM) with n8n's LangChain integration to extract personal information from user input. This is particularly useful for enterprise environments where data privacy is crucial, as it allows sensitive information to be processed locally. \ud83d\udcd6 For a detailed explanation and more insights on using open-source LLMs with n8n, take a look at our comprehensive guide on open-source LLMs. \ud83d\udd11 Key Features Local LLM Connect Ollama to run Mistral NeMo LLM locally Provide a foundation for compliant data processing, keeping sensitive information on-premises Data extraction Convert unstructured text to a consistent JSON format Adjust the JSON schema to meet your specific data extraction needs. Error handling Implement auto-fixing for LLM outputs Include error output for further processing \u2699\ufe0f Setup and \u0441onfiguration Prerequisites n8n AI Starter Kit installed Configuration steps Add the Basic LLM Chain node with system prompts. Set up the Ollama Chat Model with optimized parameters. Define the JSON schema in the Structured Output Parser node. \ud83d\udd0d Further resources Run LLMs locally with n8n Video tutorial on using local AI with n8n Apply the power of self-hosted LLMs in your n8n workflows while maintaining control over your data processing pipeline! Yulia | 1 month ago | Free", "completion": "{\"id\":\"HMoUOg8J7RzEcslH\",\"meta\":{\"instanceId\":\"3f91626b10fcfa8a3d3ab8655534ff3e94151838fd2709ecd2dcb14afb3d061a\",\"templateCredsSetupCompleted\":true},\"name\":\"Extract personal data with a self-hosted LLM Mistral NeMo\",\"tags\":[],\"nodes\":[{\"id\":\"7e67ae65-88aa-4e48-aa63-2d3a4208cf4b\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-500,20],\"webhookId\":\"3a7b0ea1-47f3-4a94-8ff2-f5e1f3d9dc32\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"e064921c-69e6-4cfe-a86e-4e3aa3a5314a\",\"name\":\"Ollama Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOllama\",\"position\":[-280,420],\"parameters\":{\"model\":\"mistral-nemo:latest\",\"options\":{\"useMLock\":true,\"keepAlive\":\"2h\",\"temperature\":0.1}},\"credentials\":{\"ollamaApi\":{\"id\":\"vgKP7LGys9TXZ0KK\",\"name\":\"Ollama account\"}},\"typeVersion\":1},{\"id\":\"fe1379da-a12e-4051-af91-9d67a7c9a76b\",\"name\":\"Auto-fixing Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserAutofixing\",\"position\":[-200,220],\"parameters\":{\"options\":{\"prompt\":\"Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions"}
{"prompt": "Create an n8n workflow for: Summarize the New Documents from Google Drive and Save Summary in Google Sheet This workflow is created by AI developers at WeblineIndia. It streamlines the process of managing content by automatically identifying and fetching the most recently added Google Doc file from your Google Drive. It extracts the content of the document for processing and leverages an AI model to generate a concise and meaningful summary of the extracted text. The summarized content is then stored in a designated Google Sheet, alongside relevant details like the document name and the date it was added, providing an organized and easily accessible reference for future use. This automation simplifies document handling, enhances productivity, and ensures seamless data management. Steps : Fetch the Most Recent Document from Google Drive Action:** Use the Google Drive Node. Details:** List files, filter by date to fetch the most recently added .doc file, and retrieve its file ID and metadata. Extract Content from the Document Action:** Use the Google Docs Node. Details:** Set the operation to \"Get Content,\" pass the file ID, and extract the document's text content. Summarize the Document Using an AI Model Action:** Use an AI Model Node (e.g., OpenAI, ChatGPT). Details:** Provide the extracted text to the AI model, use a prompt to generate a summary, and capture the result. Store the Summarized Content in Google Sheets Action:** Use the Google Sheets Node. Details:** Append a new row to the target sheet with details such as the original document name, summary, and date added. About WeblineIndia WeblineIndia specializes in delivering innovative and custom AI solutions to simplify and automate business processes. If you need any help, please reach out to us. WeblineIndia | 1 month ago | Free", "completion": "{\"id\":\"s8YgrWCxnGJxbctt\",\"meta\":{\"instanceId\":\"2b1c62c6d8c9216d51c1f40c64044e24b558ea8311c19d032d1278472159cfec\",\"templateId\":\"1750\"},\"name\":\"Google Doc Summarizer to Google Sheets\",\"tags\":[],\"nodes\":[{\"id\":\"9098b59a-68b1-48bd-9b52-41a971e689b3\",\"name\":\"Google Docs\",\"type\":\"n8n-nodes-base.googleDocs\",\"position\":[340,240],\"parameters\":{\"operation\":\"get\",\"documentURL\":\"={{ $json.id }}\",\"authentication\":\"serviceAccount\"},\"credentials\":{\"googleApi\":{\"id\":\"Xx4ObVZ3yYoA5XCx\",\"name\":\"Google Drive account\"}},\"typeVersion\":2},{\"id\":\"a7f224d4-232b-4201-82a0-d762830b546a\",\"name\":\"Wikipedia\",\"type\":\"@n8n/n8n-nodes-langchain.toolWikipedia\",\"position\":[680,180],\"parameters\":{},\"typeVersion\":1},{\"id\":\"12bb798e-fe7e-4340-846b-5caeb824959b\",\"name\":\"Calculator\",\"type\":\"@n8n/n8n-nodes-langchain.toolCalculator\",\"position\":[940,180],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7d479725-f973-45c5-a798-d1868aefdd82\",\"name\":\"Google Sheets\",\"type\":\"n8n-nodes-base.googleSheets\",\"position\":[1280,280],\"parameters\":{\"columns\":{\"value\":{\"Name\":\"={{ $('Google Drive ').item.json.lastModifyingUser.displayName }}\",\"Email \":\"={{ $('Google Drive ').item.json.lastModifyingUser.emailAddress }}\",\"Summarise Conetent data \":\"={{ $json.message.content }}\"},\"schema\":[{\"id\":\"Email \",\"type\":\"string\",\"display\":true,\"required\":false,\"displayName\":\"Email \",\"defaultMatch\":false,\"canBeUsedToMatch\":true},{\"id\":\"Name\",\"type\":\""}
{"prompt": "Create an n8n workflow for: Extract text from PDF and image using Vertex AI (Gemini) into CSV Case Study I'm too lazy to record every transaction for my expense tracking. Since all my expenses are digital, I just extract the transactions from bank PDF statements and screenshots into CSV to import into my budgeting software. Read more -&gt; How I used A.I. to track all my expenses What this workflow does Upload your PDF or screenshots into Google Drive It then passes the PDF/image to Vertex Gemini to do some A.I. image recognition It then sends the transactions as CSV and stores it into another Google Drive folder Setup Set up 2 google drive folders. 1 for uploading and 1 for the output. Input your Google Drive crendtials Input your Vertex Gemini credentials How to adjust it to your needs You can upload other types of documents for information extraction. You can extract any text data from any image or PDF You can adjust the A.I. prompt to do different things Keith Rumjahn | 2 months ago | Free", "completion": "{\"id\":\"sUIPemKdKqmUQFt6\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Extract text from PDF and image using Vertex AI (Gemini) into CSV\",\"tags\":[],\"nodes\":[{\"id\":\"f60ef5f9-bc08-4cc9-804e-697ae6f88b9b\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[980,920],\"parameters\":{\"options\":{},\"modelName\":\"models/gemini-1.5-pro-latest\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"hmNTKSKfppgtDbM5\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"81d3f7b8-20cb-4aac-82a9-d4e8e6581105\",\"name\":\"Get PDF or Images\",\"type\":\"n8n-nodes-base.googleDriveTrigger\",\"position\":[220,420],\"parameters\":{\"event\":\"fileCreated\",\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]},\"triggerOn\":\"specificFolder\",\"folderToWatch\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"1HOeRP5iwccg93UPUYmWYD7DyDmRREkhj\",\"cachedResultUrl\":\"https://drive.google.com/drive/folders/1HOeRP5iwccg93UPUYmWYD7DyDmRREkhj\",\"cachedResultName\":\"Actual Budget\"},\"authentication\":\"serviceAccount\"},\"credentials\":{\"googleApi\":{\"id\":\"axkK6IN61bEAT6GM\",\"name\":\"Google Service Account account\"}},\"typeVersion\":1},{\"id\":\"fe9a8228-7950-4e2c-8982-328e03725782\",\"name\":\"Route based on PDF or Image\",\"type\":\"n8n-nodes-base.switch\",\"position\":[480,420],\"parameters\":{\"rules\":{\"rules\":[{\"value"}
{"prompt": "Create an n8n workflow for: AI Data Extraction with Dynamic Prompts and Airtable This n8n template introduces the Dynamic Prompts Ai workflow pattern which are incredible for certain types of data extraction tasks where attributes are unknown or need to remain flexible. The general idea behind this pattern is that the prompts for requested attributes to be extracted live outside the template and so can be changed at any time - without needing to edit the template. This seriously cuts down on maintainance requirements and is reusable for any number of tables at little cost. Check out the video demo I did for n8n Studio here: https://www.youtube.com/watch?v=_fNAD1u8BZw Check out the example Airtable here: https://airtable.com/appAyH3GCBJ56cfXl/shrXzR1Tj99kuQbyL Looking for the Baserow Version? https://n8n.io/workflows/2780-ai-data-extraction-with-dynamic-prompts-and-baserow/ How it works Given we have an \"input\" field for context and a number of fields for the data we want to extract, this template will run in the background to react to any changes to either the \"input\" or fields and automatically update the rows accordingly. The key is that Airtable fields have a special property called the \"field description\". In this pattern, we use this property to allow the user to store a simple prompt describing the data that should exist in the column. Our n8n template reads these column descriptions aka \"prompts\" to use as instructions to perform tasks on the \"input\". In this template, the \"input\" is a PDF of a resume/CV and the columns are attributes a HR person would want to extract from it - such as full name, address, last position, years of experience etc. How to use First publish this template and ensure it's accessible via webhook URL. You then have to run the \"create airtable webhooks\" mini-flow to configure your Airtable to send change events to the n8n template. This mini-flow exists in the template but you'll have to update the IDs. Check the template for more instructions. Requirements Airtable for Tables/Database OpenAI for LLM and extraction. Feel free to choose another LLM if preferred. Customising this workflow If you're not using files, you can replace the \"input\" field with anything you like. For example, the \"input\" could be single line text. Jimleuk | 1 month ago | Free", "completion": "{\"nodes\":[{\"id\":\"36816ae7-414a-482e-8a50-021885237273\",\"name\":\"Event Type\",\"type\":\"n8n-nodes-base.switch\",\"position\":[-220,-140],\"parameters\":{\"rules\":{\"values\":[{\"outputKey\":\"row.updated\",\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"2162daf8-d23d-4b8f-8257-bdfc5400a3a8\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.event_type }}\",\"rightValue\":\"row.updated\"}]},\"renameOutput\":true},{\"outputKey\":\"field.created\",\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"48e112f6-afe8-40bf-b673-b37446934a62\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.event_type }}\",\"rightValue\":\"field.created\"}]},\"renameOutput\":true},{\"outputKey\":\"field.updated\",\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"5aa258cd-15c2-4156-a32d-afeed662a38e\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.event_type }}\",\"rightValue\":\"field.updated\"}]},\"renameOutput\":true}]},\"options\":{}},\"typeVersion\":3.2},{\"id\":\"920ca6d8-7a6e-4482-b003-fa643f550a85\",\"name\":\"Get Prompt Fields\",\"type\":\"n8n-nodes-base.code\",\"position\":[-900,-140],\"parameters\":{\"jsCode\":\"const fields = $input.first().json.fields\\n .filter(item => item.description)\\n .map((item, idx) => ({\\"}
{"prompt": "Create an n8n workflow for: Conversational Interviews with AI Agents and n8n Forms This n8n template combines an AI agent with n8n's multi-page forms to create a novel interaction which allows automated question-and-answer sessions. One of the more obvious use-cases of this interaction is what I'm calling the AI interviewer. You can read the full post here: https://community.n8n.io/t/build-your-own-ai-interview-agents-with-n8n-forms/62312 Live demo here: https://jimleuk.app.n8n.cloud/form/driving-lessons-survey How it works A form trigger is used to start the interview and a new session is created in redis to capture the transcript. An AI agent is then tasked to ask questions to the user regarding the topic of the interview. This is setup as a loop so the questions never stop unless the user wishes to end the interview. Each answer is recorded in our session set up earlier between questions. When the user requests to end the interview we break the loop and show the interview completion screen. Finally, the session is then saved in a Google Sheet which can then be shared with team members and for the purpose of data analysis. How to use You'll need to be on a n8n instance that is accessible to your target audience. Not technical enough to setup your own server? Try out n8n cloud and instantly deploy template! Remember to activate the workflow so the form trigger is published and available for users to use. Requirements Groq LLM for AI agent. Feel free to swap this out for any other LLM. Redis(-compatible) storage for capturing sessions Customising this workflow The next step would be adding tools! AI interviews with knowledge retrieval could definitely open up other possibilities. Eg. An onboarding wizard generating questions by pulling facts from internal knowledgebase. Jimleuk | 3 months ago | Free +3", "completion": "{\"nodes\":[{\"id\":\"d73e5113-119f-4e62-9872-48e6a971d760\",\"name\":\"Stop Interview?\",\"type\":\"n8n-nodes-base.if\",\"position\":[3380,920],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"3cf788a6-94d0-4223-9caa-30b8e4df8e01\",\"operator\":{\"type\":\"boolean\",\"operation\":\"true\",\"singleValue\":true},\"leftValue\":\"={{ $json.output.stop_interview }}\",\"rightValue\":\"\"}]}},\"typeVersion\":2.2},{\"id\":\"cda3c487-97fa-4037-b9a0-0802f4a02727\",\"name\":\"Generate Row\",\"type\":\"n8n-nodes-base.set\",\"position\":[3740,1200],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"06146a75-b67a-42cf-aa6f-241f23c47b9a\",\"name\":\"timestamp\",\"type\":\"string\",\"value\":\"={{ $now.toISO() }}\"},{\"id\":\"b0278c64-58a7-487d-b7ba-d102fb5d4a0c\",\"name\":\"type\",\"type\":\"string\",\"value\":\"next_question\"},{\"id\":\"ba034ca1-408e-422f-b071-dab0ef12fb48\",\"name\":\"question\",\"type\":\"string\",\"value\":\"={{ $('Parse Response').item.json.output.question }}\"},{\"id\":\"a2231f6e-f507-408e-b598-53888cf8d4b5\",\"name\":\"answer\",\"type\":\"string\",\"value\":\"={{ $('Get Answer').item.json.answer }}\"}]}},\"typeVersion\":3.4},{\"id\":\"3486f9ae-6a19-4f1f-be46-15376053e71f\",\"name\":\"Generate Row1\",\"type\":\"n8n-nodes-base.set\",\"position\":[3580,760],\"parameters\":{\"options\":{},\"assignments\":{\"assign"}
{"prompt": "Create an n8n workflow for: BambooHR AI-Powered Company Policies and Benefits Chatbot How it works This workflow enables companies to provide instant HR support by automating responses to employee queries about policies and benefits: Retrieves company policies, benefits, and HR documents from BambooHR. Uses AI to analyze and answer employee questions based on company records. Identifies the most relevant contact person for escalations. Seamlessly integrates with company systems to provide real-time HR assistance. Set up steps: Estimated time: ~20 minutes Connect your BambooHR account to allow policy retrieval. Configure AI parameters and access control settings. (Optional) Set up the employee lookup tool for personalized responses. Test the chatbot to ensure accurate responses and seamless integration. Benefits This workflow is perfect for HR teams looking to enhance employee support while reducing manual inquiries. Outperform BambooHR's \"Ask BambooHR\" Chatbot #1. Superior specificity of replies to general inquiries #2. More appropriate escalations when responding to sensitive employee concerns Ludwig | 27 days ago | Free +10", "completion": "{\"id\":\"dYjQS1bJmVSAxNnj\",\"meta\":{\"instanceId\":\"a9f3b18652ddc96459b459de4fa8fa33252fb820a9e5a1593074f3580352864a\",\"templateCredsSetupCompleted\":true},\"name\":\"BambooHR AI-Powered Company Policies and Benefits Chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"832e4a1d-320f-4793-be3c-8829776a3ce6\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[760,560],\"parameters\":{},\"typeVersion\":1},{\"id\":\"63be0638-d7df-4af8-ba56-555593a6de0c\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[2080,740],\"parameters\":{\"options\":{},\"dataType\":\"binary\"},\"typeVersion\":1},{\"id\":\"ffe33bb2-efd0-4b6e-a146-aaded7c28304\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1860,740],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"XXXXXX\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"32de5318-ea5d-4951-b81c-3c96167bc320\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[2060,880],\"parameters\":{\"options\":{},\"chunkOverlap\":100},\"typeVersion\":1},{\"id\":\"6306d263-16c1-4a68-9318-c58fea1e3e62\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1000,1340],\"parameters\":{},\"typeVersion\":1.2},{\"id\":\"364cf0"}
{"prompt": "Create an n8n workflow for: AI Social Media Caption Creator creates social media post captions in Airtable Welcome to my AI Social Media Caption Creator Workflow! What this workflow does This workflow automatically creates a social media post caption in an editorial plan in Airtable. It also uses background information on the target group, tonality, etc. stored in Airtable. This workflow has the following sequence: Airtable trigger (scan for new records every minute) Wait 1 Minute so the Airtable record creator has time to write the Briefing field retrieval of Airtable record data AI Agent to write a caption for a social media post. The agent is instructed to use background information stored in Airtable (such as target group, tonality, etc.) to create the post. Format the output and assign it to the correct field in Airtable. Post the caption into Airtable record. Requirements Airtable Database: Documentation AI API access (e.g. via OpenAI, Anthropic, Google or Ollama) Example of an editorial plan in Airtable: Editorial Plan example in Airtable For this workflow you need the Airtable fields \"created_at\", \"Briefing\" and \"SoMe_Text_AI\" Feel free to contact me via LinkedIn, if you have any questions! Friedemann Schuetz | 1 month ago | Free", "completion": "{\"id\":\"V8ypWn7oaOVS3zH0\",\"meta\":{\"instanceId\":\"1acdaec6c8e84424b4715cf41a9f7ec057947452db21cd2e22afbc454c8711cd\",\"templateCredsSetupCompleted\":true},\"name\":\"AI Social Media Caption Creator\",\"tags\":[],\"nodes\":[{\"id\":\"12d0470e-1030-47c4-8bd0-890d5b3a5976\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[120,-120],\"parameters\":{\"text\":\"={{ $json['Briefing'] }}\",\"options\":{\"systemMessage\":\"=<system_prompt> \\nYOU ARE AN EXPERT CAPTION CREATOR AGENT FOR INSTAGRAM, DESIGNED FOR USE IN N8N WORKFLOWS. YOUR TASK IS TO CREATE A CREATIVE, TARGET AUDIENCE-ORIENTED, AND MEMORABLE CAPTION BASED ON THE BRIEFING: `{{ $json['Briefing'] }}`. YOU SHOULD RETRIEVE ADDITIONAL INFORMATION ABOUT THE TARGET AUDIENCE AND PREFERRED WORDING USING THE TOOL \\\"BACKGROUND INFO\\\" TO MAXIMIZE THE QUALITY AND RELEVANCE OF THE CAPTION. \\n\\n###INSTRUCTIONS### \\n\\n- YOU MUST: \\n 1. READ AND UNDERSTAND THE BRIEFING CAREFULLY. \\n 2. RETRIEVE ADDITIONAL DATA ABOUT THE TARGET AUDIENCE AND COMMUNICATION STYLE USING THE \\\"BACKGROUND INFO\\\" TOOL. \\n 3. CREATE A CAPTION THAT IS CREATIVE, ENGAGING, AND TAILORED TO THE TARGET AUDIENCE. \\n 4. ENSURE THAT THE CAPTION INCLUDES A CLEAR CALL-TO-ACTION (CTA) THAT ENCOURAGES USERS TO TAKE ACTION (E.G., LIKE, COMMENT, OR CLICK). \\n 5. OUTPUT ONLY THE FINAL CAPTION WITHOUT ANY ACCOMPANYING EXPLANATIONS, FEEDBACK, OR COMMENTS. \\n\\n###CHAIN OF THOUGHTS### \\n\\n1. **UNDERSTANDING THE BRIEFING**:"}
{"prompt": "Create an n8n workflow for: AI-Driven Lead Management and Inquiry Automation with ERPNext & n8n Overview This workflow template automates lead management and customer inquiry processing by integrating ERPNext, AI agents, and email notifications. It streamlines the process of capturing leads, analyzing inquiries, and generating actionable responses. The workflow uses ERPNext to capture inquiries, analyzes them with AI, and notifies the appropriate team or individual, all while maintaining a professional approach. What This Template Does ERPNext Webhook Integration: Captures leads and inquiries through ERPNext webhooks. Triggers the workflow when a new lead is created. AI-Powered Inquiry Analysis: Uses AI to extract key details from lead notes (e.g., customer name, organization, inquiry summary). Classifies inquiries as valid or invalid based on relevance to products, services, or solutions. Contact Assignment: Matches inquiries to the appropriate contact(s) using a Google Sheets database or ERPNext contact information. Handles multiple contacts if required. Email Notifications: Generates professional email notifications for valid inquiries. Sends emails to the appropriate contact(s) with inquiry details and action steps. Invalid Lead Handling: Identifies invalid inquiries (e.g., unrelated to products or services) and flags them for follow-up or dismissal. Custom Email Formatting: Converts plain text into professionally formatted HTML emails. Ensures that communication is clear, concise, and visually appealing. How It Works Step 1: Capture Lead Data Webhook in ERPNext:** Create a webhook in ERPNext for the \"Lead\" DocType. Set the trigger to on_insert to capture new leads in real-time. Lead Details:** The workflow fetches lead details, including notes, contact information, and the source of the lead. Step 2: Validate and Analyze Inquiry AI Agent for Analysis:** An AI agent analyzes the lead notes to extract key details and classify the inquiry as valid or invalid. The analysis includes checking the relevance of the inquiry to products, services, or solutions offered by the company. Invalid Leads:** If the inquiry is invalid, the workflow flags it and stops further processing. Step 3: Assign Contact(s) Google Sheets Integration:** Uses a Google Sheets database to map products, services, or solutions to responsible contacts. Ensures that inquiries are directed to the right person or team. Multiple Contacts:** Handles cases where multiple contacts are responsible for a particular product or service. Step 4: Generate and Send Email Notifications AI-Generated Emails:** The workflow generates a professional email summarizing the inquiry. Emails include details like customer name, organization, inquiry summary, and action steps. Custom HTML Formatting:** Emails are converted to HTML for a polished and professional appearance. Send Notifications:** Sends email notifications through Microsoft Outlook or another configured email client. Optionally, notifies via WhatsApp or SMS for urgent inquiries. Step 5: Post-Inquiry Actions ERPNext Record Updates:** Updates the lead record in ERPNext with relevant details, including inquiry status and contact information. Setup Instructions Prerequisites ERPNext: A configured ERPNext instance with lead data and a webhook for the \"Lead\" DocType. Google Sheets: A sheet mapping products, services, or solutions to responsible contacts. AI Integration: Credentials for OpenAI or other supported AI platforms. Email Client: Credentials for Microsoft Outlook or another email client. Step-by-Step Setup ERPNext Configuration: Create a webhook for the \"Lead\" DocType in ERPNext. Test the webhook with sample data to ensure proper integration. Workflow Import: Import the workflow template into n8n. Configure nodes with your API credentials for ERPNext, Google Sheets, and AI tools. Google Sheets Integration: Prepare a Google Sheet with columns for product, service, or solution and the responsible contact(s). Link the sheet to the workflow. AI Agent Configuration: Customize the AI agent\u2019s prompts to align with your business\u2019s products and services. Adjust criteria for valid and invalid inquiries as needed. Email Setup: Configure the email client node with your email service credentials. Customize the email template for your organization. Testing: Run the workflow with sample leads to validate the entire process. Check email notifications, contact assignments, and record updates in ERPNext. Dos and Don\u2019ts Dos: Test Thoroughly:** Test the workflow with various scenarios before deploying in production. Secure Credentials:** Keep API and email credentials secure to avoid unauthorized access. Customize Prompts:** Tailor AI prompts to match your business needs and language style. Use Professional Email Templates:** Ensure emails are clear and well-formatted. Don\u2019ts: Skip Validation:** Always validate inquiry data to avoid sending irrelevant notifications. Overload the Workflow:** Avoid adding unnecessary nodes that can slow down processing. Ignore Errors:** Monitor logs and address errors promptly for a smooth workflow. Resources YouTube Tutorial:** Watch the full step-by-step tutorial on setting up this workflow: SyncBricks YouTube Channel Courses and Training:** Learn more about ERPNext and AI automation through my comprehensive courses: SyncBricks LMS Support and Contact:** Email: amjid@amjidali.com Website: SyncBricks LinkedIn: Amjid Ali Amjid Ali | 1 month ago | Free", "completion": "{\"meta\":{\"instanceId\":\"e4f78845dfed9ddcfba1945ae00d12e9a7d76eab052afd19299228ce02349d86\"},\"nodes\":[{\"id\":\"23291d25-3e1a-4b0d-9b1d-d066e8c04a1f\",\"name\":\"Customer Lead AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[-640,460],\"parameters\":{\"text\":\"=**System Prompt:**\\n\\nYou are an AI assistant designed to process new leads and generate appropriate responses. Your role includes analyzing lead notes, categorizing them, and generating an email from the system to inform the relevant contact about the inquiry. Do not send the email as if it is directly from the customer; instead, draft it as a notification from the system summarizing the inquiry.\\n\\n### **Process Flow**\\n\\n1. **Analyzing Lead Notes:**\\n - Extract key details such as the customer name, organization, contact information, and their specific request. \\n - Determine if the inquiry relates to products, services, or solutions offered by the company.\\n\\n2. **Finding the Appropriate Contact(s):**\\n - Search the contact database to find the responsible person(s) for the relevant product, service, or solution. \\n - If one person is responsible, provide their email. \\n - If multiple people are responsible, list all emails separated by commas.\\n\\n3. **Generating an Email Notification:**\\n - Draft a professional email as a notification from the system.\\n - Summarize the customer\u2019s inquiry.\\n - Include all relevant details to assist the recipient in addressing the inquiry.\\n\\n4. **Handling Invalid Leads:**\\n - If the inquiry is unrelated to products, services, or solutions (e.g., job inquiries or general product inquiries), classify it as invalid and return: \\n `\\\"Invalid Lead - Not related to products, services, or solutions.\\\"`\\n\\n### **Output Requirements**\\n\\n1. **For Relevant Leads:**\\n - **Email Address(es):** Provide the appropriate email(s). \\n - **Email Message Body:** Generate an email notification"}
{"prompt": "Create an n8n workflow for: Make OpenAI Citation for File Retrieval RAG Make OpenAI Citation for File Retrieval RAG Use case In this example, we will ensure that all texts from the OpenAI assistant search for citations and sources in the vector store files. We can also format the output for Markdown or HTML tags. This is necessary because the assistant sometimes generates strange characters, and we can also use dynamic references such as citations 1, 2, 3, for example. What this workflow does In this workflow, we will use an OpenAI assistant created within their interface, equipped with a vector store containing some files for file retrieval. The assistant will perform the file search within the OpenAI infrastructure and will return the content with citations. We will make an HTTP request to retrieve all the details we need to format the text output. Setup Insert an OpenAI Key How to adjust it to your needs At the end of the workflow, we have a block of code that will format the output, and there we can add Markdown tags to create links. Optionally, we can transform the Markdown formatting into HTML. Davi Saranszky Mesquita | 2 months ago | Free", "completion": "{\"id\":\"5NAbfX550LJsfz6f\",\"meta\":{\"instanceId\":\"00493e38fecfc163cb182114bc2fab90114038eb9aad665a7a752d076920d3d5\",\"templateCredsSetupCompleted\":true},\"name\":\"Make OpenAI Citation for File Retrieval RAG\",\"tags\":[{\"id\":\"urxRtGxxLObZWPvX\",\"name\":\"sample\",\"createdAt\":\"2024-09-13T02:43:13.014Z\",\"updatedAt\":\"2024-09-13T02:43:13.014Z\"},{\"id\":\"nMXS3c9l1WqDwWF5\",\"name\":\"assist\",\"createdAt\":\"2024-12-23T16:09:38.737Z\",\"updatedAt\":\"2024-12-23T16:09:38.737Z\"}],\"nodes\":[{\"id\":\"b9033511-3421-467a-9bfa-73af01b99c4f\",\"name\":\"Aggregate\",\"type\":\"n8n-nodes-base.aggregate\",\"position\":[740,120],\"parameters\":{\"options\":{},\"aggregate\":\"aggregateAllItemData\"},\"typeVersion\":1,\"alwaysOutputData\":true},{\"id\":\"a61dd9d3-4faa-4878-a6f3-ba8277279002\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[980,-320],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"2daabca5-37ec-4cad-9157-29926367e1a7\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[220,320],\"parameters\":{\"color\":3,\"width\":840,\"height\":80,\"content\":\"## Within N8N, there will be a chat button to test\"},\"typeVersion\":1},{\"id\":\"bf4485b1-cd94-41c8-a183-bf1b785f2761\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-440,-520],\"parameters\":{\"color\":5,\"width\":500,\"height\":720"}
{"prompt": "Create an n8n workflow for: Create a Google Analytics Data Report with AI and sent it to E-Mail and Telegram What this workflow does This workflow retrieves Google Analytics data from the last 7 days and the same period in the previous year. The data is then prepared by AI as a table, analyzed and provided with a small summary. The summary is then sent by email to a desired address and, shortened and summarized again, sent to a Telegram account. This workflow has the following sequence: time trigger (e.g. every Monday at 7 a.m.) retrieval of Google Analytics data from the last 7 days assignment and summary of the data retrieval of Google Analytics data from the last 7 days of the previous year allocation and summary of the data preparation in tabular form and brief analysis by AI. sending the report as an email preparation in short form by AI for Telegram (optional) sending as Telegram message. Requirements The following accesses are required for the workflow: Google Analytics (via Google Analytics API): Documentation AI API access (e.g. via OpenAI, Anthropic, Google or Ollama) SMTP access data (for sending the mail) Telegram access data (optional for sending as Telegram message): Documentation Feel free to contact me via LinkedIn, if you have any questions! Friedemann Schuetz | 2 months ago | Free", "completion": "{\"id\":\"AAjX1BuwhyXpo8xP\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\"},\"name\":\"Google Analytics: Weekly Report\",\"tags\":[],\"nodes\":[{\"id\":\"91ba5982-e226-4f0b-af0d-8c9a44b08279\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[-1740,300],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\",\"triggerAtDay\":[1],\"triggerAtHour\":7}]}},\"typeVersion\":1.2},{\"id\":\"62c38eaf-2222-4d22-8589-677f36bce10d\",\"name\":\"Google Analytics Letzte 7 Tage\",\"type\":\"n8n-nodes-base.googleAnalytics\",\"position\":[-1540,300],\"parameters\":{\"metricsGA4\":{\"metricValues\":[{\"listName\":\"screenPageViews\"},{},{\"listName\":\"sessions\"},{\"listName\":\"sessionsPerUser\"},{\"name\":\"averageSessionDuration\",\"listName\":\"other\"},{\"name\":\"ecommercePurchases\",\"listName\":\"other\"},{\"name\":\"averagePurchaseRevenue\",\"listName\":\"other\"},{\"name\":\"purchaseRevenue\",\"listName\":\"other\"}]},\"propertyId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"345060083\",\"cachedResultUrl\":\"https://analytics.google.com/analytics/web/#/p345060083/\",\"cachedResultName\":\"https://www.ep-reisen.de \u2013 GA4\"},\"dimensionsGA4\":{\"dimensionValues\":[{}]},\"additionalFields\":{}},\"credentials\":{\"googleAnalyticsOAuth2\":{\"id\":\"onRKXREI8izfGzv0\",\"name\":\"Google Analytics account\"}},\"typeVersion\":2},{\"id\":\"0a51c2f3-a487-4226-884f-63d4cb2bf4e4\",\"name\":\"Send Email\",\"type\":\"n8n-nodes-base.emailSend\",\"position\":[420,80],\"parameters\":{\"html\":\"={{ $json.message.content }}\",\"options\":{},\"subject\":\"Weekly"}
{"prompt": "Create an n8n workflow for: Flux AI Image Generator Easily generate images with Black Forest's Flux Text-to-Image AI models using Hugging Face\u2019s Inference API. This template serves a webform where you can enter prompts and select predefined visual styles that are customizable with no-code. The workflow integrates seamlessly with Hugging Face's free tier, and it\u2019s easy to modify for any Text-to-Image model that supports API access. Try it Curious what this template does? Try a public version here: https://devrel.app.n8n.cloud/form/flux Set Up Watch this quick set up video \ud83d\udc47 Accounts required Huggingface.co account (free) Cloudflare.com account (free - used for storage; but can be swapped easily e.g. GDrive) Key Features: Text-to-Image Creation**: Generates unique visuals based on your prompt and style. Hugging Face Integration**: Utilizes Hugging Face\u2019s Inference API for reliable image generation. Customizable Visual Styles**: Select from preset styles or easily add your own. Adaptable**: Swap in any Hugging Face Text-to-Image model that supports API calls. Ideal for: Creators**: Rapidly create visuals for projects. Marketers**: Prototype campaign visuals. Developers**: Test different AI image models effortlessly. How It Works: You submit an image prompt via the webform and select a visual style, which appends style instructions to your prompt. The Hugging Face Inference API then generates and returns the image, which gets hosted on Cloudflare S3. The workflow can be easily adjusted to use other models and styles for complete flexibility. Max Tkacz | 5 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"6abe578b-d503-4da5-9af8-f9977de71139\",\"name\":\"Vivid Pop Explosion\",\"type\":\"n8n-nodes-base.set\",\"notes\":\" \",\"position\":[380,980],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"9ec60f33-b940-40a6-9f8a-cb944b7065f1\",\"name\":\"stylePrompt\",\"type\":\"string\",\"value\":\"=rule of thirds, golden ratio, hyper-maximalist, vibrant neon, high-contrast, octane render, photorealism, 8k ::7 --ar 16:9 --s 1000\\n\\nDesign a fun, energetic scene filled with bold, neon colors, and playful shapes that pop off the screen. The image should evoke a sense of joy and movement, using fluid, organic forms and exaggerated, cartoon-like proportions. Focus on creating a lively atmosphere with contrasting, saturated tones and dynamic lighting. Use a mix of asymmetrical and balanced compositions to create a playful visual flow. Render in 8K with a hyper-maximalist approach using Octane Render for vibrant, high-gloss textures and photorealistic lighting effects. Include:\"}]},\"includeOtherFields\":true},\"notesInFlow\":true,\"typeVersion\":3.4},{\"id\":\"7de1ea42-3b18-4bfb-8ea4-a8b6c8d16763\",\"name\":\"AI Dystopia\",\"type\":\"n8n-nodes-base.set\",\"notes\":\" \",\"position\":[380,620],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"9ec60f33-b940-40a6-9f8a-cb944b7065f1\",\"name\":\"stylePrompt\",\"type\":\"string\",\"value\":\"=golden ratio, rule of thirds, cyberpunk, glitch art, octane render, cinematic realism, 8k ::7 --ar 16:9 --s 1000\\n\\nGenerate a futuristic, cyberpunk dystopia with metallic textures, digital glitches, and neon lights. Blend cold, dystopian structures with traces of organic life. Use photorealistic lighting and dynamic reflections to enhance the visual depth of the scene. Include:\"}]},\"includeOtherFields\":true},\"notesInFlow"}
{"prompt": "Create an n8n workflow for: Daily meetings summarization with Gemini AI This workflow implements the Gemini AI chat model to summarize your daily meetings and send the summary to a Slack channel daily at 9 AM (or any other time you choose). It automatically retrieves your Google Calendar events and feeds them to the model. The workflow uses Google\u2019s Gemini AI for response generation. How it works The workflow uses a Scheduled Trigger Node as the main trigger. The AI Agent Node uses the Google Calendar action to retrieve relevant meeting data. The AI Agent sends the retrieved information to the Google Gemini Chat Model (gemini-flash). The Google Gemini Chat Model generates a summary and informative response based on today\u2019s meetings. ++Setup Steps++ Google Cloud Project and Vertex AI API: Create a Google Cloud project. Enable the Vertex AI API for your project. Google AI API Key: Obtain a Google AI API key from Google AI Studio. Credentials in n8n: Configure credentials in your n8n environment for: Google Gemini (PaLM) API (using your Google AI API key). Import the Workflow: Import this workflow into your n8n instance. Configure the Workflow: Update both Slack and Gemini nodes with your credentials. Johnny Rafael | 1 month ago | Free", "completion": "{\"id\":\"jAML9xW28lOdsObH\",\"meta\":{\"instanceId\":\"be04c66ddabda64dad2c5d4c4611c3879370cfcff746359dfed22dbbfaacfc1a\",\"templateCredsSetupCompleted\":true},\"name\":\"Daily meetings summarization with Gemini AI\",\"tags\":[],\"nodes\":[{\"id\":\"2f5c6f8b-023a-4fc0-8684-66d7f743af0a\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[100,380],\"parameters\":{\"color\":4,\"width\":217.47708894878716,\"height\":233,\"content\":\"### Gemini Flash model a base\"},\"typeVersion\":1},{\"id\":\"8c159251-d78c-4f18-a886-b930194e6459\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[600,40],\"parameters\":{\"color\":4,\"width\":223.7196765498655,\"height\":236.66152029520293,\"content\":\"### Send the response from AI back to slack channel\\n\"},\"typeVersion\":1},{\"id\":\"ee7164d8-f257-4e47-9867-239440153fd4\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[0,-20],\"parameters\":{\"color\":4,\"width\":561,\"height\":360,\"content\":\"## Trigger the task daily, receive the meetings data, process the data and return response for sending\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNo memory assigned to the model since the model is running one task and doesn't need a followup, then send the data to the user.\"},\"typeVersion\":1},{\"id\":\"30ac78b7-08ba-4df9-a67c-e6825a9de380\",\"name\":\"Send response back to slack channel\",\"type\":\"n8n-nodes-base.slack\",\"position\":[660,100],\"webhookId\":\"636ae330-cc22-408b-b6a5-caf02e4889"}
{"prompt": "Create an n8n workflow for: AI Agent to chat with Supabase/PostgreSQL DB Video Guide I prepared a detailed guide that showed the whole process of building a resume analyzer. Who is this for? This workflow is ideal for developers, data analysts, and business owners who want to enable conversational interactions with their database. It\u2019s particularly useful for cases where users need to extract, analyze, or aggregate data without writing SQL queries manually. What problem does this workflow solve? Accessing and analyzing database data often requires SQL expertise or dedicated reports, which can be time-consuming. This workflow empowers users to interact with a database conversationally through an AI-powered agent. It dynamically generates SQL queries based on user requests, streamlining data retrieval and analysis. What this workflow does This workflow integrates OpenAI with a Supabase database, enabling users to interact with their data via an AI agent. The agent can: Retrieve records from the database. Extract and analyze JSON data stored in tables. Provide summaries, aggregations, or specific data points based on user queries. Dynamic SQL Querying: The agent uses user prompts to create and execute SQL queries on the database. Understand JSON Structure: The workflow identifies JSON schema from sample records, enabling the agent to parse and analyze JSON fields effectively. Database Schema Exploration: It provides the agent with tools to retrieve table structures, column details, and relationships for precise query generation. Setup Preparation Create Accounts: N8N: For workflow automation. Supabase: For database hosting and management. OpenAI: For building the conversational AI agent. Configure Database Connection: Set up a PostgreSQL database in Supabase. Use appropriate credentials (username, password, host, and database name) in your workflow. N8N Workflow AI agent with tools: Code Tool: Execute SQL queries based on user input. Database Schema Tool: Retrieve a list of all tables in the database. Use a predefined SQL query to fetch table definitions, including column names, types, and references. Table Definition: Retrieve a list of columns with types for one table. Mark Shcherbakov | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"0a4e65b7-39be-44eb-8c66-913ebfe8a87a\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1140,840],\"parameters\":{\"color\":3,\"width\":215,\"height\":80,\"content\":\"**Replace password and username for Supabase**\"},\"typeVersion\":1},{\"id\":\"2cea21fc-f3fe-47b7-a7b6-12acb0bc03ac\",\"name\":\"Sticky Note5\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-160,320],\"parameters\":{\"color\":7,\"width\":280.2462120317618,\"height\":545.9087885077763,\"content\":\"### Set up steps\\n\\n#### Preparation\\n1. **Create Accounts**:\\n - [N8N](https://n8n.partnerlinks.io/2hr10zpkki6a): For workflow automation.\\n - [Supabase](https://supabase.com/): For database hosting and management.\\n - [OpenAI](https://openai.com/): For building the conversational AI agent.\\n2. **Configure Database Connection**:\\n - Set up a PostgreSQL database in Supabase.\\n - Use appropriate credentials (`username`, `password`, `host`, and `database` name) in your workflow.\\n\\n#### N8N Workflow\\n\\nAI agent with tools:\\n\\n1. **Code Tool**:\\n - Execute SQL queries based on user input.\\n2. **Database Schema Tool**:\\n - Retrieve a list of all tables in the database.\\n - Use a predefined SQL query to fetch table definitions, including column names, types, and references.\\n3. **Table Definition**:\\n - Retrieve a list of columns with types for one table.\"},\"typeVersion\":1},{\"id\":\"eacc0c8c-11d5-44fb-8ff1-10533a233693\",\"name\":\"Sticky Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-160,-200],\"parameters\":{\"color\":7,\"width\":"}
{"prompt": "Create an n8n workflow for: Chat with local LLMs using n8n and Ollama Chat with local LLMs using n8n and Ollama This n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n. Use cases Private AI Interactions Ideal for scenarios where data privacy and confidentiality are important. Cost-Effective LLM Usage Avoid ongoing cloud API costs by running models on your own hardware. Experimentation & Learning A great way to explore and experiment with different LLMs in a local, controlled environment. Prototyping & Development Build and test AI-powered applications without relying on external services. How it works When chat message received: Captures the user's input from the chat interface. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response. Delivers the LLM's response back to the chat interface. Set up steps Make sure Ollama is installed and running on your machine before executing this workflow. Edit the Ollama address if different from the default. Mihai Farcas | 6 months ago | Free", "completion": "{\"id\":\"af8RV5b2TWB2LclA\",\"meta\":{\"instanceId\":\"95f2ab28b3dabb8da5d47aa5145b95fe3845f47b20d6343dd5256b6a28ba8fab\",\"templateCredsSetupCompleted\":true},\"name\":\"Chat with local LLMs using n8n and Ollama\",\"tags\":[],\"nodes\":[{\"id\":\"475385fa-28f3-45c4-bd1a-10dde79f74f2\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[700,460],\"webhookId\":\"ebdeba3f-6b4f-49f3-ba0a-8253dd226161\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"61133dc6-dcd9-44ff-85f2-5d8cc2ce813e\",\"name\":\"Ollama Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOllama\",\"position\":[900,680],\"parameters\":{\"options\":{}},\"credentials\":{\"ollamaApi\":{\"id\":\"MyYvr1tcNQ4e7M6l\",\"name\":\"Local Ollama\"}},\"typeVersion\":1},{\"id\":\"3e89571f-7c87-44c6-8cfd-4903d5e1cdc5\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[160,80],\"parameters\":{\"width\":485,\"height\":473,\"content\":\"## Chat with local LLMs using n8n and Ollama\\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\\n\\n### How it works\\n1. When chat message received: Captures the user's input from the chat interface.\\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated"}
{"prompt": "Create an n8n workflow for: AI-Generated Summary Block for WordPress Posts What is this workflow? This n8n template automates the process of adding an AI-generated summary at the top of your WordPress posts. It retrieves, processes, and updates your posts dynamically, ensuring efficiency and flexibility without relying on a heavy WordPress plugin. Example of AI Summary Section How It Works Triggers \u2192 Runs on a scheduled interval or via a webhook when a new post is published. Retrieves posts \u2192 Fetches content from WordPress and converts HTML to Markdown for AI processing. AI Summary Generation \u2192 Uses OpenAI to create a concise summary. Post Update \u2192 Inserts the summary at the top of the post while keeping the original excerpt intact. Data Logging & Notifications \u2192 Saves processed posts to Google Sheets and notifies a Slack channel. Why use this workflow? \u2705 No need for a WordPress plugin \u2192 Keeps your site lightweight. \u2705 Highly flexible \u2192 Easily connect with Google Sheets, Slack, or other services. \u2705 Customizable \u2192 Adapt AI prompts, formatting, and integrations to your needs. \u2705 Smart filtering \u2192 Ensures posts are not reprocessed unnecessarily. \ud83d\udca1 Check the detailed sticky notes for setup instructions and customization options! Dataki | 1 month ago | Free +2", "completion": "{\"id\":\"AhP1Fgv0eCrh9Jxs\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\",\"templateCredsSetupCompleted\":true},\"name\":\"AI-Generated Summary Block for WordPress Posts - with OpenAI, WordPress, Google Sheets & Slack\",\"tags\":[],\"nodes\":[{\"id\":\"0733b902-6707-4548-9498-44993ed6a16c\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[500,-780],\"parameters\":{},\"typeVersion\":1},{\"id\":\"fa1fea27-c44d-4c8b-89ab-e7f84e91048f\",\"name\":\"Text Classifier\",\"type\":\"@n8n/n8n-nodes-langchain.textClassifier\",\"position\":[5520,-800],\"parameters\":{\"options\":{\"systemPromptTemplate\":\"Analyze the provided text and classify it into one of the following categories: {categories}. \\n- If the text contains an 'AI Summary', classify it as \\\"summarized\\\".\\n- If the text does not contain an 'AI Summary', classify it as \\\"not_summarized\\\".\\n\\nFollow these instructions strictly:\\n- Provide the result in JSON format.\\n- Do not include any explanations, comments, or additional text.\\n\"},\"inputText\":\"={{ $json.data }}\",\"categories\":{\"categories\":[{\"category\":\"not_summarized\",\"description\":\"Content that does not contain an 'AI Summary'.\"},{\"category\":\"=summarized\",\"description\":\"Content that contains an 'AI Summary'.\"}]}},\"typeVersion\":1},{\"id\":\"258d93f8-50db-4c95-8315-b7284100a426\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[5540,-600],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"\",\"name\":\"OpenAi Connection\"}},\"typeVersion\":1.1},{\"id\":\"7634c"}
{"prompt": "Create an n8n workflow for: Analyse papers from Hugging Face with AI and store them in Notion This workflow automates the process of retrieving Hugging Face paper summaries, analyzing them with OpenAI, and storing the results in Notion. Here\u2019s a breakdown of how it works: \u23f0 Scheduled Trigger: The flow is set to run automatically at 8 AM on weekdays. \ud83d\udcc4 Fetching Paper Data: It fetches Hugging Face paper summaries using their API. \ud83d\udd0d Data Check: Before processing, the workflow checks if the paper already exists in Notion to avoid duplicates. \ud83e\udd16 Content Analysis with OpenAI: If the paper is new, it extracts the summary and uses OpenAI to analyze the content. \ud83d\udce5 Store Results in Notion: After analysis, the summarized data is saved in Notion for easy reference. \u2699\ufe0f Set Up Steps for Automation Follow these steps to set up this automated workflow with Hugging Face, OpenAI, and Notion integration: \ud83d\udd11 Obtain API Tokens: You\u2019ll need the Notion and OpenAI API tokens to authenticate and connect these services with n8n. \ud83d\udd17 Integration in n8n: Link Hugging Face, OpenAI, and Notion by configuring the appropriate nodes in n8n. \ud83d\udd27 Configure Workflow Logic: Set up a cron trigger for automatic execution at 8 AM on weekdays. Use an HTTP request node to fetch Hugging Face paper data. Add logic to check if the data already exists in Notion. Set up the OpenAI integration to analyze the paper\u2019s summary. Store the results in Notion for easy access and reference. Result: AI Native | 1 month ago | Free", "completion": "{\"id\":\"FU3MrLkaTHmfdG4n\",\"meta\":{\"instanceId\":\"3294023dd650d95df294922b9d55d174ef26f4a2e6cce97c8a4ab5f98f5b8c7b\",\"templateCredsSetupCompleted\":true},\"name\":\"Hugging Face to Notion\",\"tags\":[],\"nodes\":[{\"id\":\"32d5bfee-97f1-4e92-b62e-d09bdd9c3821\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[-2640,-300],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\",\"triggerAtDay\":[1,2,3,4,5],\"triggerAtHour\":8}]}},\"typeVersion\":1.2},{\"id\":\"b1f4078e-ac77-47ec-995c-f52fd98fafef\",\"name\":\"If\",\"type\":\"n8n-nodes-base.if\",\"position\":[-1360,-280],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"7094d6db-1fa7-4b59-91cf-6bbd5b5f067e\",\"operator\":{\"type\":\"object\",\"operation\":\"empty\",\"singleValue\":true},\"leftValue\":\"={{ $json }}\",\"rightValue\":\"\"}]}},\"typeVersion\":2.2},{\"id\":\"afac08e1-b629-4467-86ef-907e4a5e8841\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[-1760,-300],\"parameters\":{\"options\":{\"reset\":false}},\"typeVersion\":3},{\"id\":\"807ba450-9c89-4f88-aa84-91f43e3adfc6\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[-1960,-300],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"url, url\"},\"typeVersion\":1},{\"id\":\"08dd3f15-2030-48f2-ab0f-f"}
{"prompt": "Create an n8n workflow for: Notion to Pinecone Vector Store Integration This n8n automation is designed to extract, process, and store content from Notion pages into a Pinecone vector store. Here's a breakdown of the workflow: Notion - Page Added Trigger: The automation starts by monitoring for newly added pages in a specific Notion database. It triggers whenever a new page is created, capturing the page's metadata. Notion - Retrieve Page Content: Once triggered, the automation fetches the full content of the newly added Notion page, including blocks like text, images, and videos. Filter Non-Text Content: The next step filters out non-text content (such as images and videos), ensuring only textual content is processed. Summarize - Concatenate Notion's blocks content: The remaining text content is concatenated into a single block of text for easier processing. Token Splitter: The concatenated text is then split into manageable tokens, which are chunks of text that can be used for embedding. Create metadata and load content: Metadata such as the page ID, creation time, and title are added to the content, making it easy to reference and track. Embeddings Google Gemini: The processed text is passed through a Google Gemini model to generate embeddings, which are numerical representations of the text that capture its semantic meaning. Pinecone Vector Store: Finally, the embeddings, along with the content and metadata, are stored in a Pinecone vector store, making it searchable and ready for use in applications like document retrieval or natural language processing tasks. This workflow ensures that every new page added to the Notion database is processed into a format that can be easily searched and used in machine learning applications. The automation runs every minute to capture new data in real-time, providing an up-to-date and searchable vector database of Notion content. Use Case: This automation converts Notion pages into vector embeddings and stores them in Pinecone for enhanced search and AI-driven insights. It\u2019s ideal for teams using Notion for knowledge management, enabling semantic search and context-based content retrieval. For example, employees can easily find relevant information across documents, and data scientists can use AI models to analyze and summarize the content stored in Notion. Udit Rawat | 1 month ago | Free", "completion": "{\"id\":\"vOSQYz747gtzj1zF\",\"meta\":{\"instanceId\":\"d16fb7d4b3eb9b9d4ad2ee6a7fbae593d73e9715e51f583c2a0e9acd1781c08e\",\"templateId\":\"2290\"},\"name\":\"Prod: Notion to Vector Store - Dimension 768\",\"tags\":[{\"id\":\"Vs70y1mj5s2XzUap\",\"name\":\"Production\",\"createdAt\":\"2024-12-24T14:42:00.549Z\",\"updatedAt\":\"2024-12-24T14:42:00.549Z\"}],\"nodes\":[{\"id\":\"6d2579b8-376f-44c3-82e8-9dc608efd98b\",\"name\":\"Token Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter\",\"position\":[2200,800],\"parameters\":{\"chunkSize\":256,\"chunkOverlap\":30},\"typeVersion\":1},{\"id\":\"79b3c147-08ca-4db4-9116-958a868cbfd9\",\"name\":\"Notion - Page Added Trigger\",\"type\":\"n8n-nodes-base.notionTrigger\",\"position\":[1080,360],\"parameters\":{\"simple\":false,\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]},\"databaseId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"17b11930-c10f-8000-a545-ece7cade03f9\",\"cachedResultUrl\":\"https://www.notion.so/17b11930c10f8000a545ece7cade03f9\",\"cachedResultName\":\"Embeddings\"}},\"credentials\":{\"notionApi\":{\"id\":\"oktwaKqpFztx5hYX\",\"name\":\"Auto: Notion\"}},\"typeVersion\":1},{\"id\":\"e4a6f524-e3f5-4d02-949a-8523f2d21965\",\"name\":\"Notion - Retrieve Page Content\",\"type\":\"n8n-nodes-base.notion\",\"position\":[1300,360],\"parameters\":{\"blockId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"={{ $json.url }}"}
{"prompt": "Create an n8n workflow for: Spot Workplace Discrimination Patterns with AI How It Works: \u2022 Scrapes company review data from Glassdoor using ScrapingBee. \u2022 Extracts demographic-based ratings using AI-powered text analysis. \u2022 Calculates workplace disparities with statistical measures like z-scores, effect sizes, and p-values. \u2022 Generates visualizations (scatter plots, bar charts) to highlight patterns of discrimination or bias. Example Visualizations: Set Up Steps: Estimated time: ~20 minutes. \u2022 Replace ScrapingBee and OpenAI credentials with your own. \u2022 Input the company name you want to analyze (best results with large U.S.-based organizations). \u2022 Run the workflow and review the AI-generated insights and visual reports. This workflow empowers users to identify potential workplace discrimination trends, helping advocate for greater equity and accountability. Additional Credit: Wes Medford For algorithms and inspiration Ludwig | 1 month ago | Free +2", "completion": "{\"id\":\"vzU9QRZsHcyRsord\",\"meta\":{\"instanceId\":\"a9f3b18652ddc96459b459de4fa8fa33252fb820a9e5a1593074f3580352864a\",\"templateCredsSetupCompleted\":true},\"name\":\"Spot Workplace Discrimination Patterns with AI\",\"tags\":[{\"id\":\"76EYz9X3GU4PtgSS\",\"name\":\"human_resources\",\"createdAt\":\"2025-01-30T18:52:17.614Z\",\"updatedAt\":\"2025-01-30T18:52:17.614Z\"},{\"id\":\"ey2Mx4vNaV8cKvao\",\"name\":\"openai\",\"createdAt\":\"2024-12-23T07:10:13.400Z\",\"updatedAt\":\"2024-12-23T07:10:13.400Z\"}],\"nodes\":[{\"id\":\"b508ab50-158a-4cbf-a52e-f53e1804e770\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[280,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"11a1a2d5-a274-44f7-97ca-5666a59fcb31\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2220,800],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"XXXXXX\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"395f7b67-c914-4aae-8727-0573fdbfc6ad\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2220,380],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"XXXXXX\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"6ab194a9-b869-4296-aea9-19afcbffc0d7\",\"name\":\"Merge\",\"type\":\"n8n-n"}
{"prompt": "Create an n8n workflow for: Hacker News Throwback Machine - See What Was Hot on This Day, Every Year! This is a simple workflow that grabs HackerNews front-page headlines from today's date across every year since 2007 and uses a little AI magic (Google Gemini) to sort 'em into themes, sends a neat Markdown summary on Telegram. How it works Runs daily, grabs Hacker News front page for this day across every year since 2007. Pulls headlines & dates. Uses Google Gemini to sort headlines into topics & spot trends. Sends a Markdown summary to Telegram. Set up steps Clone the workflow. Add your Google Gemini API key. Add your Telegram bot token and chat ID. **Built on Day-01 as part of the #100DaysOfAgenticAi Fork it, tweak it, have fun!** ibrhdotme | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"6ea4e702-1af8-407b-b653-964a519db1c2\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1560,-360],\"parameters\":{\"text\":\"=You are a highly skilled news categorizer, specializing in indentifying interesting stuff from Hacker News front-page headlines.\\n\\nYou are provided with JSON data containing a list of dates and their corresponding top headlines from the Hacker News front page. Each headline will also include a URL linking to the original article or discussion. Importantly, the dates provided will be the SAME DAY across MULTIPLE YEARS (e.g., January 1st, 2023, January 1st, 2022, January 1st, 2021, etc.). You need to indentify key headlines and also analyze how the tech landscape has evolved over the years, as reflected in the headlines for this specific day.\\n\\nYour task is to indentify top 10-15 headlines from across the years from the given json data and return in Markdown formatted bullet points categorizing into themes and adding markdown hyperlinks to the source URL with Prefixing Year before the headline. Follow the Output Foramt Mentioned.\\n\\n**Input Format:**\\n\\n```json\\n[\\n {\\n \\\"headlines\\\": [\\n \\\"Headline 1 Title [URL1]\\\",\\n \\\"Headline 2 Title [URL2]\\\",\\n \\\"Headline 3 Title [URL3]\\\",\\n ...\\n ]\\n \\\"date\\\": \\\"YYYY-MM-DD\\\",\\n },\\n {\\n \\\"headlines\\\": [\\n \\\"Headline 1 Title [URL1]\\\",\\n \\\"Headline 2 Title [URL2]\\\",\\n ...\\n ]\\n \\\"date\\\": \\\"YYYY-MM-DD\\\",\\n },\\n ...\\n]\\n```\\n\\n**Output Format In Markdown**\\n\\n```\\n# HN Lookback <FullMonthName-DD> | <start YYYY> to <end YYYY> \\n\\n## [Theme 1]\\n- YYYY [Headline 1](URL1)\\n- YYYY [Headline 2](URL2)\\n..."}
{"prompt": "Create an n8n workflow for: \ud83d\udcda Auto-generate documentation for n8n workflows with GPT and Docsify This workflow creates a documentation system for n8n instances using Docsify.js. It serves a dynamic documentation website that allows users to: View an overview of all workflows in a tabular format Filter workflows by tags Access automatically generated documentation for each workflow Edit documentation with a live Markdown preview Visualize workflow structures using Mermaid.js diagrams &gt; \ud83d\udcfa Check out the short 2-min demonstration on LinkedIn. Don't forget to connect! \ud83d\udd27 Key Components Main Documentation Portal Serves a Docsify-powered website Provides a navigation sidebar with workflow tags Displays workflow status, creation date, and documentation links Documentation Generator Uses GPT model to auto-generate workflow descriptions Creates Mermaid.js diagrams of workflow structures Maintains consistent documentation format Live Editor Split-screen Markdown editor with preview Real-time Mermaid diagram rendering Save/Cancel functionality \u2699\ufe0f Technical Details Environment Setup Requires write access to the specified project directory Uses environment variables for n8n instance URL configuration Implements webhook endpoints for serving documentation \u26a0\ufe0f Security Considerations &gt; Note: The current implementation doesn't include authentication for editing. Consider adding authentication for production use. Dependencies Docsify.js for documentation rendering Mermaid.js for workflow visualization OpenAI GPT for documentation generation \ud83d\udd0d Part of the n8n Observability Series This workflow is part of a broader series focused on n8n instance observability. Check out these related workflows: Workflow Dashboard - Get comprehensive analytics of your n8n instance Visualize Your n8n Workflows with Mermaid.js - Create beautiful workflow visualizations Each workflow in this series helps you better understand and manage your n8n automation ecosystem! Eduard | 2 months ago | Free", "completion": "{\"id\":\"VY4TXYGmqth57Een\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"Docsify example\",\"tags\":[],\"nodes\":[{\"id\":\"f41906c3-ee4c-4333-bfd5-426f82ba4bd9\",\"name\":\"CONFIG\",\"type\":\"n8n-nodes-base.set\",\"position\":[660,60],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"b48986ec-f58d-4a7f-afba-677edcb28d31\",\"name\":\"project_path\",\"type\":\"string\",\"value\":\"./.n8n/test_docs\"},{\"id\":\"cf632419-f839-4045-922c-03784bb3ae07\",\"name\":\"instance_url\",\"type\":\"string\",\"value\":\"={{$env[\\\"N8N_PROTOCOL\\\"]}}://{{$env[\\\"N8N_HOST\\\"]}}\"},{\"id\":\"7a7c70a6-1853-4ca7-b5b1-e36bb0e190d0\",\"name\":\"HTML_headers\",\"type\":\"string\",\"value\":\"= <meta http-equiv=\\\"X-UA-Compatible\\\" content=\\\"IE=edge,chrome=1\\\" />\\n <meta name=\\\"viewport\\\" content=\\\"width=device-width,initial-scale=1\\\" />\\n <meta charset=\\\"UTF-8\\\" />\\n <link rel=\\\"stylesheet\\\" href=\\\"//cdn.jsdelivr.net/npm/docsify@4/themes/vue.css\\\" />\\n <script src=\\\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\\\"></script>\"},{\"id\":\"1e785afe-f05f-4e51-a164-f341da81ccac\",\"name\":\"HTML_styles_editor\",\"type\":\"string\",\"value\":\"= <style>\\n body {\\n margin: 0;\\n padding: 0;\\n overflow: hidden"}
{"prompt": "Create an n8n workflow for: AI Agent for realtime insights on meetings Video Guide I prepared a detailed guide explaining how to build an AI-powered meeting assistant that provides real-time transcription and insights during virtual meetings. Youtube Link Who is this for? This workflow is ideal for business professionals, project managers, and team leaders who require effective transcription of meetings for improved documentation and note-taking. It's particularly beneficial for those who conduct frequent virtual meetings across various platforms like Zoom and Google Meet. What problem does this workflow solve? Transcribing meetings manually can be tedious and prone to error. This workflow automates the transcription process in real-time, ensuring that key discussions and decisions are accurately captured and easily accessible for later review, thus enhancing productivity and clarity in communications. What this workflow does The workflow employs an AI-powered assistant to join virtual meetings and capture discussions through real-time transcription. Key functionalities include: Automatic joining of meetings on platforms like Zoom, Google Meet, and others with the ability to provide real-time transcription. Integration with transcription APIs (e.g., AssemblyAI) to deliver seamless and accurate capture of dialogue. Structuring and storing transcriptions efficiently in a database for easy retrieval and analysis. Real-Time Transcription: The assistant captures audio during meetings and transcribes it in real-time, allowing participants to focus on discussions. Keyword Recognition: Key phrases can trigger specific actions, such as noting important points or making prompts to the assistant. Structured Data Management: The assistant maintains a database of transcriptions linked to meeting details for organized storage and quick access later. Setup Preparation Create Recall.ai API key Setup Supabase account and table create table public.data ( id uuid not null default gen_random_uuid (), date_created timestamp with time zone not null default (now() at time zone 'utc'::text), input jsonb null, output jsonb null, constraint data_pkey primary key (id), ) tablespace pg_default; Create OpenAI API key Development Bot Creation: Use a node to create the bot that will join meetings. Provide the meeting URL and set transcription options within the API request. Authentication: Configure authentication settings via a Bearer token for interacting with your transcription service. Webhook Setup: Create a webhook to receive real-time transcription updates, ensuring timely data capture during meetings. Join Meeting: Set the bot to join the specified meeting and actively listen to capture conversations. Transcription Handling: Combine transcription fragments into cohesive sentences and manage dialog arrays for coherence. Trigger Actions on Keywords: Set up keyword recognition that can initiate requests to the OpenAI API for additional interactions based on captured dialogue. Output and Summary Generation: Produce insights and summary notes from the transcriptions that can be stored back into the database for future reference. Mark Shcherbakov | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"d44489b8-8cb7-4776-8c16-a8bb01e52171\",\"name\":\"OpenAI1\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[300,-300],\"parameters\":{\"text\":\"={{ \\n JSON.parse($('Insert Transcription Part').item.json.dialog)\\n .filter(item => item.date_updated && new Date(item.date_updated) >= new Date($('Insert Transcription Part').item.json.date_updated))\\n .sort((a, b) => a.order - b.order)\\n .map(item => `${item.words}\\\\n${item.speaker}`)\\n .join('\\\\n\\\\n')\\n}}\",\"memory\":\"threadId\",\"prompt\":\"define\",\"options\":{},\"resource\":\"assistant\",\"threadId\":\"={{ $json.thread_id }}\",\"assistantId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"asst_D5t6bNnNpenmfC7PmvywMqyR\",\"cachedResultName\":\"5minAI - Realtime Agent\"}},\"credentials\":{\"openAiApi\":{\"id\":\"SphXAX7rlwRLkiox\",\"name\":\"Test club key\"}},\"typeVersion\":1.6},{\"id\":\"3425f1c1-ad68-495e-bb9a-95ea92e7cf23\",\"name\":\"Insert Transcription Part\",\"type\":\"n8n-nodes-base.postgres\",\"position\":[-120,-300],\"parameters\":{\"query\":\"UPDATE public.data\\nSET output = jsonb_set(\\n output,\\n '{dialog}', \\n (\\n COALESCE(\\n (output->'dialog')::jsonb, \\n '[]'::jsonb -- Initialize as empty array if dialog does not exist\\n ) || jsonb_build_object(\\n 'order', (COALESCE(jsonb_array_length(output->'dialog'), 0) + 1), -- Calculate the next order\\n 'words', '{{ $('Webhook2').item.json.body.data.transcript.words.map(word => word.text.replace"}
{"prompt": "Create an n8n workflow for: Ultimate Scraper Workflow for n8n What this template does The Ultimate Scraper for n8n uses Selenium and AI to retrieve any information displayed on a webpage. You can also use session cookies to log in to the targeted webpage for more advanced scraping needs. \u26a0\ufe0f Important: This project requires specific setup instructions. Please follow the guidelines provided in the GitHub repository: n8n Ultimate Scraper Setup : https://github.com/Touxan/n8n-ultimate-scraper/tree/main. The workflow version on n8n and the GitHub project may differ; however, the most up-to-date version will always be the one available on the GitHub repository : https://github.com/Touxan/n8n-ultimate-scraper/tree/main. How to use Deploy the project with all the requirements and request your webhook. Example of request: curl -X POST http://localhost:5678/webhook-test/yourwebhookid \\ -H \"Content-Type: application/json\" \\ -d '{ \"subject\": \"Hugging Face\", \"Url\": \"github.com\", \"Target data\": [ { \"DataName\": \"Followers\", \"description\": \"The number of followers of the GitHub page\" }, { \"DataName\": \"Total Stars\", \"description\": \"The total numbers of stars on the different repos\" } ], \"cookie\": [] }' Or to just scrap a url : curl -X POST http://localhost:5678/webhook-test/67d77918-2d5b-48c1-ae73-2004b32125f0 \\ -H \"Content-Type: application/json\" \\ -d '{ \"Target Url\": \"https://github.com\", \"Target data\": [ { \"DataName\": \"Followers\", \"description\": \"The number of followers of the GitHub page\" }, { \"DataName\": \"Total Stars\", \"description\": \"The total numbers of stars on the different repo\" } ], \"cookies\": [] }' ` Pablo | 5 months ago | Free", "completion": "{\"id\":\"kZ3aL4r7xc96Q7lp\",\"meta\":{\"instanceId\":\"b8b2c0d20b02864cf66adc9cbefc86e9e56de0252b653d37ba6613341b5e0bef\",\"templateCredsSetupCompleted\":true},\"name\":\"Selenium Ultimate Scraper Workflow\",\"tags\":[],\"nodes\":[{\"id\":\"20d35d68-db49-4183-a913-85ad06c13912\",\"name\":\"Extract First Url Match\",\"type\":\"n8n-nodes-base.html\",\"position\":[1820,540],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"Url Find \",\"attribute\":\"href\",\"cssSelector\":\"=a[href*=\\\"https://\\\"][href*=\\\"{{ $('Edit Fields (For testing prupose )').item.json['Website Domaine'] }}\\\"]\\n\",\"returnArray\":true,\"returnValue\":\"attribute\"}]}},\"typeVersion\":1.2},{\"id\":\"9167ea20-fc9c-4d75-bf4d-bb2016079dd0\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2060,700],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"FmszNHDDVS32ud21\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"42a8646d-1b0b-4309-a87d-9c8aeb355a28\",\"name\":\"Clean Webdriver \",\"type\":\"n8n-nodes-base.httpRequest\",\"notes\":\"Script to delete traces of selenium in the browser \",\"position\":[3120,560],\"parameters\":{\"url\":\"=http://selenium_chrome:4444/wd/hub/session/{{ $('Create Selenium Session').item.json.value.sessionId }}/execute/sync\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"{\\n \\\"script\\\": \\\"Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); window.navigator.chrome ="}
{"prompt": "Create an n8n workflow for: Chat Assistant (OpenAI assistant) with Postgres Memory And API Calling Capabalities Workflow Description Your workflow is an intelligent chatbot, using ++OpenAI assistant++, integrated with a backend that supports WhatsApp Business, designed to handle various use cases such as sales and customer support. Below is a breakdown of its functionality and key components: Workflow Structure and Functionality Chat Input (Chat Trigger) The flow starts by receiving messages from customers via WhatsApp Business. Collects basic information, such as session_id, to organize interactions. Condition Check (If Node) Checks if additional customer data (e.g., name, age, dependents) is sent along with the message. If additional data is present, a customized prompt is generated, which includes this information. The prompt specifies that this data is for the assistant's awareness and doesn\u2019t require a response. Data Preparation (Edit Fields Nodes) Formats customer data and the interaction details to be processed by the AI assistant. Compiles the customer data and their query into a single text block. AI Responses (OpenAI Nodes) The assistant\u2019s prompt is carefully designed to guide the AI in providing accurate and relevant responses based on the customer\u2019s query and data provided. Prompts describe the available functionalities, including which APIs to call and their specific purposes, helping to prevent \u201challucinated\u201d or irrelevant responses. Memory and Context (Postgres Chat Memory) Stores context and messages in continuous sessions using a database, ensuring the chatbot maintains conversation history. API Calls The workflow allows the use of APIs with any endpoints you choose, depending on your specific use case. This flexibility enables integration with various services tailored to your needs. The OpenAI assistant understands JSON structures, and you can define in the prompt how the responses should be formatted. This allows you to structure responses neatly for the client, ensuring clarity and professionalism. Make sure to describe the purpose of each endpoint in the assistant\u2019s prompt to help guide the AI and prevent misinterpretation. Customer Response Delivery After processing and querying APIs, the generated response is sent to the backend and ultimately delivered to the customer through WhatsApp Business. Best Practices Implemented Preventing Hallucinations** Every API has a clear description in its prompt, ensuring the AI understands its intended use case. Versatile Functionality** The chatbot is modular and flexible, capable of handling both sales and general customer inquiries. Context Persistence** By utilizing persistent memory, the flow maintains continuous interaction context, which is crucial for longer conversations or follow-up queries. Additional Recommendations Include practical examples in the assistant\u2019s prompt, such as frequently asked questions or decision-making flows based on API calls. Ensure all responses align with the customer\u2019s objectives (e.g., making a purchase or resolving technical queries). Log interactions in detail for future analysis and workflow optimization. This workflow provides a solid foundation for a robust and multifunctional virtual assistant \ud83d\ude80 Fernanda Silva | 2 months ago | Free", "completion": "{\"id\":\"BMI5WkmyU8nZqfII\",\"meta\":{\"instanceId\":\"e03b0f22ca12c92061d789d5980a9bc31d9d7e7dd7513ac93c09ac5a0d147623\",\"templateCredsSetupCompleted\":true},\"name\":\"modelo do chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"c6e454af-70a1-4c65-8450-8159f7fc738b\",\"name\":\"If\",\"type\":\"n8n-nodes-base.if\",\"position\":[160,560],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":1,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"7ea831a4-0e20-4725-a6f5-3dc2f41f1cf4\",\"operator\":{\"type\":\"object\",\"operation\":\"exists\",\"singleValue\":true},\"leftValue\":\"={{ $json.leadData }}\",\"rightValue\":\"\"},{\"id\":\"ccb46339-4e43-42e6-aa45-d5a0cbd62214\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"\",\"rightValue\":\"\"}]}},\"typeVersion\":2},{\"id\":\"2221736f-ef99-4ac8-8a81-51af6d4e7dcd\",\"name\":\"Edit Fields1\",\"type\":\"n8n-nodes-base.set\",\"position\":[440,960],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"19a16867-b574-4b99-82f1-a86752b7fe9f\",\"name\":\"chatInput\",\"type\":\"string\",\"value\":\"=\\\"Hello, just so you can get to know me, with no intention of a response, please save this information in your memory. My name is {{ $json.leadData.name }}. I am {{ $json.leadData.age }} years old and currently live in {{ $json.leadData.city }}, {{ $json.leadData.state }}. My profession is {{ $json.leadData.profession }}, and my education level is {{ $json.leadData."}
{"prompt": "Create an n8n workflow for: AI agent chat This workflow employs OpenAI's language models and SerpAPI to create a responsive, intelligent conversational agent. It comes equipped with manual chat triggers and memory buffer capabilities to ensure seamless interactions. To use this template, you need to be on n8n version 1.50.0 or later. n8n Team | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"939bb301-5e12-4d5b-9a56-61a61cca5f0d\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[640,460],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"372777e8-ce90-4dea-befc-ac1b2eb4729f\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[780,460],\"parameters\":{},\"typeVersion\":1.2},{\"id\":\"7a8f0ad1-1c00-4043-b3e5-c88521140a1a\",\"name\":\"SerpAPI\",\"type\":\"@n8n/n8n-nodes-langchain.toolSerpApi\",\"position\":[920,460],\"parameters\":{\"options\":{}},\"credentials\":{\"serpApi\":{\"id\":\"aJCKjxx6U3K7ydDe\",\"name\":\"SerpAPI account\"}},\"typeVersion\":1},{\"id\":\"a7624108-e3da-4193-a625-887314216b8b\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[360,240],\"webhookId\":\"53c136fe-3e77-4709-a143-fe82746dd8b6\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"6b8b7de8-fe3f-43b5-97ce-a52a9e44eb5e\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[680"}
{"prompt": "Create an n8n workflow for: Automated End-to-End Fine-Tuning of OpenAI Models with Google Drive Integration 1. How it Works This n8n workflow automates fine-tuning OpenAI models through these key steps: Manual Trigger**: Starts with the \"When clicking \u2018Test workflow\u2019\" event to initiate the process. Downloads a .jsonl file from Google Drive Upload to OpenAI**: Uploads the .jsonl file to OpenAI via the \"Upload File\" node (with purpose \"fine-tune\"). Create Fine-tuning Job**: Sends a POST request to the endpoint https://api.openai.com/v1/fine_tuning/jobs with: { \"training_file\": \"{{ $json.id }}\", \"model\": \"gpt-4o-mini-2024-07-18\" } OpenAI automatically starts training the model based on the provided file. Interaction with the Trained Model**: An \"AI Agent\" uses the custom model (e.g., ft:gpt-4o-mini-2024-07-18:n3w-italia::XXXX7B) to respond to chat messages. 2. Set up Steps To configure the workflow: Prepare the Training File: Create a .jsonl file following the specified syntax (e.g., travel assistant Q/A examples). Upload it to Google Drive and update the ID in the \"Google Drive\" node. Configure Credentials: Google Drive: Connect an account via OAuth2 (googleDriveOAuth2Api). OpenAI: Add your API key in the \"OpenAI Chat Model\" and \"Upload File\" nodes. Customize the Model: In the \"OpenAI Chat Model\" node, specify the name of your fine-tuned model (e.g., ft:gpt-4o-mini-...). Update the HTTP request body (Create Fine-tuning Job) if needed (e.g., a different base model). Start the Workflow: Use the manual trigger (\"Test workflow\") to begin the upload and training process. Test the model via the \"Chat Trigger\" (chat messages). Integrated Documentation: Follow the instructions in the Sticky Notes to: Properly format the .jsonl (Step 1). Monitor progress on OpenAI (Step 2, link: https://platform.openai.com/finetune/). Note: Ensure the .jsonl file adheres to OpenAI\u2019s required structure and that credentials are valid. n3w Italia | 1 month ago | Free", "completion": "{\"id\":\"gAzsjTGbfWuvAObi\",\"meta\":{\"instanceId\":\"a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462\",\"templateCredsSetupCompleted\":true},\"name\":\"Fine-tuning with OpenAI models\",\"tags\":[{\"id\":\"2VG6RbmUdJ2VZbrj\",\"name\":\"Google Drive\",\"createdAt\":\"2024-12-04T16:50:56.177Z\",\"updatedAt\":\"2024-12-04T16:50:56.177Z\"},{\"id\":\"paTcf5QZDJsC2vKY\",\"name\":\"OpenAI\",\"createdAt\":\"2024-12-04T16:52:10.768Z\",\"updatedAt\":\"2024-12-04T16:52:10.768Z\"}],\"nodes\":[{\"id\":\"ff65c2db-6a94-4e56-a10c-2538c9617df6\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[220,320],\"parameters\":{},\"typeVersion\":1},{\"id\":\"208fc618-0543-4552-bd65-9c808c879d88\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[440,320],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"1wvlEcbxFIENvqL-bACzlLEfy5gA6uF9J\",\"cachedResultUrl\":\"https://drive.google.com/file/d/1wvlEcbxFIENvqL-bACzlLEfy5gA6uF9J/view?usp=drivesdk\",\"cachedResultName\":\"test_fine_tuning.jsonl\"},\"options\":{\"binaryPropertyName\":\"data.jsonl\",\"googleFileConversion\":{\"conversion\":{\"docsToFormat\":\"application/pdf\"}}},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"HEy5EuZkgPZVEa9w\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"3580d925-c8c9-446"}
{"prompt": "Create an n8n workflow for: AI-Powered Candidate Shortlisting Automation for ERPNext Template Guide for Employee Shortlisting AI Agent Automation Overview This template automates the process of shortlisting job applicants using ERPNext, n8n, and AI-powered decision-making tools like Google Gemini and OpenAI. It reduces manual effort, ensures fast evaluations, and provides justifiable decisions about applicants. This is ideal for businesses aiming to streamline their recruitment process while maintaining accuracy and professionalism. YouTube Tutorial:** For a full walkthrough of this template, visit: Integrate AI in ERPNext: Automate Recruitment Job Applicant Shortlisting in Seconds! What Does This Template Do? Webhook Integration with ERPNext: Automatically triggers the workflow when a job application is created in ERPNext. Resume Validation: Ensures resumes are attached and correctly processes various file formats like PDF and DOC. AI-Powered Evaluation: Uses AI to compare resumes against job descriptions and provides a: Fit Level (Strong, Moderate, or Weak) Score (0\u2013100) Justification for the decision. Automated Decision Making: Based on AI-generated scores: Candidates with a score of 80 or higher are Accepted. Candidates below 80 are Rejected. Applications missing required fields or attachments are put On Hold. ERPNext Integration: Updates applicant records in ERPNext, including custom fields such as justification, fit level, and scores. Notifications: Notifies candidates via email, WhatsApp, or SMS about their application status. Step-by-Step Guide Step 1: Set Up ERPNext Webhook Go to Webhooks in ERPNext. Create a webhook for the Job Applicant DocType. Set the trigger to Insert. Pin and test the webhook to ensure proper data flow. Step 2: Import the Template into n8n Open your n8n instance. Import the provided workflow template. Check all nodes for proper configuration. Step 3: Configure Credentials Add your ERPNext API credentials to the ERPNext nodes. Add credentials for AI services like OpenAI or Google Gemini. Configure additional services like WhatsApp or email if you plan to use them for notifications. Step 4: Test Resume Validation Test how the workflow handles different file types (e.g., PDF, DOC, JPG). Ensure resumes without the proper format or attachment are flagged and rejected. Step 5: AI Evaluation The AI model (Google Gemini or OpenAI) will evaluate resumes against job descriptions. Customize the AI prompt to suit your job evaluation needs. The output will include a Fit Level, Score, Rating, and Justification. Step 6: Decision Automation The workflow automatically categorizes applicants: Accepted for scores \u2265 80. Rejected for scores &lt; 80. On Hold if essential fields or attachments are missing. Step 7: Update ERPNext Records The workflow updates the Job Applicant record in ERPNext with: Status (Accepted, Rejected, On Hold) AI-generated Fit Level, Score, Rating, and Justification. Step 8: Notify Candidates Configure notification nodes (email, WhatsApp, or SMS). Inform candidates about their application status and include feedback if required. How It Works Trigger: The workflow starts when a job application is submitted in ERPNext. Validation: Checks if the resume is attached and in the correct format. AI Evaluation: Compares the resume with the job description and generates a decision. ERPNext Update: Updates the applicant's record with the decision and justification. Notification: Sends a personalized notification to the candidate. Dos and Don\u2019ts Dos: Customize Prompts:** Tailor the AI prompt to match your specific job evaluation requirements. Test the Workflow:** Run sample data to ensure the process works as intended. Secure Your Credentials:** Keep your API credentials safe and do not share them publicly. Optimize for Different Formats:** Ensure the workflow can handle all types of resumes you expect. Don\u2019ts: Avoid Manual Intervention:** Let the workflow handle most of the tasks to ensure efficiency. Do Not Skip Testing:** Always test the workflow with various scenarios to avoid errors. Do Not Overlook Notifications:** Ensure candidates are notified promptly to maintain professionalism. Customization Options Add logic for more file types (e.g., scanned images using OCR). Enhance the AI prompts to analyze more complex resume data. Integrate additional tools like Slack or Trello for recruitment tracking. Resources YouTube Tutorial:** For a full walkthrough of this template, visit: SyncBricks YouTube Channel Detailed Guides and Courses:** Learn more about ERPNext and AI-driven automation at: SyncBricks LMS Support If you encounter issues or want to explore more possibilities with AI-driven automation, feel free to reach out: Email:** amjid@amjidali.com Website:** ERPNext and Other Courses LinkedIn:** Amjid Ali Let me know if you'd like further details or modifications to the guide! Amjid Ali | 1 month ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"e4f78845dfed9ddcfba1945ae00d12e9a7d76eab052afd19299228ce02349d86\"},\"nodes\":[{\"id\":\"a8b14ffd-4a8b-4a3d-ba54-5997197e5457\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-2880,340],\"parameters\":{\"color\":6,\"width\":429.96763122593137,\"height\":322,\"content\":\"### Get Apprropraite Data\\nThese Nodes are to extract only the Required Data from the Webhook Node\"},\"typeVersion\":1},{\"id\":\"c8ae8766-3942-4b8d-8815-9e96ab9dc1de\",\"name\":\"Sticky Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-840,340],\"parameters\":{\"color\":5,\"width\":677,\"height\":660,\"content\":\"### N8N has the Node to convert PDF to Text, but PDF should be converted from Text. Scanned Image PDF will not be converted. \\n\\nYou can use ohter nodes here to convert word file to text or JPG to Text, possiblities are limitless.\"},\"typeVersion\":1},{\"id\":\"529f5756-c1a2-4c41-8245-38164543eb8e\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1334.2232779572741,340],\"parameters\":{\"color\":7,\"width\":477.2230145794152,\"height\":648.5051458745238,\"content\":\"### Download the File.\\nIf you are using S3 to Upload attachments you can use S3 node here to download the attachment\\n\"},\"typeVersion\":1},{\"id\":\"101c6544-d319-495d-a14f-e180f51be1f0\",\"name\":\"Code\",\"type\":\"n8n-nodes-base.code\",\"position\":[-2840,480],\"parameters\":{\"jsCode\":\"// Loop over input items and add a new field called 'myNewField' to the JSON of each one\\nfor (const item"}
{"prompt": "Create an n8n workflow for: AI Agent for project management and meetings with Airtable and Fireflies Video Guide I prepared a comprehensive guide detailing how to create a Smart Agent that automates meeting task management by analyzing transcripts, generating tasks in Airtable, and scheduling follow-ups when necessary. Youtube Link Who is this for? This workflow is ideal for project managers, team leaders, and business owners looking to enhance productivity during meetings. It is particularly helpful for those who need to convert discussions into actionable items swiftly and effectively. What problem does this workflow solve? Managing action items from meetings can often lead to missed tasks and poor follow-up. This automation alleviates that issue by automatically generating tasks from meeting transcripts, keeping everyone informed about their responsibilities and streamlining communication. What this workflow does The workflow leverages n8n to create a Smart Agent that listens for completed meeting transcripts, processes them using AI, and generates tasks in Airtable. Key functionalities include: Capturing completed meeting events through webhooks. Extracting relevant meeting details such as transcripts and participants using API calls. Generating structured tasks from meeting discussions and sending notifications to clients. Webhook Integration: Listens for meeting completion events to trigger subsequent actions. API Requests for Data: Pulls necessary details like transcripts and participant information from Fireflies. Task and Notification Generation: Automatically creates tasks in Airtable and notifies clients of their responsibilities. Setup N8N Workflow Configure the Webhook: Set up a webhook to capture meeting completion events and integrate it with Fireflies. Retrieve Meeting Content: Use GraphQL API requests to extract meeting details and transcripts, ensuring appropriate authentication through Bearer tokens. AI Processing Setup: Define system messages for AI tasks and configure connections to the AI chat model (e.g., OpenAI's GPT) to process transcripts. Task Creation Logic: Create structured tasks based on AI output, ensuring necessary details are captured and records are created in Airtable. Client Notifications: Use an email node to notify clients about their tasks, ensuring communications are client-specific. Scheduling Follow-Up Calls: Set up Google Calendar events if follow-up meetings are required, populating details from the original meeting context. Mark Shcherbakov | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"38972c5c-09f4-4120-a468-731e720914e1\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[900,-240],\"parameters\":{\"text\":\"=Title: {{ $json.data.transcript.title }}\\n\\nParticipants: {{ $json.data.transcript.participants }}\\n\\nTranscript: {{ JSON.stringify($json.data.transcript.sentences) }}\\n\\nBullet gist:{{ $json.data.transcript.summary.bullet_gist }}\",\"agent\":\"openAiFunctionsAgent\",\"options\":{\"systemMessage\":\"=You get my calls' transcripts from Firefiles.\\nThere can be meetings about projects. You can understand if it's about a project if meeting's title contains \\\"project\\\". If so - you need to:\\n1. Analyze transcript, use tool \\\"Create Tasks\\\" to create tasks for me in my AirTable base.\\n2. You need to use tool \\\"Notify Client About Tasks\\\" to nofity client about his tasks.\\n3. If transcript contains info there's a call needed - you'll use \\\"Create Event\\\" tool to create call on Google Meet\\nCurrent date: {{ $now }}\"},\"promptType\":\"define\"},\"typeVersion\":1.7},{\"id\":\"db5c1bfa-b979-4749-84c8-8cd7d777748c\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[880,40],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"9RivS2BmSh1DDBFm\",\"name\":\"OpenAi account 3\"}},\"typeVersion\":1},{\"id\":\"334873ba-ec5c-42b3-b8d0-def79d07c0aa\",\"name\":\"Create Tasks\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1040,40],\"parameters\":{\"name\":\"create_task\",\"schemaType\":\"manual\",\"workflow"}
{"prompt": "Create an n8n workflow for: Qualifying Appointment Requests with AI & n8n Forms This n8n template builds upon a simple appointment request form design which uses AI to qualify if the incoming enquiry is suitable and/or time-worthy of an appointment. This demonstrates a lighter approach to using AI in your templates but handles a technically difficult problem - contextual understanding! This example can be used in a variety of contexts where figuring out what is and isn't relevant can save a lot of time for your organisation. How it works We start with a form trigger which asks for the purpose of the appointment. Instantly, we can qualify this by using a text classifier node which uses AI's contextual understanding to ensure the appointment is worthwhile. If not, an alternative is suggested instead. Multi-page forms are then used to set the terms of the appointment and ask the user for a desired date and time. An acknowledgement is sent to the user while an approval by email process is triggered in the background. In a subworkflow, we use Gmail with the wait for approval operation to send an approval form to the admin user who can either confirm or decline the appointment request. When approved, a Google Calendar event is created. When declined, the user is notified via email that the appointment request was declined. How to use Modify the enquiry classifier to determine which contexts are relevant to you. Configure the wait for approval node to send to an email address which is accessible to all appropriate team members. Requirements OpenAI for LLM Gmail for Email Google Calendar for Appointments Customising this workflow Not using Google Mail or Calendar? Feel free to swap this with other services. The wait for approval step is optional. Remove if you wish to handle appointment request resolution in another way. Jimleuk | 3 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"76589d1c-45f3-4a89-906f-8ef300d34964\",\"name\":\"n8n Form Trigger\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-2520,-280],\"webhookId\":\"5e7637dd-d222-4786-8cdc-7b66cebc1481\",\"parameters\":{\"path\":\"schedule_appointment\",\"options\":{\"ignoreBots\":true,\"appendAttribution\":true,\"useWorkflowTimezone\":true},\"formTitle\":\"Schedule an Appointment\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Your Name\",\"placeholder\":\"eg. Sam Smith\",\"requiredField\":true},{\"fieldType\":\"email\",\"fieldLabel\":\"Email\",\"placeholder\":\"eg. sam@example.com\",\"requiredField\":true},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Enquiry\",\"placeholder\":\"eg. I'm looking for...\",\"requiredField\":true}]},\"formDescription\":\"Welcome to Jim's Appointment Form.\\nBefore we set a date, please tell me a little about yourself and how I can help.\"},\"typeVersion\":2.1},{\"id\":\"194b7073-fa33-4e75-85ed-c02724c8075c\",\"name\":\"Form End\",\"type\":\"n8n-nodes-base.form\",\"position\":[-420,-260],\"webhookId\":\"8fcc907b-bc2e-4fdf-a829-82c83e677724\",\"parameters\":{\"options\":{\"formTitle\":\"Appointment Request Sent!\"},\"operation\":\"completion\",\"completionTitle\":\"Appointment Request Sent!\",\"completionMessage\":\"=Thank you for submitting an appointment request. A confirmation of this request will be sent to your inbox. I'll get back to you shortly with a confirmation of the appointment.\\n\\nHere is the summary of the appointment request.\\n\\nName: {{ $('Get Form Values').item.json.name }}\\nDate & Time: {{ DateTime.fromISO($('Get Form Values').item.json.dateTime).format('EEE, dd MMM @ t"}
{"prompt": "Create an n8n workflow for: CV Screening with OpenAI Video Guide I prepared a detailed guide that showed the whole process of building a resume analyzer. Who is this for? This workflow is ideal for recruitment agencies, HR professionals, and hiring managers looking to automate the initial screening of CVs. It is especially useful for organizations handling large volumes of applications and seeking to streamline their recruitment process. What problem does this workflow solve? Manually screening resumes is time-consuming and prone to human error. This workflow automates the process, providing consistent and objective analysis of CVs against job descriptions. It helps filter out unsuitable candidates early, reducing workload and improving the overall efficiency of the recruitment process. What this workflow does This workflow automates the resume screening process using OpenAI for analysis. It provides a matching score, a summary of candidate suitability, and key insights into why the candidate fits (or doesn\u2019t fit) the job. Retrieve Resume: The workflow downloads CVs from a direct link (e.g., Supabase storage or Dropbox). Extract Data: Extracts text data from PDF or DOC files for analysis. Analyze with OpenAI: Sends the extracted data and job description to OpenAI to: Generate a matching score. Summarize candidate strengths and weaknesses. Provide actionable insights into their suitability for the job. Setup Preparation Create Accounts: N8N: For workflow automation. OpenAI: For AI-powered CV analysis. Get CV Link: Upload CV files to Supabase storage or Dropbox to generate a direct link for processing. Prepare Artifacts for OpenAI: Define Metrics: Identify the metrics you want from the analysis (e.g., matching percentage, strengths, weaknesses). Generate JSON Schema: Use OpenAI to structure responses, ensuring compatibility with your database. Write a Prompt: Provide OpenAI with a clear and detailed prompt to ensure accurate analysis. N8N Scenario Download File: Fetch the CV using its direct URL. Extract Data: Use N8N\u2019s PDF or text extraction nodes to retrieve text from the CV. Send to OpenAI: URL: POST to OpenAI\u2019s API for analysis. Parameters: Include the extracted CV data and job description. Use JSON Schema to structure the response. Summary This workflow provides a seamless, automated solution for CV screening, helping recruitment agencies and HR teams save time while maintaining consistency in candidate evaluation. It enables organizations to focus on the most suitable candidates, improving the overall hiring process. Mark Shcherbakov | 3 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6a2a7715680b8313f7cb4676321c5baa46680adfb913072f089f2766f42e43bd\"},\"nodes\":[{\"id\":\"0f3b39af-2802-462c-ac54-a7bccf5b78c5\",\"name\":\"Extract Document PDF\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[520,400],\"parameters\":{\"options\":{},\"operation\":\"pdf\"},\"typeVersion\":1,\"alwaysOutputData\":false},{\"id\":\"6f76e3a6-a3be-4f9f-a0db-3f002eafc2ad\",\"name\":\"Download File\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[340,400],\"parameters\":{\"url\":\"={{ $json.file_url }}\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"2c4e0b0f-28c7-48f5-b051-6e909ac878d2\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-20,400],\"parameters\":{},\"typeVersion\":1},{\"id\":\"a70d972b-ceb4-4f4d-8737-f0be624d6234\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[120,280],\"parameters\":{\"width\":187.37066290133808,\"height\":80,\"content\":\"**Add direct link to CV and Job description**\"},\"typeVersion\":1},{\"id\":\"9fdff1be-14cf-4167-af2d-7c5e60943831\",\"name\":\"Sticky Note5\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-800,140],\"parameters\":{\"color\":7,\"width\":280.2462120317618,\"height\":438.5821431288714,\"content\":\"### Setup\\n\\n1. **Download File**: Fetch the CV using its direct URL.\\n2. **Extract Data**: Use N8N\u2019s PDF or text extraction nodes to retrieve text"}
{"prompt": "Create an n8n workflow for: Generate SEO Seed Keywords Using AI What this workflow does: This flow uses an AI node to generate Seed Keywords to focus SEO efforts on based on your ideal customer profile. You can use these keywords to form part of your SEO strategy. Outputs: List of 20 Seed Keywords Setup Fill the Set Ideal Customer Profile (ICP) Connect with your credentials Replace the Connect to your own database with your own database Pre-requisites / Dependencies You know your ideal customer profile (ICP) An AI API account (either OpenAI or Anthropic recommended) More templates and n8n workflows &gt;&gt;&gt; @simonscrapes simonscrapes | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"257476b1ef58bf3cb6a46e65fac7ee34a53a5e1a8492d5c6e4da5f87c9b82833\",\"templateId\":\"2473\"},\"nodes\":[{\"id\":\"1205b121-8aaa-4e41-874b-4e81aad6374e\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[120,600],\"parameters\":{\"color\":4,\"width\":462.4041757955455,\"height\":315.6388466176832,\"content\":\"## Generate SEO Seed Keywords Using AI\\n\\nThis flow uses an AI node to generate Seed Keywords to focus SEO efforts on based on your ideal customer profile\\n\\n**Outputs:** \\n- List of 20 Seed Keywords\\n\\n\\n**Pre-requisites / Dependencies:**\\n- You know your ideal customer profile (ICP)\\n- An AI API account (either OpenAI or Anthropic recommended)\"},\"typeVersion\":1},{\"id\":\"d2654d75-2b64-4ec3-b583-57d2b6b7b195\",\"name\":\"Sticky Note13\",\"type\":\"n8n-nodes-base.stickyNote\",\"disabled\":true,\"position\":[640,920],\"parameters\":{\"color\":7,\"width\":287.0816455493243,\"height\":330.47923074942287,\"content\":\"**Generate draft seed KW based on ICP**\\n\\n\"},\"typeVersion\":1},{\"id\":\"d248a58e-3705-4b6f-99cb-e9187e56781c\",\"name\":\"Anthropic Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\"position\":[680,1120],\"parameters\":{\"options\":{}},\"typeVersion\":1.2},{\"id\":\"71517d83-59f5-441a-8a75-c35f4e06a8a2\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[980,980],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"output.answer\"},\""}
{"prompt": "Create an n8n workflow for: Auto Categorise Outlook Emails with AI Automate your email management with this workflow, designed for freelancers and business professionals who receive high volumes of emails. By leveraging AI-powered categorisation and dynamic email processing, this template helps you organise your inbox and streamline communication for better efficiency and productivity. Check out the YouTube video for step-by-step set up instructions! How it works: Fetch & Filter Emails: The workflow retrieves emails from your Microsoft Outlook account, filtering out flagged emails and those already categorised. Content Preparation: Each email is cleaned up and converted to a structured format using Markdown, making it easier for AI processing. AI Categorization: The content is analysed using an AI model, which categorises the emails into predefined categories (e.g., Action, Junk, Business, SaaS) based on the context and content. Email Categorization & Folder Management: The categorised emails are updated in Microsoft Outlook and moved to respective folders such as \"Junk Email\" or \"Receipts\" based on the AI's classification. Conditional Processing & Final Checks: Additional checks and conditions ensure that only unread emails are processed, and errors are gracefully managed to maintain workflow stability. Set up steps: Connect Microsoft Outlook: Link your Microsoft Outlook account using the built-in credentials node to enable email fetching, updating, and folder management. Configure AI Model (Ollama API): Set up the AI model by connecting to the Ollama API and choosing your desired language model for categorisation. Modify Email Categories (Optional): Customize the categories and subcategories within the workflow to suit your unique email management needs. Set Up Error Handling: Review the error handling node settings to ensure smooth workflow execution. This template offers a robust solution for managing and organising your inbox, helping you save time and keep your focus on important emails. Wayne Simpson | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"67d4d33d8b0ad4e5e12f051d8ad92fc35893d7f48d7f801bc6da4f39967b3592\"},\"nodes\":[{\"id\":\"30f5203b-469d-4f0c-8493-e8f08e14e4fe\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-560,440],\"parameters\":{},\"typeVersion\":1},{\"id\":\"d16f59dd-f54e-487b-9aac-67f109ba9869\",\"name\":\"Sticky Note8\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1000,-280],\"parameters\":{\"color\":7,\"width\":727.9032097745135,\"height\":110.58643966444157,\"content\":\"# Auto Categorise Outlook Emails with AI\\nBuilt by [Wayne Simpson](https://www.linkedin.com/in/simpsonwayne/) at [nocodecreative.io](https://nocodecreative.io)\"},\"typeVersion\":1},{\"id\":\"4e110412-8530-4322-bc5c-7f9df2b63bcb\",\"name\":\"Sticky Note9\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1100,-120],\"parameters\":{\"color\":7,\"width\":506.8102696237577,\"height\":337.24177957113216,\"content\":\"### Watch Set Up Video \ud83d\udc47\\n[![Auto Categorise Outlook Emails with AI](https://vdyfnvnstovfxpabhdjc.supabase.co/storage/v1/object/public/images/Thumbnails/auto-categories-emails.png?t=2024-10-11T09%3A56%3A37.961Z#full-width)](https://www.youtube.com/watch?v=EhRBkkjv_3c)\\n\\n\"},\"typeVersion\":1},{\"id\":\"9d79875f-148e-46ef-967a-95c07298456d\",\"name\":\"Ollama"}
{"prompt": "Create an n8n workflow for: Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI Create a recommendation tool without hallucinations based on RAG with the Qdrant Vector database. This example is based on movie recommendations on the IMDB-top1000 dataset. You can provide your wishes and your \"big no's\" to the chatbot, for example: \"A movie about wizards but not Harry Potter\", and get top-3 recommendations. How it works a video with the full design process Upload IMDB-1000 dataset to Qdrant Vector Store, embedding movie descriptions with OpenAI; Set up an AI agent with a chat. This agent will call a workflow tool to get movie recommendations based on a request written in the chat; Create a workflow which calls Qdrant's Recommendation API to retrieve top-3 recommendations of movies based on your positive and negative examples. Set Up Steps You'll need to create a free tier Qdrant Cluster (Qdrant can also be used locally; it's open-sourced) and set up API credentials You'll OpenAI credentials You'll need GitHub credentials & to upload the IMDB Kaggle dataset to your GitHub. Jenny | 5 months ago | Free +5", "completion": "{\"id\":\"a58HZKwcOy7lmz56\",\"meta\":{\"instanceId\":\"178ef8a5109fc76c716d40bcadb720c455319f7b7a3fd5a39e4f336a091f524a\",\"templateCredsSetupCompleted\":true},\"name\":\"Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI\",\"tags\":[],\"nodes\":[{\"id\":\"06a34e3b-519a-4b48-afd0-4f2b51d2105d\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[4980,740],\"parameters\":{},\"typeVersion\":1},{\"id\":\"9213003d-433f-41ab-838b-be93860261b2\",\"name\":\"GitHub\",\"type\":\"n8n-nodes-base.github\",\"position\":[5200,740],\"parameters\":{\"owner\":{\"__rl\":true,\"mode\":\"name\",\"value\":\"mrscoopers\"},\"filePath\":\"Top_1000_IMDB_movies.csv\",\"resource\":\"file\",\"operation\":\"get\",\"repository\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n_demo\",\"cachedResultUrl\":\"https://github.com/mrscoopers/n8n_demo\",\"cachedResultName\":\"n8n_demo\"},\"additionalParameters\":{}},\"credentials\":{\"githubApi\":{\"id\":\"VbfC0mqEq24vPIwq\",\"name\":\"GitHub n8n demo\"}},\"typeVersion\":1},{\"id\":\"9850d1a9-3a6f-44c0-9f9d-4d20fda0b602\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[5360,740],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"7704f993-b1c9-477a-8b5a-77dc2cb68161\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[5560,940],\""}
{"prompt": "Create an n8n workflow for: Auto-Categorize blog posts in wordpress using A.I. WordPress Post Auto-Categorization Workflow \ud83d\udca1 Click here to read detailed case study \ud83d\udcfa Click here to watch youtube tutorial \ud83c\udfaf Purpose Automatically categorize WordPress blog posts using AI, saving hours of manual work. This workflow analyzes your post titles and assigns them to predefined categories using artificial intelligence. \ud83d\udd04 What This Workflow Does \u2022 Connects to your WordPress site \u2022 Retrieves all uncategorized posts \u2022 Uses AI to analyze post titles \u2022 Automatically assigns appropriate category IDs \u2022 Updates posts with new categories \u2022 Processes dozens of posts in minutes \u2699\ufe0f Setup Requirements WordPress site with admin access Predefined categories in WordPress OpenAI API credentials (or your preferred AI provider) n8n with WordPress credentials \ud83d\udee0\ufe0f Configuration Steps Add your WordPress categories (manually in WordPress) Note down category IDs Update the AI prompt with your category IDs Configure WordPress credentials in n8n Set up AI API connection \ud83d\udd27 Customization Options \u2022 Modify AI prompts for different categorization criteria \u2022 Adjust for multiple category assignments \u2022 Add tag generation functionality \u2022 Customize for different content types \u2022 Add additional metadata updates \u26a0\ufe0f Important Notes \u2022 Backup your WordPress database before running \u2022 Test with a few posts first \u2022 Review AI categorization results initially \u2022 Categories must be created manually first \ud83c\udf81 Bonus Features \u2022 Can be modified for tag generation \u2022 Works with scheduled posts \u2022 Handles bulk processing \u2022 Maintains categorization consistency Perfect for content managers, bloggers, and website administrators looking to organize their WordPress content efficiently. #n8n #WordPress #ContentManagement #Automation #AI Created by rumjahn Keith Rumjahn | 1 month ago | Free", "completion": "{\"id\":\"caaf1WFANPKAikiH\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Auto categorize wordpress template\",\"tags\":[],\"nodes\":[{\"id\":\"2017403c-7496-48f8-a487-8a017c7adfe3\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[680,320],\"parameters\":{},\"typeVersion\":1},{\"id\":\"82ff288f-4234-4192-9046-33e5ffee5264\",\"name\":\"Wordpress\",\"type\":\"n8n-nodes-base.wordpress\",\"position\":[1500,320],\"parameters\":{\"postId\":\"={{ $('Get All Wordpress Posts').item.json.id }}\",\"operation\":\"update\",\"updateFields\":{\"categories\":\"={{ $json.output }}\"}},\"credentials\":{\"wordpressApi\":{\"id\":\"lGWPwxTdfPDDbFjj\",\"name\":\"Rumjahn.com wordpress\"}},\"typeVersion\":1},{\"id\":\"521deb22-62dd-4b5f-8b9a-aab9777821da\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[620,-100],\"parameters\":{\"width\":504.88636363636317,\"content\":\"## How to Auto-Categorize 82 Blog Posts in 2 Minutes using A.I. (No Coding Required)\\n\\n\ud83d\udca1 Read the [case study here](https://rumjahn.com/how-to-use-a-i-to-categorize-wordpress-posts-and-streamline-your-content-organization-process/).\\n\\n\ud83d\udcfa Watch the [youtube tutorial here](https://www.youtube.com/watch?v=IvQioioVqhw)\\n\\n\"},\"typeVersion\":1},{\"id\":\"4090d827-f8cd-47ef-ad4f-654ee58216f6\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-"}
{"prompt": "Create an n8n workflow for: Using External Workflows as Tools in n8n This guide will show you how to use a workflow as a reusable tool in n8n, such as integrating an AI Agent or other specialized processes into your workflows. By the end of this example, you'll have a simple, reusable workflow that can be easily plugged into larger projects, making your automations more efficient and scalable. With this approach, you can create reusable workflows like \"Scrape a Page,\" \"Search Brave,\" or \"Generate an Image,\" which you can then call whenever needed. While n8n makes it easy to build these workflows from scratch, setting them up as reusable components saves time as your automations grow in complexity. Setup Add the \"Execute Workflow Trigger\" node Add the node(s) to perform the desired tasks in the workflow Add a final \"Set\" or \"Edit Fields\" node at the end to ensure all external workflows return a consistent output format Details In this example, the \"Execute Workflow Trigger\" expects input in the following JSON format: [ { \"query\": { \"url\": \"https://en.wikipedia.org/wiki/some_info\" } } ] Once your external workflow is ready, you can instruct the AI Agent to use this tool by connecting it to the external workflow. Set up the schema type to \"Generate from JSON Example\" using this structure: { \"url\": \"URL_TO_GET\" } Finally, ensure your external workflow includes a \"Set\" or \"Edit Fields\" node at the end to define the response format. This helps keep the outputs of your reusable workflows consistent and predictable. Alfred Nutile | 1 month ago | Free", "completion": "{\"id\":\"7DPLpEkww5Uctcml\",\"meta\":{\"instanceId\":\"75d76ac1fb686d403c2294ca007b62282f34c3e15dc3528cc1dbe36a827c0c6e\"},\"name\":\"get_a_web_page\",\"tags\":[{\"id\":\"7v5QbLiQYkQ7zGTK\",\"name\":\"tools\",\"createdAt\":\"2025-01-08T16:33:21.887Z\",\"updatedAt\":\"2025-01-08T16:33:21.887Z\"}],\"nodes\":[{\"id\":\"290cc9b8-e4b1-4124-ab0e-afbb02a9072b\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[-460,-100],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f256ed59-ba61-4912-9a75-4e7703547de5\",\"name\":\"FireCrawl\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-220,-100],\"parameters\":{\"url\":\"https://api.firecrawl.dev/v1/scrape\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"url\\\": \\\"{{ $json.query.url }}\\\",\\n \\\"formats\\\": [\\n \\\"markdown\\\"\\n ]\\n} \",\"sendBody\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"headerParameters\":{\"parameters\":[{}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"RoJ6k6pWBzSVp9JK\",\"name\":\"Firecrawl\"}},\"typeVersion\":4.2},{\"id\":\"a28bdbe6-fa59-4bf1-b0ab-c34ebb10cf0f\",\"name\":\"Edit Fields\",\"type\":\"n8n-nodes-base.set\",\"position\":[-20,-100],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"1af62ef9-7385-411a-8aba-e4087f09c3a9\",\"name\":\"response\",\"type\":\""}
{"prompt": "Create an n8n workflow for: Generate 9:16 Images from Content and Brand Guidelines Overview This n8n workflow automates the creation of 9:16 aspect ratio images optimized for short-form video content and thumbnails. It integrates multiple tools to retrieve content, generate scripts, and create AI-generated imagery. Key Features Trigger Workflow Manually The workflow starts when triggered manually in n8n. Retrieve Brand Guidelines Fetch brand elements like style, tone, and guidelines from Airtable. SEO Keywords and Blog Post Retrieval Retrieves blog posts and associated SEO keywords from Airtable to form the basis of image content. Content Preparation Uses GPT-4 to prepare a 4-scene script and thumbnail prompts for short-form videos. AI Image Generation Uses Leonardo.ai API to generate: Thumbnail Images Scene-specific Images (9:16 Aspect Ratio) Airtable Asset Management Generated assets (images) are saved back into Airtable with metadata like URLs and file sizes. Tools and Integrations n8n**: Workflow automation platform. OpenAI**: Generates scripts and prompts (GPT-4O-MINI). Leonardo.ai**: AI tool for improving prompts and generating high-quality images. Airtable**: Used as a data source for brand guidelines, blog posts, and to store generated assets. Workflow Steps Manual Trigger Initiate the workflow. Retrieve Brand and SEO Guidelines Fetch essential brand elements like tone, style, and keywords. Filter and Fetch Blog Content Search for blog posts relevant to selected SEO keywords. Script Preparation Use GPT-4 to generate a script with image prompts for scenes and thumbnails. Image Generation Call Leonardo.ai to create: Scene Images in 9:16 Aspect Ratio. A Thumbnail Image with an improved prompt. Store Assets Save generated assets (images) to Airtable for future use. Workflow Structure Nodes Breakdown: Manual Trigger**: Start the workflow. Get Brand Guidelines**: Pull brand-related information (style, tagline, tone, etc.) from Airtable. Set Guidelines**: Prepare fetched data. Get SEO Keywords**: Retrieve keywords to filter relevant content. Keyword Filter*: Filter results for specified keywords (e.g., *\"AI Automation\"). Script Prep**: Generate 4-scene scripts and prompts with GPT-4. Leo - Improve Prompt**: Improve image prompts for clarity and detail. Leo - Generate Image**: Create AI-generated images for scenes and thumbnails. Wait Nodes**: Ensures Leonardo image generation is complete. Add Asset Info**: Store the generated images back into Airtable with metadata. API Credentials Required Ensure the following credentials are configured in n8n: OpenAI API Key Leonardo.ai API Key Airtable API Token Output Generated Images**: High-quality AI-generated images with a 9:16 aspect ratio. Saved Metadata**: Asset details (URLs, sizes, types) stored in Airtable. Usage Import this workflow into n8n. Set up your Airtable API, Leonardo.ai API, and OpenAI API credentials. Run the workflow manually. Monitor image generation and check the Airtable output for results. Tags OpenAI** RunwayML** Leonardo** Airtable** Video Automation** Author AlexK1919 AI-Native Workflow Architect More Workflow Templates YouTube Channel Connect with Alex Alex Kim | 2 months ago | Free", "completion": "{\"id\":\"VlCgU5K9SYQbdxTa\",\"meta\":{\"instanceId\":\"d868e3d040e7bda892c81b17cf446053ea25d2556fcef89cbe19dd61a3e876e9\"},\"name\":\"Content to 9:16 Aspect Image Generator v1\",\"tags\":[{\"id\":\"QsH2EXuw2e7YCv0K\",\"name\":\"OpenAI\",\"createdAt\":\"2024-11-15T04:05:20.872Z\",\"updatedAt\":\"2024-11-15T04:05:20.872Z\"},{\"id\":\"04PL2irdWYmF2Dg3\",\"name\":\"RunwayML\",\"createdAt\":\"2024-11-15T05:55:30.783Z\",\"updatedAt\":\"2024-11-15T05:55:30.783Z\"},{\"id\":\"yrY6updwSCXMsT0z\",\"name\":\"Video\",\"createdAt\":\"2024-11-15T05:55:34.333Z\",\"updatedAt\":\"2024-11-15T05:55:34.333Z\"},{\"id\":\"lvPj9rYRsKOHCi4J\",\"name\":\"Creatomate\",\"createdAt\":\"2024-11-19T15:59:16.134Z\",\"updatedAt\":\"2024-11-19T15:59:16.134Z\"},{\"id\":\"9LXACqpQLNtrM6or\",\"name\":\"Leonardo\",\"createdAt\":\"2024-11-19T15:59:21.368Z\",\"updatedAt\":\"2024-11-19T15:59:21.368Z\"},{\"id\":\"2DYOnQD6moK2E2VF\",\"name\":\"App 2\",\"createdAt\":\"2024-12-19T04:43:15.771Z\",\"updatedAt\":\"2024-12-19T04:43:15.771Z\"}],\"nodes\":[{\"id\":\"be5c3e43-cc86-4081-aa98-e7af3d22267d\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[200,-960],\"parameters\":{},\"typeVersion\":1},{\"id\":\"28f"}
{"prompt": "Create an n8n workflow for: Generate SQL queries from schema only - AI-powered This workflow is a modification of the previous template on how to create an SQL agent with LangChain and SQLite. The key difference \u2013 the agent has access only to the database schema, not to the actual data. To achieve this, SQL queries are made outside the AI Agent node, and the results are never passed back to the agent. This approach allows the agent to generate SQL queries based on the structure of tables and their relationships, without having to access the actual data. This makes the process more secure and efficient, especially in cases where data confidentiality is crucial. \ud83d\ude80 Setup To get started with this workflow, you\u2019ll need to set up a free MySQL server and import your database (check Step 1 and 2 in this tutorial). Of course, you can switch MySQL to another SQL database such as PostgreSQL, the principle remains the same. The key is to download the schema once and save it locally to avoid repeated remote connections. Run the top part of the workflow once to download and store the MySQL chinook database schema file on the server. With this approach, we avoid the need to repeatedly connect to a remote db4free database and fetch the schema every time. As a result, we reach greater processing speed and efficiency. \ud83d\udde3\ufe0f Chat with your data Start a chat: send a message in the chat window. The workflow loads the locally saved MySQL database schema, without having the ability to touch the actual data. The file contains the full structure of your MySQL database for analysis. The Langchain AI Agent receives the schema, your input and begins to work. The AI Agent generates SQL queries and brief comments based solely on the schema and the user\u2019s message. An IF node checks whether the AI Agent has generated a query. When: Yes: the AI Agent passes the SQL query to the next MySQL node for execution. No: You get a direct answer from the Agent without further action. The workflow formats the results of the SQL query, ensuring they are convenient to read and easy to understand. Once formatted, you get both the Agent answer and the query result in the chat window. \ud83c\udf1f Example queries Try these sample queries to see the schema-driven AI Agent in action: Would you please list me all customers from Germany? What are the music genres in the database? What tables are available in the database? Please describe the relationships between tables. - In this example, the AI Agent does not need to create the SQL query. And if you prefer to keep the data private, you can manually execute the generated SQL query in your own environment using any database client or tool you trust \ud83d\uddc4\ufe0f \ud83d\udcad The AI Agent memory node does not store the actual data as we run SQL-queries outside the agent. It contains the database schema, user questions and the initial Agent reply. Actual SQL query results are passed to the chat window, but the values are not stored in the Agent memory. Yulia | 4 months ago | Free", "completion": "{\"id\":\"P307QnrxpA1ddsM5\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"Generate SQL queries from schema only - AI-powered\",\"tags\":[],\"nodes\":[{\"id\":\"b7c3ca47-11b3-4378-81fa-68b2f56b295e\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1460,440],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.2}},\"credentials\":{\"openAiApi\":{\"id\":\"rveqdSfp7pCRON1T\",\"name\":\"Ted's Tech Talks OpenAi\"}},\"typeVersion\":1},{\"id\":\"977c3a82-440b-4d44-9042-47a673bcb52c\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1640,440],\"parameters\":{\"contextWindowLength\":10},\"typeVersion\":1.2},{\"id\":\"c6e9c0e2-d238-4f0b-a4c8-2271f2c8b31b\",\"name\":\"No Operation, do nothing\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[2340,520],\"parameters\":{},\"typeVersion\":1},{\"id\":\"4c141ae8-d2d1-45c7-bb5d-f33841d3cee6\",\"name\":\"List all tables in a database\",\"type\":\"n8n-nodes-base.mySql\",\"position\":[520,-35],\"parameters\":{\"query\":\"SHOW TABLES;\",\"options\":{},\"operation\":\"executeQuery\"},\"credentials\":{\"mySql\":{\"id\":\"ICakJ1LRuVl4dRTs\",\"name\":\"db4free TTT account\"}},\"typeVersion\":2.4},{\"id\":\"54fb3362-041b-4e4f-bfea-f0"}
{"prompt": "Create an n8n workflow for: Notion AI Assistant Generator This n8n workflow template lets teams easily generate a custom AI chat assistant based on the schema of any Notion database. Simply provide the Notion database URL, and the workflow downloads the schema and creates a tailored AI assistant designed to interact with that specific database structure. Set Up Watch this quick set up video \ud83d\udc47 Key Features Instant Assistant Generation**: Enter a Notion database URL, and the workflow produces an AI assistant configured to the database schema. Advanced Querying**: The assistant performs flexible queries, filtering records by multiple fields (e.g., tags, names). It can also search inside Notion pages to pull relevant content from specific blocks. Schema Awareness**: Understands and interacts with various Notion column types like text, dates, and tags for accurate responses. Reference Links**: Each query returns direct links to the exact Notion pages that inform the assistant\u2019s response, promoting transparency and easy access. Self-Validation**: The workflow has logic to check the generated assistant, and if any errors are detected, it reruns the agent to fix them. Ideal for Product Managers**: Easily access and query product data across Notion databases. Support Teams**: Quickly search through knowledge bases for precise information to enhance support accuracy. Operations Teams**: Streamline access to HR, finance, or logistics data for fast, efficient retrieval. Data Teams**: Automate large dataset queries across multiple properties and records. How It Works This AI assistant leverages two HTTP request tools\u2014one for querying the Notion database and another for retrieving data within individual pages. It\u2019s powered by the Anthropic LLM (or can be swapped for GPT-4) and always provides reference links for added transparency. Max Tkacz | 5 months ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"9052b5b2-1e2d-425c-92e5-1ed51323e71c\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"color\":7,\"width\":616.7964812508943,\"height\":231.27721611949534,\"content\":\"# Generate new workflow version for specific notion db schema\\nInput a Notion database URL and get an AI Assistant chatbot workflow for it based on this template: https://n8n.io/workflows/2413-notion-knowledge-base-ai-assistant/\\n\\nProject in notion: https://www.notion.so/n8n/Chat-with-notion-database-84eec91b74dd4e36ba97edda17c2c306\"},\"typeVersion\":1},{\"id\":\"b4a83f76-2bad-4bbe-9b7f-1df684166035\",\"name\":\"Notion\",\"type\":\"n8n-nodes-base.notion\",\"onError\":\"continueErrorOutput\",\"position\":[1280,480],\"parameters\":{\"simple\":false,\"resource\":\"database\",\"databaseId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"={{ $json.chatInput.match(/https?:\\\\/\\\\/[^\\\\s/$.?#].[^\\\\s]*/g)[0] }}\"}},\"credentials\":{\"notionApi\":{\"id\":\"aDS2eHXMOtsMrQnJ\",\"name\":\"Nathan's notion account\"}},\"typeVersion\":2.2},{\"id\":\"39537c95-5ca0-47a9-b2bf-2c0134d3f236\",\"name\":\"Return success to chat\",\"type\":\"n8n-nodes-base.set\",\"position\":[3540,740],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"bebcb43c-461d-40d7-af83-436d94733622\",\"name\":\"output\",\"type\":\"string\",\"value\":\"=Created workflow:\\n```\\n{{ $json.generatedWorkflow }}\\n```\\n\\n\u261d\ufe0f Copy and paste JSON above into an n8"}
{"prompt": "Create an n8n workflow for: Upsert huge documents in a vector store with Supabase and Notion Purpose This workflow adds the capability to build a RAG on living data. In this case Notion is used as a Knowledge Base. Whenever a page is updated, the embeddings get upserted in a Supabase Vector Store. It can also be fairly easily adapted to PGVector, Pinecone, or Qdrant by using a custom HTTP request for the latter two. Demo How it works A trigger checks every minute for changes in the Notion Database. The manual polling approach improves accuracy and prevents changes from being lost between cached polling intervals. Afterwards every updated page is processed sequentially The Vector Database is searched using the Notion Page ID stored in the metadata of each embedding. If old entries exist, they are deleted. All blocks of the Notion Database Page are retrieved and combined into a single string The content is embedded and split into chunks if necessary. Metadata, including the Notion Page ID, is added during storage for future reference. A simple Question and Answer Chain enables users to ask questions about the embedded content through the integrated chat function Prerequisites To setup a new Vector Store in Supabase, follow this guide Prepare a simple Database in Notion with each Database Page containing at least a title and some content in the blocks section. You can of course also connect this to an existing Database of your choice. Setup Select your credentials in the nodes which require those If you are on an n8n cloud plan, switch to the native Notion Trigger by activating it and deactivating the Schedule Trigger along with its subsequent Notion Node Choose your Notion Database in the first Node related to Notion Adjust the chunk size and overlap in the Token Splitter to your preference Activate the workflow How to use Populate your Notion Database with useful information and use the chat mode of this workflow to ask questions about it. Updates to a Notion Page should quickly reflect in future conversations. Mario | 3 months ago | Free +4", "completion": "{\"id\":\"JxFP8FJ2W7e4Kmqn\",\"meta\":{\"instanceId\":\"fb8bc2e315f7f03c97140b30aa454a27bc7883a19000fa1da6e6b571bf56ad6d\",\"templateCredsSetupCompleted\":true},\"name\":\"RAG on living data\",\"tags\":[],\"nodes\":[{\"id\":\"49086cdf-a38c-4cb8-9be9-d3e6ea5bdde5\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1740,1040],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"X7Jf0zECd3IkQdSw\",\"name\":\"OpenAi (octionicsolutions)\"}},\"typeVersion\":1},{\"id\":\"f0670721-92f4-422a-99c9-f9c2aa6fe21f\",\"name\":\"Token Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter\",\"position\":[2380,540],\"parameters\":{\"chunkSize\":500},\"typeVersion\":1},{\"id\":\"fe80ecac-4f79-4b07-ad8e-60ab5f980cba\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[1180,-200],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"81b79248-08e8-4214-872b-1796e51ad0a4\",\"name\":\"Question and Answer Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainRetrievalQa\",\"position\":[744,495],\"parameters\":{\"options\":{}},\"typeVersion\":1.3},{\"id\":\"e78f7b63-baef-4834-8f1b-aecfa9102d6c\",\"name\":\"Vector Store Retriever\",\"type\":\"@n8n/n8n-nodes-langchain.retrieverVectorStore\",\"position\":[844,715],\"parameters\":{},\"typeVersion"}
{"prompt": "Create an n8n workflow for: Automate Customer Support Issue Resolution using AI Text Classifier This n8n template is designed to assist and improve customer support team member capacity by automating the resolution of long-lived and forgotten JIRA issues. How it works Schedule Trigger runs daily to check for long-lived unresolved issues and imports them into the workflow. Each Issue is handled as a separate subworkflow by using an execute workflow node. This allows parallel processing. A report is generated from the issue using its comment history allowing the issue to be classified by AI - determining the state and progress of the issue. If determined to be resolved, sentiment analysis is performed to track customer satisfaction. If negative, a slack message is sent to escalate, otherwise the issue is closed automatically. If no response has been initiated, an AI agent will attempt to search and resolve the issue itself using similar resolved issues or from the notion database. If a solution is found, it is posted to the issue and closed. If the issue is blocked and waiting for responses, then a reminder message is added. How to use This template searches for JIRA issues which are older than 7 days which are not in the \"Done\" status. Ensure there are some issues that meet this criteria otherwise adjust the search query to suit. Works best if you frequently have long-lived issues that need resolving. Ensure the notion tool is configured as to not read documents you didn't intend it to ie. private and/or internal documentation. Requirements JIRA for issues management OpenAI for LLM Slack for notifications Customising this workflow Why not try classifying issues as they are created? One use-case may be for quality control such as ensuring reporting criteria is adhered to, summarising and rephrasing issue for easier reading or adjusting priority. Jimleuk | 4 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"645799b0-7ddb-4acb-a95d-3b04eadff445\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1480,20],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"e2923385-2f73-439c-9d5c-5a3c560993cb\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2040,420],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"c24728f9-73b9-45f7-9c4e-aee872c59714\",\"name\":\"OpenAI Chat Model3\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[3180,-80],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"0bc19e46-4a65-45fb-9571-d1f00d204c63\",\"name\":\"OpenAI Chat Model4\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2060,-261],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccI"}
{"prompt": "Create an n8n workflow for: Notion knowledge base AI assistant Who is this for This workflow is perfect for teams and individuals who manage extensive data in Notion and need a quick, AI-powered way to interact with their databases. If you're looking to streamline your knowledge management, automate searches, and get faster insights from your Notion databases, this workflow is for you. It\u2019s ideal for support teams, project managers, or anyone who needs to query specific data across multiple records or within individual pages of their Notion setup. Check out the Notion template this Assistant is set up to use: https://www.notion.so/templates/knowledge-base-ai-assistant-with-n8n How it works The Notion Database Assistant uses an AI Agent built with Retrieval-Augmented Generation (RAG) to query this Knowledge Base style Notion database. The assistant can search across multiple properties like tags or question and retrieves content from inside individual Notion pages for additional context. Key features include: Querying the database with flexible filters. Searching within individual Notion pages and extracting relevant blocks. Providing a reference link to the exact Notion pages used to inform its responses, ensuring transparency and easy verification. This assistant uses two HTTP request tools\u2014one for querying the Notion database and another for pulling data from within specific pages. It streamlines knowledge retrieval, offering a conversational, AI-driven way to interact with large datasets. Set up Find basic set up instructions inside the workflow itself or watch a quickstart video \ud83d\udc47 Max Tkacz | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"nodes\":[{\"id\":\"d1d4291e-fa37-43d0-81e0-f0a594371426\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[680,620],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"timeout\":25000,\"temperature\":0.7}},\"credentials\":{\"openAiApi\":{\"id\":\"AzPPV759YPBxJj3o\",\"name\":\"Max's DevRel OpenAI account\"}},\"typeVersion\":1},{\"id\":\"68e6805b-9c19-4c9e-a300-8983f2b7c28a\",\"name\":\"Search notion database\",\"type\":\"@n8n/n8n-nodes-langchain.toolHttpRequest\",\"position\":[980,620],\"parameters\":{\"url\":\"=https://api.notion.com/v1/databases/{{ $json.notionID }}/query\",\"method\":\"POST\",\"jsonBody\":\"{\\n \\\"filter\\\": {\\n \\\"or\\\": [\\n {\\n \\\"property\\\": \\\"question\\\",\\n \\\"rich_text\\\": {\\n \\\"contains\\\": \\\"{keyword}\\\"\\n }\\n },\\n {\\n \\\"property\\\": \\\"tags\\\",\\n \\\"multi_select\\\": {\\n \\\"contains\\\": \\\"{tag}\\\"\\n }\\n }\\n ]\\n },\\n \\\"sorts\\\": [\\n {\\n \\\"property\\\": \\\"updated_at\\\",\\n \\\"direction\\\": \\\"ascending\\\"\\n }\\n ]\\n}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"toolDescription\":\"=Use this tool to search the \\\"\\\" Notion app database.\\n\\nIt is structured with question and answer format. \\nYou can filter query result by:\\n- By keyword\\n- filter by tag.\\n\\nKeyword and Tag have an OR relationship not AND.\\n\\n\",\"nodeCredentialType\":\"notionA"}
{"prompt": "Create an n8n workflow for: AI web researcher for sales Who is this for? This workflow is for all sales reps and lead generation manager who need to prepare their prospecting activities, and find relevant information to personalize their outreach. Use Case This workflow allows you to do account research with the web using AI. It has the potential to replace manual work done by sales rep when preparing their prospecting activities by searching complex information available online. What this workflow does The advanced AI module has 2 capabilities: Research Google using SerpAPI Visit and get website content using a sub-workflow From an unstructured input like a domain or a company name. It will return the following properties: domain company Linkedin Url cheapest plan has free trial has entreprise plan has API market (B2B or B2C) The strength of n8n here is that you can adapt this workflow to research whatever information you need. You just have to precise it in the prompt and to precise the output format in the \"Strutured Output Parser\" module. Detailed instructions + video guide can be found by following this link. Lucas Perret | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"2b1cc1a8b0a2fb9caab11ab2d5eb3712f9973066051b2e898cf4041a1f2a7757\",\"templateId\":\"2324\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"71b06728-7f59-49e3-9365-3281189a6659\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[920,340],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b37019e3-c7ab-4119-986d-c27d082a036e\",\"name\":\"Input\",\"type\":\"n8n-nodes-base.set\",\"position\":[1340,340],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"fcc97354-b9f6-4459-a004-46e87902c77c\",\"name\":\"company_input\",\"type\":\"string\",\"value\":\"={{ $json.input }}\"},{\"id\":\"e5415c49-5204-45b1-a0e9-814157127b12\",\"name\":\"row_number\",\"type\":\"number\",\"value\":\"={{ $json.row_number }}\"}]}},\"typeVersion\":3.3},{\"id\":\"7d5d53ac-6d3c-4b24-97c7-deb6b76749e5\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2020,660],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"FMTQypGcsAwaRQdC\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"24e2f3b0-8b90-49a9-bde6-0fb0c2baf52a\",\"name\":\"Get website content\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[2580,680],\"parameters\":{\"name\":\"get_website_content\",\"source\":\"param"}
{"prompt": "Create an n8n workflow for: Autonomous AI crawler This workflow with AI agent is designed to navigate through the page to retrieve specific type of information (in this example: social media profile links). The agent is equipped with 2 tools: text tool:** to retrieve all the text from the page, URLs tool:** to extract all possible links from the page. \ud83d\udca1 You can edit prompt and JSON schema connected to the agent in order to return other data then social media profile links. \ud83d\udc49 This workflow uses Supabase as storage (input/output). Feel free to change it to any other database of your choice. \ud83c\udfac See this workflow in action in my YouTube video. How it works? The workflow uses the input URL (website) as a starting point to retrieve the data (e.g. example.com). Using the \"URLs tool\", the agent is able to retrieve all links from the page and navigate to them. For example, if you want to retrieve contact information, agent will try to find a subpage that might contain this information (e.g. example.com/contact) and extract the information using the text tool. Set up steps Connect database with input data (website addresses) or pin sample data to trigger node. Configure the crawling agent to retrieve the desired data (e.g. modify prompt and/or parsing schema). Set credentials for OpenAI. Optionally: split agent tools to separate workflows. If you like this workflow, please subscribe to my YouTube channel and/or my newsletter. Oskar | 8 months ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"6cdc45e5-1fa4-47fe-b80a-0e1560996936\",\"name\":\"Text\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1460,980],\"parameters\":{\"name\":\"text_retrieval_tool\",\"source\":\"parameter\",\"description\":\"Call this tool to return all text from the given website. Query should be full website URL.\",\"workflowJson\":\"{\\n \\\"nodes\\\": [\\n {\\n \\\"parameters\\\": {},\\n \\\"id\\\": \\\"05107436-c9cb-419b-ae8a-b74d309a130d\\\",\\n \\\"name\\\": \\\"Execute workflow\\\",\\n \\\"type\\\": \\\"n8n-nodes-base.manualTrigger\\\",\\n \\\"typeVersion\\\": 1,\\n \\\"position\\\": [\\n 2220,\\n 620\\n ]\\n },\\n {\\n \\\"parameters\\\": {\\n \\\"assignments\\\": {\\n \\\"assignments\\\": [\\n {\\n \\\"id\\\": \\\"253c2b17-c749-4f0a-93e8-5ff74f1ce49b\\\",\\n \\\"name\\\": \\\"domain\\\",\\n \\\"value\\\": \\\"={{ $json.query }}\\\",\\n \\\"type\\\": \\\"string\\\"\\n }\\n ]\\n },\\n \\\"options\\\": {}\\n },\\n \\\"id\\\": \\\"bb8be616-3227-4705-8520-1827069faacd\\\",\\n \\\"name\\\": \\\"Set domain\\\",\\n \\\"type\\\": \\\"n8n-nodes-base.set\\\",\\n \\\"typeVersion\\\": 3.3,\\n \\\"position\\\": [\\n 2440,\\n 620\\n ]\\n },\\n {\\n \\\"parameters\\\": {\\n \\\"assignments\\\": {\\n \\\"assignments\\\": [\\n {\\n \\\"id\\\": \\\"ed0f1505-82b6-4393-a0d8-088055137ec9\\\",\\n \\\"name\\\": \\\"domain\\\",\\n \\\"value\\\": \\\"={{ $json.domain.startsWith(\\\\\\\"http\\\\\\\") ? $json.domain : \\\\\\\"http://\\\\\\\" + $json.domain }}\\\","}
{"prompt": "Create an n8n workflow for: Using External Workflows as Tools in n8n This guide will show you how to use a workflow as a reusable tool in n8n, such as integrating an AI Agent or other specialized processes into your workflows. By the end of this example, you'll have a simple, reusable workflow that can be easily plugged into larger projects, making your automations more efficient and scalable. With this approach, you can create reusable workflows like \"Scrape a Page,\" \"Search Brave,\" or \"Generate an Image,\" which you can then call whenever needed. While n8n makes it easy to build these workflows from scratch, setting them up as reusable components saves time as your automations grow in complexity. Setup Add the \"Execute Workflow Trigger\" node Add the node(s) to perform the desired tasks in the workflow Add a final \"Set\" or \"Edit Fields\" node at the end to ensure all external workflows return a consistent output format Details In this example, the \"Execute Workflow Trigger\" expects input in the following JSON format: [ { \"query\": { \"url\": \"https://en.wikipedia.org/wiki/some_info\" } } ] Once your external workflow is ready, you can instruct the AI Agent to use this tool by connecting it to the external workflow. Set up the schema type to \"Generate from JSON Example\" using this structure: { \"url\": \"URL_TO_GET\" } Finally, ensure your external workflow includes a \"Set\" or \"Edit Fields\" node at the end to define the response format. This helps keep the outputs of your reusable workflows consistent and predictable. Alfred Nutile | 1 month ago | Free", "completion": "{\"id\":\"7DPLpEkww5Uctcml\",\"meta\":{\"instanceId\":\"75d76ac1fb686d403c2294ca007b62282f34c3e15dc3528cc1dbe36a827c0c6e\"},\"name\":\"get_a_web_page\",\"tags\":[{\"id\":\"7v5QbLiQYkQ7zGTK\",\"name\":\"tools\",\"createdAt\":\"2025-01-08T16:33:21.887Z\",\"updatedAt\":\"2025-01-08T16:33:21.887Z\"}],\"nodes\":[{\"id\":\"290cc9b8-e4b1-4124-ab0e-afbb02a9072b\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[-460,-100],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f256ed59-ba61-4912-9a75-4e7703547de5\",\"name\":\"FireCrawl\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-220,-100],\"parameters\":{\"url\":\"https://api.firecrawl.dev/v1/scrape\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"url\\\": \\\"{{ $json.query.url }}\\\",\\n \\\"formats\\\": [\\n \\\"markdown\\\"\\n ]\\n} \",\"sendBody\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"headerParameters\":{\"parameters\":[{}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"RoJ6k6pWBzSVp9JK\",\"name\":\"Firecrawl\"}},\"typeVersion\":4.2},{\"id\":\"a28bdbe6-fa59-4bf1-b0ab-c34ebb10cf0f\",\"name\":\"Edit Fields\",\"type\":\"n8n-nodes-base.set\",\"position\":[-20,-100],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"1af62ef9-7385-411a-8aba-e4087f09c3a9\",\"name\":\"response\",\"type\":\""}
{"prompt": "Create an n8n workflow for: Generate 9:16 Images from Content and Brand Guidelines Overview This n8n workflow automates the creation of 9:16 aspect ratio images optimized for short-form video content and thumbnails. It integrates multiple tools to retrieve content, generate scripts, and create AI-generated imagery. Key Features Trigger Workflow Manually The workflow starts when triggered manually in n8n. Retrieve Brand Guidelines Fetch brand elements like style, tone, and guidelines from Airtable. SEO Keywords and Blog Post Retrieval Retrieves blog posts and associated SEO keywords from Airtable to form the basis of image content. Content Preparation Uses GPT-4 to prepare a 4-scene script and thumbnail prompts for short-form videos. AI Image Generation Uses Leonardo.ai API to generate: Thumbnail Images Scene-specific Images (9:16 Aspect Ratio) Airtable Asset Management Generated assets (images) are saved back into Airtable with metadata like URLs and file sizes. Tools and Integrations n8n**: Workflow automation platform. OpenAI**: Generates scripts and prompts (GPT-4O-MINI). Leonardo.ai**: AI tool for improving prompts and generating high-quality images. Airtable**: Used as a data source for brand guidelines, blog posts, and to store generated assets. Workflow Steps Manual Trigger Initiate the workflow. Retrieve Brand and SEO Guidelines Fetch essential brand elements like tone, style, and keywords. Filter and Fetch Blog Content Search for blog posts relevant to selected SEO keywords. Script Preparation Use GPT-4 to generate a script with image prompts for scenes and thumbnails. Image Generation Call Leonardo.ai to create: Scene Images in 9:16 Aspect Ratio. A Thumbnail Image with an improved prompt. Store Assets Save generated assets (images) to Airtable for future use. Workflow Structure Nodes Breakdown: Manual Trigger**: Start the workflow. Get Brand Guidelines**: Pull brand-related information (style, tagline, tone, etc.) from Airtable. Set Guidelines**: Prepare fetched data. Get SEO Keywords**: Retrieve keywords to filter relevant content. Keyword Filter*: Filter results for specified keywords (e.g., *\"AI Automation\"). Script Prep**: Generate 4-scene scripts and prompts with GPT-4. Leo - Improve Prompt**: Improve image prompts for clarity and detail. Leo - Generate Image**: Create AI-generated images for scenes and thumbnails. Wait Nodes**: Ensures Leonardo image generation is complete. Add Asset Info**: Store the generated images back into Airtable with metadata. API Credentials Required Ensure the following credentials are configured in n8n: OpenAI API Key Leonardo.ai API Key Airtable API Token Output Generated Images**: High-quality AI-generated images with a 9:16 aspect ratio. Saved Metadata**: Asset details (URLs, sizes, types) stored in Airtable. Usage Import this workflow into n8n. Set up your Airtable API, Leonardo.ai API, and OpenAI API credentials. Run the workflow manually. Monitor image generation and check the Airtable output for results. Tags OpenAI** RunwayML** Leonardo** Airtable** Video Automation** Author AlexK1919 AI-Native Workflow Architect More Workflow Templates YouTube Channel Connect with Alex Alex Kim | 2 months ago | Free", "completion": "{\"id\":\"VlCgU5K9SYQbdxTa\",\"meta\":{\"instanceId\":\"d868e3d040e7bda892c81b17cf446053ea25d2556fcef89cbe19dd61a3e876e9\"},\"name\":\"Content to 9:16 Aspect Image Generator v1\",\"tags\":[{\"id\":\"QsH2EXuw2e7YCv0K\",\"name\":\"OpenAI\",\"createdAt\":\"2024-11-15T04:05:20.872Z\",\"updatedAt\":\"2024-11-15T04:05:20.872Z\"},{\"id\":\"04PL2irdWYmF2Dg3\",\"name\":\"RunwayML\",\"createdAt\":\"2024-11-15T05:55:30.783Z\",\"updatedAt\":\"2024-11-15T05:55:30.783Z\"},{\"id\":\"yrY6updwSCXMsT0z\",\"name\":\"Video\",\"createdAt\":\"2024-11-15T05:55:34.333Z\",\"updatedAt\":\"2024-11-15T05:55:34.333Z\"},{\"id\":\"lvPj9rYRsKOHCi4J\",\"name\":\"Creatomate\",\"createdAt\":\"2024-11-19T15:59:16.134Z\",\"updatedAt\":\"2024-11-19T15:59:16.134Z\"},{\"id\":\"9LXACqpQLNtrM6or\",\"name\":\"Leonardo\",\"createdAt\":\"2024-11-19T15:59:21.368Z\",\"updatedAt\":\"2024-11-19T15:59:21.368Z\"},{\"id\":\"2DYOnQD6moK2E2VF\",\"name\":\"App 2\",\"createdAt\":\"2024-12-19T04:43:15.771Z\",\"updatedAt\":\"2024-12-19T04:43:15.771Z\"}],\"nodes\":[{\"id\":\"be5c3e43-cc86-4081-aa98-e7af3d22267d\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[200,-960],\"parameters\":{},\"typeVersion\":1},{\"id\":\"28f"}
{"prompt": "Create an n8n workflow for: Generate SQL queries from schema only - AI-powered This workflow is a modification of the previous template on how to create an SQL agent with LangChain and SQLite. The key difference \u2013 the agent has access only to the database schema, not to the actual data. To achieve this, SQL queries are made outside the AI Agent node, and the results are never passed back to the agent. This approach allows the agent to generate SQL queries based on the structure of tables and their relationships, without having to access the actual data. This makes the process more secure and efficient, especially in cases where data confidentiality is crucial. \ud83d\ude80 Setup To get started with this workflow, you\u2019ll need to set up a free MySQL server and import your database (check Step 1 and 2 in this tutorial). Of course, you can switch MySQL to another SQL database such as PostgreSQL, the principle remains the same. The key is to download the schema once and save it locally to avoid repeated remote connections. Run the top part of the workflow once to download and store the MySQL chinook database schema file on the server. With this approach, we avoid the need to repeatedly connect to a remote db4free database and fetch the schema every time. As a result, we reach greater processing speed and efficiency. \ud83d\udde3\ufe0f Chat with your data Start a chat: send a message in the chat window. The workflow loads the locally saved MySQL database schema, without having the ability to touch the actual data. The file contains the full structure of your MySQL database for analysis. The Langchain AI Agent receives the schema, your input and begins to work. The AI Agent generates SQL queries and brief comments based solely on the schema and the user\u2019s message. An IF node checks whether the AI Agent has generated a query. When: Yes: the AI Agent passes the SQL query to the next MySQL node for execution. No: You get a direct answer from the Agent without further action. The workflow formats the results of the SQL query, ensuring they are convenient to read and easy to understand. Once formatted, you get both the Agent answer and the query result in the chat window. \ud83c\udf1f Example queries Try these sample queries to see the schema-driven AI Agent in action: Would you please list me all customers from Germany? What are the music genres in the database? What tables are available in the database? Please describe the relationships between tables. - In this example, the AI Agent does not need to create the SQL query. And if you prefer to keep the data private, you can manually execute the generated SQL query in your own environment using any database client or tool you trust \ud83d\uddc4\ufe0f \ud83d\udcad The AI Agent memory node does not store the actual data as we run SQL-queries outside the agent. It contains the database schema, user questions and the initial Agent reply. Actual SQL query results are passed to the chat window, but the values are not stored in the Agent memory. Yulia | 4 months ago | Free", "completion": "{\"id\":\"P307QnrxpA1ddsM5\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"Generate SQL queries from schema only - AI-powered\",\"tags\":[],\"nodes\":[{\"id\":\"b7c3ca47-11b3-4378-81fa-68b2f56b295e\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1460,440],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.2}},\"credentials\":{\"openAiApi\":{\"id\":\"rveqdSfp7pCRON1T\",\"name\":\"Ted's Tech Talks OpenAi\"}},\"typeVersion\":1},{\"id\":\"977c3a82-440b-4d44-9042-47a673bcb52c\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1640,440],\"parameters\":{\"contextWindowLength\":10},\"typeVersion\":1.2},{\"id\":\"c6e9c0e2-d238-4f0b-a4c8-2271f2c8b31b\",\"name\":\"No Operation, do nothing\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[2340,520],\"parameters\":{},\"typeVersion\":1},{\"id\":\"4c141ae8-d2d1-45c7-bb5d-f33841d3cee6\",\"name\":\"List all tables in a database\",\"type\":\"n8n-nodes-base.mySql\",\"position\":[520,-35],\"parameters\":{\"query\":\"SHOW TABLES;\",\"options\":{},\"operation\":\"executeQuery\"},\"credentials\":{\"mySql\":{\"id\":\"ICakJ1LRuVl4dRTs\",\"name\":\"db4free TTT account\"}},\"typeVersion\":2.4},{\"id\":\"54fb3362-041b-4e4f-bfea-f0"}
{"prompt": "Create an n8n workflow for: Notion AI Assistant Generator This n8n workflow template lets teams easily generate a custom AI chat assistant based on the schema of any Notion database. Simply provide the Notion database URL, and the workflow downloads the schema and creates a tailored AI assistant designed to interact with that specific database structure. Set Up Watch this quick set up video \ud83d\udc47 Key Features Instant Assistant Generation**: Enter a Notion database URL, and the workflow produces an AI assistant configured to the database schema. Advanced Querying**: The assistant performs flexible queries, filtering records by multiple fields (e.g., tags, names). It can also search inside Notion pages to pull relevant content from specific blocks. Schema Awareness**: Understands and interacts with various Notion column types like text, dates, and tags for accurate responses. Reference Links**: Each query returns direct links to the exact Notion pages that inform the assistant\u2019s response, promoting transparency and easy access. Self-Validation**: The workflow has logic to check the generated assistant, and if any errors are detected, it reruns the agent to fix them. Ideal for Product Managers**: Easily access and query product data across Notion databases. Support Teams**: Quickly search through knowledge bases for precise information to enhance support accuracy. Operations Teams**: Streamline access to HR, finance, or logistics data for fast, efficient retrieval. Data Teams**: Automate large dataset queries across multiple properties and records. How It Works This AI assistant leverages two HTTP request tools\u2014one for querying the Notion database and another for retrieving data within individual pages. It\u2019s powered by the Anthropic LLM (or can be swapped for GPT-4) and always provides reference links for added transparency. Max Tkacz | 5 months ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"9052b5b2-1e2d-425c-92e5-1ed51323e71c\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"color\":7,\"width\":616.7964812508943,\"height\":231.27721611949534,\"content\":\"# Generate new workflow version for specific notion db schema\\nInput a Notion database URL and get an AI Assistant chatbot workflow for it based on this template: https://n8n.io/workflows/2413-notion-knowledge-base-ai-assistant/\\n\\nProject in notion: https://www.notion.so/n8n/Chat-with-notion-database-84eec91b74dd4e36ba97edda17c2c306\"},\"typeVersion\":1},{\"id\":\"b4a83f76-2bad-4bbe-9b7f-1df684166035\",\"name\":\"Notion\",\"type\":\"n8n-nodes-base.notion\",\"onError\":\"continueErrorOutput\",\"position\":[1280,480],\"parameters\":{\"simple\":false,\"resource\":\"database\",\"databaseId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"={{ $json.chatInput.match(/https?:\\\\/\\\\/[^\\\\s/$.?#].[^\\\\s]*/g)[0] }}\"}},\"credentials\":{\"notionApi\":{\"id\":\"aDS2eHXMOtsMrQnJ\",\"name\":\"Nathan's notion account\"}},\"typeVersion\":2.2},{\"id\":\"39537c95-5ca0-47a9-b2bf-2c0134d3f236\",\"name\":\"Return success to chat\",\"type\":\"n8n-nodes-base.set\",\"position\":[3540,740],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"bebcb43c-461d-40d7-af83-436d94733622\",\"name\":\"output\",\"type\":\"string\",\"value\":\"=Created workflow:\\n```\\n{{ $json.generatedWorkflow }}\\n```\\n\\n\u261d\ufe0f Copy and paste JSON above into an n8"}
{"prompt": "Create an n8n workflow for: Upsert huge documents in a vector store with Supabase and Notion Purpose This workflow adds the capability to build a RAG on living data. In this case Notion is used as a Knowledge Base. Whenever a page is updated, the embeddings get upserted in a Supabase Vector Store. It can also be fairly easily adapted to PGVector, Pinecone, or Qdrant by using a custom HTTP request for the latter two. Demo How it works A trigger checks every minute for changes in the Notion Database. The manual polling approach improves accuracy and prevents changes from being lost between cached polling intervals. Afterwards every updated page is processed sequentially The Vector Database is searched using the Notion Page ID stored in the metadata of each embedding. If old entries exist, they are deleted. All blocks of the Notion Database Page are retrieved and combined into a single string The content is embedded and split into chunks if necessary. Metadata, including the Notion Page ID, is added during storage for future reference. A simple Question and Answer Chain enables users to ask questions about the embedded content through the integrated chat function Prerequisites To setup a new Vector Store in Supabase, follow this guide Prepare a simple Database in Notion with each Database Page containing at least a title and some content in the blocks section. You can of course also connect this to an existing Database of your choice. Setup Select your credentials in the nodes which require those If you are on an n8n cloud plan, switch to the native Notion Trigger by activating it and deactivating the Schedule Trigger along with its subsequent Notion Node Choose your Notion Database in the first Node related to Notion Adjust the chunk size and overlap in the Token Splitter to your preference Activate the workflow How to use Populate your Notion Database with useful information and use the chat mode of this workflow to ask questions about it. Updates to a Notion Page should quickly reflect in future conversations. Mario | 3 months ago | Free +4", "completion": "{\"id\":\"JxFP8FJ2W7e4Kmqn\",\"meta\":{\"instanceId\":\"fb8bc2e315f7f03c97140b30aa454a27bc7883a19000fa1da6e6b571bf56ad6d\",\"templateCredsSetupCompleted\":true},\"name\":\"RAG on living data\",\"tags\":[],\"nodes\":[{\"id\":\"49086cdf-a38c-4cb8-9be9-d3e6ea5bdde5\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1740,1040],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"X7Jf0zECd3IkQdSw\",\"name\":\"OpenAi (octionicsolutions)\"}},\"typeVersion\":1},{\"id\":\"f0670721-92f4-422a-99c9-f9c2aa6fe21f\",\"name\":\"Token Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter\",\"position\":[2380,540],\"parameters\":{\"chunkSize\":500},\"typeVersion\":1},{\"id\":\"fe80ecac-4f79-4b07-ad8e-60ab5f980cba\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[1180,-200],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"81b79248-08e8-4214-872b-1796e51ad0a4\",\"name\":\"Question and Answer Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainRetrievalQa\",\"position\":[744,495],\"parameters\":{\"options\":{}},\"typeVersion\":1.3},{\"id\":\"e78f7b63-baef-4834-8f1b-aecfa9102d6c\",\"name\":\"Vector Store Retriever\",\"type\":\"@n8n/n8n-nodes-langchain.retrieverVectorStore\",\"position\":[844,715],\"parameters\":{},\"typeVersion"}
{"prompt": "Create an n8n workflow for: Automate Customer Support Issue Resolution using AI Text Classifier This n8n template is designed to assist and improve customer support team member capacity by automating the resolution of long-lived and forgotten JIRA issues. How it works Schedule Trigger runs daily to check for long-lived unresolved issues and imports them into the workflow. Each Issue is handled as a separate subworkflow by using an execute workflow node. This allows parallel processing. A report is generated from the issue using its comment history allowing the issue to be classified by AI - determining the state and progress of the issue. If determined to be resolved, sentiment analysis is performed to track customer satisfaction. If negative, a slack message is sent to escalate, otherwise the issue is closed automatically. If no response has been initiated, an AI agent will attempt to search and resolve the issue itself using similar resolved issues or from the notion database. If a solution is found, it is posted to the issue and closed. If the issue is blocked and waiting for responses, then a reminder message is added. How to use This template searches for JIRA issues which are older than 7 days which are not in the \"Done\" status. Ensure there are some issues that meet this criteria otherwise adjust the search query to suit. Works best if you frequently have long-lived issues that need resolving. Ensure the notion tool is configured as to not read documents you didn't intend it to ie. private and/or internal documentation. Requirements JIRA for issues management OpenAI for LLM Slack for notifications Customising this workflow Why not try classifying issues as they are created? One use-case may be for quality control such as ensuring reporting criteria is adhered to, summarising and rephrasing issue for easier reading or adjusting priority. Jimleuk | 4 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"645799b0-7ddb-4acb-a95d-3b04eadff445\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1480,20],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"e2923385-2f73-439c-9d5c-5a3c560993cb\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2040,420],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"c24728f9-73b9-45f7-9c4e-aee872c59714\",\"name\":\"OpenAI Chat Model3\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[3180,-80],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"0bc19e46-4a65-45fb-9571-d1f00d204c63\",\"name\":\"OpenAI Chat Model4\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2060,-261],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccI"}
{"prompt": "Create an n8n workflow for: Notion knowledge base AI assistant Who is this for This workflow is perfect for teams and individuals who manage extensive data in Notion and need a quick, AI-powered way to interact with their databases. If you're looking to streamline your knowledge management, automate searches, and get faster insights from your Notion databases, this workflow is for you. It\u2019s ideal for support teams, project managers, or anyone who needs to query specific data across multiple records or within individual pages of their Notion setup. Check out the Notion template this Assistant is set up to use: https://www.notion.so/templates/knowledge-base-ai-assistant-with-n8n How it works The Notion Database Assistant uses an AI Agent built with Retrieval-Augmented Generation (RAG) to query this Knowledge Base style Notion database. The assistant can search across multiple properties like tags or question and retrieves content from inside individual Notion pages for additional context. Key features include: Querying the database with flexible filters. Searching within individual Notion pages and extracting relevant blocks. Providing a reference link to the exact Notion pages used to inform its responses, ensuring transparency and easy verification. This assistant uses two HTTP request tools\u2014one for querying the Notion database and another for pulling data from within specific pages. It streamlines knowledge retrieval, offering a conversational, AI-driven way to interact with large datasets. Set up Find basic set up instructions inside the workflow itself or watch a quickstart video \ud83d\udc47 Max Tkacz | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"nodes\":[{\"id\":\"d1d4291e-fa37-43d0-81e0-f0a594371426\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[680,620],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"timeout\":25000,\"temperature\":0.7}},\"credentials\":{\"openAiApi\":{\"id\":\"AzPPV759YPBxJj3o\",\"name\":\"Max's DevRel OpenAI account\"}},\"typeVersion\":1},{\"id\":\"68e6805b-9c19-4c9e-a300-8983f2b7c28a\",\"name\":\"Search notion database\",\"type\":\"@n8n/n8n-nodes-langchain.toolHttpRequest\",\"position\":[980,620],\"parameters\":{\"url\":\"=https://api.notion.com/v1/databases/{{ $json.notionID }}/query\",\"method\":\"POST\",\"jsonBody\":\"{\\n \\\"filter\\\": {\\n \\\"or\\\": [\\n {\\n \\\"property\\\": \\\"question\\\",\\n \\\"rich_text\\\": {\\n \\\"contains\\\": \\\"{keyword}\\\"\\n }\\n },\\n {\\n \\\"property\\\": \\\"tags\\\",\\n \\\"multi_select\\\": {\\n \\\"contains\\\": \\\"{tag}\\\"\\n }\\n }\\n ]\\n },\\n \\\"sorts\\\": [\\n {\\n \\\"property\\\": \\\"updated_at\\\",\\n \\\"direction\\\": \\\"ascending\\\"\\n }\\n ]\\n}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"toolDescription\":\"=Use this tool to search the \\\"\\\" Notion app database.\\n\\nIt is structured with question and answer format. \\nYou can filter query result by:\\n- By keyword\\n- filter by tag.\\n\\nKeyword and Tag have an OR relationship not AND.\\n\\n\",\"nodeCredentialType\":\"notionA"}
{"prompt": "Create an n8n workflow for: AI web researcher for sales Who is this for? This workflow is for all sales reps and lead generation manager who need to prepare their prospecting activities, and find relevant information to personalize their outreach. Use Case This workflow allows you to do account research with the web using AI. It has the potential to replace manual work done by sales rep when preparing their prospecting activities by searching complex information available online. What this workflow does The advanced AI module has 2 capabilities: Research Google using SerpAPI Visit and get website content using a sub-workflow From an unstructured input like a domain or a company name. It will return the following properties: domain company Linkedin Url cheapest plan has free trial has entreprise plan has API market (B2B or B2C) The strength of n8n here is that you can adapt this workflow to research whatever information you need. You just have to precise it in the prompt and to precise the output format in the \"Strutured Output Parser\" module. Detailed instructions + video guide can be found by following this link. Lucas Perret | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"2b1cc1a8b0a2fb9caab11ab2d5eb3712f9973066051b2e898cf4041a1f2a7757\",\"templateId\":\"2324\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"71b06728-7f59-49e3-9365-3281189a6659\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[920,340],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b37019e3-c7ab-4119-986d-c27d082a036e\",\"name\":\"Input\",\"type\":\"n8n-nodes-base.set\",\"position\":[1340,340],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"fcc97354-b9f6-4459-a004-46e87902c77c\",\"name\":\"company_input\",\"type\":\"string\",\"value\":\"={{ $json.input }}\"},{\"id\":\"e5415c49-5204-45b1-a0e9-814157127b12\",\"name\":\"row_number\",\"type\":\"number\",\"value\":\"={{ $json.row_number }}\"}]}},\"typeVersion\":3.3},{\"id\":\"7d5d53ac-6d3c-4b24-97c7-deb6b76749e5\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2020,660],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"FMTQypGcsAwaRQdC\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"24e2f3b0-8b90-49a9-bde6-0fb0c2baf52a\",\"name\":\"Get website content\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[2580,680],\"parameters\":{\"name\":\"get_website_content\",\"source\":\"param"}
{"prompt": "Create an n8n workflow for: Autonomous AI crawler This workflow with AI agent is designed to navigate through the page to retrieve specific type of information (in this example: social media profile links). The agent is equipped with 2 tools: text tool:** to retrieve all the text from the page, URLs tool:** to extract all possible links from the page. \ud83d\udca1 You can edit prompt and JSON schema connected to the agent in order to return other data then social media profile links. \ud83d\udc49 This workflow uses Supabase as storage (input/output). Feel free to change it to any other database of your choice. \ud83c\udfac See this workflow in action in my YouTube video. How it works? The workflow uses the input URL (website) as a starting point to retrieve the data (e.g. example.com). Using the \"URLs tool\", the agent is able to retrieve all links from the page and navigate to them. For example, if you want to retrieve contact information, agent will try to find a subpage that might contain this information (e.g. example.com/contact) and extract the information using the text tool. Set up steps Connect database with input data (website addresses) or pin sample data to trigger node. Configure the crawling agent to retrieve the desired data (e.g. modify prompt and/or parsing schema). Set credentials for OpenAI. Optionally: split agent tools to separate workflows. If you like this workflow, please subscribe to my YouTube channel and/or my newsletter. Oskar | 8 months ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"6cdc45e5-1fa4-47fe-b80a-0e1560996936\",\"name\":\"Text\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1460,980],\"parameters\":{\"name\":\"text_retrieval_tool\",\"source\":\"parameter\",\"description\":\"Call this tool to return all text from the given website. Query should be full website URL.\",\"workflowJson\":\"{\\n \\\"nodes\\\": [\\n {\\n \\\"parameters\\\": {},\\n \\\"id\\\": \\\"05107436-c9cb-419b-ae8a-b74d309a130d\\\",\\n \\\"name\\\": \\\"Execute workflow\\\",\\n \\\"type\\\": \\\"n8n-nodes-base.manualTrigger\\\",\\n \\\"typeVersion\\\": 1,\\n \\\"position\\\": [\\n 2220,\\n 620\\n ]\\n },\\n {\\n \\\"parameters\\\": {\\n \\\"assignments\\\": {\\n \\\"assignments\\\": [\\n {\\n \\\"id\\\": \\\"253c2b17-c749-4f0a-93e8-5ff74f1ce49b\\\",\\n \\\"name\\\": \\\"domain\\\",\\n \\\"value\\\": \\\"={{ $json.query }}\\\",\\n \\\"type\\\": \\\"string\\\"\\n }\\n ]\\n },\\n \\\"options\\\": {}\\n },\\n \\\"id\\\": \\\"bb8be616-3227-4705-8520-1827069faacd\\\",\\n \\\"name\\\": \\\"Set domain\\\",\\n \\\"type\\\": \\\"n8n-nodes-base.set\\\",\\n \\\"typeVersion\\\": 3.3,\\n \\\"position\\\": [\\n 2440,\\n 620\\n ]\\n },\\n {\\n \\\"parameters\\\": {\\n \\\"assignments\\\": {\\n \\\"assignments\\\": [\\n {\\n \\\"id\\\": \\\"ed0f1505-82b6-4393-a0d8-088055137ec9\\\",\\n \\\"name\\\": \\\"domain\\\",\\n \\\"value\\\": \\\"={{ $json.domain.startsWith(\\\\\\\"http\\\\\\\") ? $json.domain : \\\\\\\"http://\\\\\\\" + $json.domain }}\\\","}
{"prompt": "Create an n8n workflow for: AI Agent to chat with you Search Console Data, using OpenAI and Postgres Edit 19/11/2024: As explained on the workflow, the AI Agent with the original system prompt was not effective when using gpt4-o-mini. To address this, I optimized the prompt to work better with this model. You can find the prompts I\u2019ve tested on this Notion Page. And yes, there is one that works well with gpt4-o-mini. AI Agent to chat with you Search Console Data, using OpenAI and Postgres This AI Agent enables you to interact with your Search Console data through a chat interface. Each node is documented within the template, providing sufficient information for setup and usage. You will also need to configure Search Console OAuth credentials. Follow this n8n documentation to set up the OAuth credentials. Important Notes Correctly Configure Scopes for Search Console API Calls It\u2019s essential to configure the scopes correctly in your Google Search Console API OAuth2 credentials. Incorrect configuration can cause issues with the refresh token, requiring frequent reconnections. Below is the configuration I use to avoid constant re-authentication: Of course, you'll need to add your client_id and client_secret from the Google Cloud Platform app you created to access your Search Console data. Configure Authentication for the Webhook Since the webhook will be publicly accessible, don\u2019t forget to set up authentication. I\u2019ve used Basic Auth, but feel free to choose the method that best meets your security requirements. \ud83e\udd29\ud83d\udc96 Example of awesome things you can do with this AI Agent Dataki | 3 months ago | Free", "completion": "{\"id\":\"PoiRk5w0xd1ysq4U\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\",\"templateCredsSetupCompleted\":true},\"name\":\"AI Agent to chat with you Search Console Data, using OpenAI and Postgres\",\"tags\":[],\"nodes\":[{\"id\":\"9ee6710b-19b7-4bfd-ac2d-0fe1e2561f1d\",\"name\":\"Postgres Chat Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryPostgresChat\",\"position\":[1796,220],\"parameters\":{\"tableName\":\"insights_chat_histories\"},\"credentials\":{\"postgres\":{\"id\":\"\",\"name\":\"Postgres\"}},\"typeVersion\":1.1},{\"id\":\"eb9f07e9-ded1-485c-9bf3-cf223458384a\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1356,240],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"maxTokens\":16000}},\"credentials\":{\"openAiApi\":{\"id\":\"\",\"name\":\"OpenAi\"}},\"typeVersion\":1},{\"id\":\"1d3d6fb7-a171-4590-be42-df7eb0c208ed\",\"name\":\"Set fields\",\"type\":\"n8n-nodes-base.set\",\"position\":[940,-20],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"9f47b322-e42f-42d7-93eb-a57d22adb849\",\"name\":\"chatInput\",\"type\":\"string\",\"value\":\"={{ $json.body?.chatInput || $json.chatInput }}\"},{\"id\":\"73ec4dd0-e986-4f60-9dca-6aad2f86bdeb\",\"name\":\"sessionId\",\"type\":\"string\",\"value\":\"={{ $json.body?.sessionId || $json.sessionId }}\"},{\"id\":\"4b688c46-b60f-4f0a-83d8-e283"}
{"prompt": "Create an n8n workflow for: Automatic Background Removal for Images in Google Drive This n8n workflow simplifies the process of removing backgrounds from images stored in Google Drive. By leveraging the PhotoRoom API, this template enables automatic background removal, padding adjustments, and output formatting, all while storing the updated images back in a designated Google Drive folder. This workflow is very useful for companies or individuals that are spending a lot of time into removing the background from product images. How it Works The workflow begins with a Google Drive Trigger node that monitors a specific folder for new image uploads. Upon detecting a new image, the workflow downloads the file and extracts essential metadata, such as the file size. Configurations are set for background color, padding, output size, and more, which are all customizable to match specific requirements. The PhotoRoom API is called to process the image by removing its background and adding padding based on the settings. The processed image is saved back to Google Drive in the specified output folder with an updated name indicating the background has been removed. Requirements PhotoRoom API Key Google Drive API Access Customizing the Workflow Easily adjust the background color, padding, and output size using the configuration node. Modify the output folder path in Google Drive or replace Google Drive with another storage service if needed. For advanced use cases, integrate further image processing steps, such as adding captions or analyzing content using AI. Simon | 3 months ago | Free", "completion": "{\"id\":\"oNJCLq4egGByMeSl\",\"meta\":{\"instanceId\":\"1bc0f4fa5e7d17ac362404cbb49337e51e5061e019cfa24022a8667c1f1ce287\",\"templateCredsSetupCompleted\":true},\"name\":\"Remove Advanced Background from Google Drive Images\",\"tags\":[],\"nodes\":[{\"id\":\"99582f98-3707-4480-954a-f091e4e8133a\",\"name\":\"Config\",\"type\":\"n8n-nodes-base.set\",\"position\":[820,620],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"42b02a2f-a642-42db-a565-fd2a01a26fb9\",\"name\":\"bg_color\",\"type\":\"string\",\"value\":\"white\"},{\"id\":\"f68b2280-ec85-4400-8a98-10e644b56076\",\"name\":\"padding\",\"type\":\"string\",\"value\":\"5%\"},{\"id\":\"8bdee3a1-9107-4bf8-adea-332d299e43ae\",\"name\":\"keepInputSize\",\"type\":\"boolean\",\"value\":true},{\"id\":\"89d9e4fb-ed14-4ee2-b6f0-73035bafbc39\",\"name\":\"outputSize\",\"type\":\"string\",\"value\":\"1600x1600\"},{\"id\":\"ad53bf64-5493-4c4d-a52c-cd4d657cc9f9\",\"name\":\"inputFileName\",\"type\":\"string\",\"value\":\"={{ $json.originalFilename }}\"},{\"id\":\"9fc440c6-289b-4a6a-8391-479a6660836f\",\"name\":\"OutputDriveFolder\",\"type\":\"string\",\"value\":\"ENTER GOOGLE DRIVE FOLDER URL\"},{\"id\":\"f0f1767a-b659-48c4-bef6-8ee4111cb939\",\"name\":\"api-key\",\"type\":\"string\",\"value\":\"ENTER API KEY\"}]}},\"typeVersion\":3.4},{\"id\":\"7b5973d4-0d9f-4d17-8b71-e6c4f81d682e\",\"name\":\"remove background\",\"type\":\"n8n-nodes-base"}
{"prompt": "Create an n8n workflow for: \ud83d\udcc8 Receive Daily Market News from FT.com to your Microsoft outlook inbox \ud83d\udcc8 Daily Financial News - Description This workflow automates the process of collecting, organizing, and delivering a daily summary of financial news by following these key steps: Scheduled Activation The workflow starts at 7:00 AM each day, triggered by the Schedule Trigger node. News Retrieval The HTTP Request node fetches the latest financial news from FT.com. Content Extraction The Extract Specific Content node scrapes targeted sections of the HTML (headlines, editor's picks, top stories, etc.) using CSS selectors to locate and capture relevant content. News Aggregation The Set Node gathers and organizes the extracted news data, preparing it for summarization. Categories like \"Headline #1,\" \"Editor's Picks,\" and \"Europe News\" are all structured into a single data block. Summarization An AI Agent (Google Gemini) takes the aggregated news data and creates a concise, HTML-formatted summary tailored to give investors an insightful market snapshot. Email Delivery Finally, the Microsoft Outlook node sends the summary via email to the designated recipient with the subject \"Financial news from today.\" This process ensures that financial news is efficiently curated, summarized, and delivered without manual intervention. Louis | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6045c639951d83c8706b0dd8d6330164bda01fe58f103cedc2c276bf1f9c11f1\"},\"nodes\":[{\"id\":\"d2a24a9b-9cf3-4de0-82e7-5d858658d4b4\",\"name\":\"Extract specific content\",\"type\":\"n8n-nodes-base.html\",\"notes\":\"Extract selected headlines, editor's picks, spotlight etc.\",\"position\":[800,340],\"parameters\":{\"options\":{\"cleanUpText\":true},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"Headline #1\",\"cssSelector\":\"#site-content > div:nth-child(1) > section > div > div > div.layout-desktop__grid.layout-desktop__grid--span4.layout-desktop__grid--column-start-1.layout-desktop__grid--row-start-1.layout-desktop__grid--with-border.layout--default > div > div > div > div.story-group-stacked__primary-story > div > div > div > div > div.primary-story__teaser\"},{\"key\":\"Headline #2\",\"cssSelector\":\"#site-content > div:nth-child(1) > section > div > div > div.layout-desktop__grid.layout-desktop__grid--span6.layout-desktop__grid--column-start-5.layout-desktop__grid--row-start-1.layout-desktop__grid--with-border.layout--default > div > div > div > div > div > div.story-group__article.story-group__article--featured > div > div.featured-story-content > div.headline.js-teaser-headline.headline--scale-5.headline--color-black > a > span\"},{\"key\":\"Editor's Picks\",\"cssSelector\":\"#site-content > div:nth-child(1) > section > div > div > div.layout-desktop__grid.layout-desktop__grid--span2.layout-desktop__grid--column-start-11.layout-desktop__grid--row-start-1.layout--default > div\"},{\"key\":\"Top Stories\",\"cssSelector\":\""}
{"prompt": "Create an n8n workflow for: AI agent that can scrape webpages \u2699\ufe0f\ud83d\udee0\ufe0f\ud83d\ude80\ud83e\udd16\ud83e\uddbe This template is a PoC of a ReAct AI Agent capable of fetching random pages (not only Wikipedia or Google search results). On the top part there's a manual chat node connected to a LangChain ReAct Agent. The agent has access to a workflow tool for getting page content. The page content extraction starts with converting query parameters into a JSON object. There are 3 pre-defined parameters: url** \u2013 an address of the page to fetch method** = full / simplified maxlimit** - maximum length for the final page. For longer pages an error message is returned back to the agent Page content fetching is a multistep process: An HTTP Request mode tries to get the page content. If the page content was successfuly retrieved, a series of post-processing begin: Extract HTML BODY; content Remove all unnecessary tags to recude the page size Further eliminate external URLs and IMG scr values (based on the method query parameter) Remaining HTML is converted to Markdown, thus recuding the page lengh even more while preserving the basic page structure The remaining content is sent back to an Agent if it's not too long (maxlimit = 70000 by default, see CONFIG node). NB: You can isolate the HTTP Request part into a separate workflow. Check the Workflow Tool description, it guides the agent to provide a query string with several parameters instead of a JSON object. Please reach out to Eduard is you need further assistance with you n8n workflows and automations! Note that to use this template, you need to be on n8n version 1.19.4 or later. Eduard | 1 year ago | Free", "completion": "{\"id\":\"dsKnCFwysROIA4MT\",\"meta\":{\"instanceId\":\"03524270bab2c2dfd5b82778cd1355e56cdda3cf098bf2dfd865e18164c00485\"},\"name\":\"Agent with custom HTTP Request\",\"tags\":[],\"nodes\":[{\"id\":\"e7374976-f3c1-4f60-ae57-9eec65444216\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[763,676],\"parameters\":{},\"typeVersion\":1},{\"id\":\"97e84a23-9536-43cd-94e9-b8166be8ed32\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[983,896],\"parameters\":{\"model\":\"gpt-4-1106-preview\",\"options\":{\"timeout\":300000,\"temperature\":0.7,\"frequencyPenalty\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"wPFAzp4ZHdLLwvkK\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"63d98361-8978-4042-84e7-53a0e226f946\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"onError\":\"continueRegularOutput\",\"position\":[1360,1200],\"parameters\":{\"url\":\"={{ encodeURI($json.query.url) }}\",\"options\":{\"response\":{\"response\":{\"neverError\":true}},\"allowUnauthorizedCerts\":true}},\"typeVersion\":4.1,\"alwaysOutputData\":false},{\"id\":\"17d4b5ae-f5d3-4793-8419-d3c879f7f50d\",\"name\":\"Exctract HTML Body\",\"type\":\"n8n-nodes-base.set\",\"position\":[1780,1480],\"parameters\":{\"fields\":{\"values\":[{\"name\":\"HTML\",\"stringValue\":\"={{ $json?.data.match(/<body[^>]*>([\\\\s\\\\S]*?)<\\\\/body>/i)[1]"}
{"prompt": "Create an n8n workflow for: Scrape and summarize webpages with AI This workflow integrates both web scraping and NLP functionalities. It uses HTML parsing to extract links, HTTP requests to fetch essay content, and AI-based summarization using GPT-4o. It's an excellent example of an end-to-end automated task that is not only efficient but also provides real value by summarizing valuable content. Note that to use this template, you need to be on n8n version 1.50.0 or later. n8n Team | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"67850bd7-f9f4-4d5b-8c9e-bd1451247ba6\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-740,1000],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0d9133f9-b6d3-4101-95c6-3cd24cdb70c3\",\"name\":\"Fetch essay list\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-520,1000],\"parameters\":{\"url\":\"http://www.paulgraham.com/articles.html\",\"options\":{}},\"typeVersion\":4.1},{\"id\":\"ee634297-a456-4f70-a995-55b02950571e\",\"name\":\"Extract essay names\",\"type\":\"n8n-nodes-base.html\",\"position\":[-300,1000],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"dataPropertyName\":\"=data\",\"extractionValues\":{\"values\":[{\"key\":\"essay\",\"attribute\":\"href\",\"cssSelector\":\"table table a\",\"returnArray\":true,\"returnValue\":\"attribute\"}]}},\"typeVersion\":1},{\"id\":\"83d75693-dbb8-44c4-8533-da06f611c59c\",\"name\":\"Fetch essay texts\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[360,1000],\"parameters\":{\"url\":\"=http://www.paulgraham.com/{{ $json.essay }}\",\"options\":{}},\"typeVersion\":4.1},{\"id\":\"151022b5-8570-4176-bf3f-137f27ac7036\",\"name\":\"Extract title\",\"type\":\"n8n-nodes-base.html\",\"position\":[700,700],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"title\",\"cssSelector\":\"title\"}]}},\"typeVersion\":1},{\"id\":\"07bcf095-3c"}
{"prompt": "Create an n8n workflow for: Transform Image to Lego Style Using Line and Dall-E Who is this for? This workflow is designed for: Content creators**, artists, or hobbyists looking to experiment with AI-generated art. Small business owners* or *marketers** using LEGO-style designs for branding or promotions. Developers* or *AI enthusiasts** wanting to automate image transformations through messaging platforms like LINE. What problem is this workflow solving? Simplifies the process of creating custom AI-generated LEGO-style images. Automates the manual effort of transforming user-uploaded images into AI-generated artwork. Bridges the gap between messaging platforms (LINE) and advanced AI tools (DALL\u00b7E). Provides a seamless system for users to upload an image and receive an AI-transformed output without technical expertise. What this workflow does Image Upload via LINE: Users send an image to the LINE chatbot. AI-Powered Prompt Creation: GPT generates a prompt to describe the uploaded image for LEGO-style conversion. AI Image Generation: DALL\u00b7E 3 processes the prompt and creates a LEGO-style isometric image. Image Delivery: The generated image is returned to the user in LINE. Setup Prerequisites LINE Developer Account** with API credentials. Access to OpenAI API with DALL\u00b7E and GPT-4 capabilities. A configured n8n instance to run this workflow. Steps Environment Setup: Add your LINE API Token and OpenAI credentials as environment variables (LINE_API_TOKEN, OPENAI_API_KEY) in n8n. Configure LINE Webhook: Point the LINE webhook to your n8n instance. Connect OpenAI: Set up OpenAI API credentials in the workflow nodes for GPT-4 and DALL\u00b7E. Test Workflow: Upload a sample image in LINE and ensure it returns the LEGO-style AI image. How to customize this workflow to your needs Localization**: Modify response messages in LINE to fit your audience's language and tone. Integration**: Add nodes to send notifications through other platforms like Slack or email. Image Style**: Replace the LEGO-style image prompt with other artistic styles or themes. Advanced Use Cases Art Contests: Users upload images and receive AI-enhanced outputs for community voting or branding. Marketing Campaigns: Quickly generate creative visual content for ads and promotions using customer-submitted photos. Education: Use the workflow to teach students about AI-generated art and automation through a hands-on approach. Tips for Optimization Error Handling**: Add fallback nodes to handle invalid images or API errors gracefully. Logging**: Implement a logging mechanism to track requests and outputs for debugging and analytics. Scalability**: Use queue-based systems or cloud scaling to handle large volumes of image requests. Enhancements Add sticky notes in n8n to provide inline instructions for configuring each node. Create a tutorial video or documentation for first-time users to set up and customize the workflow. Include advanced filters to allow users to select from multiple styles beyond LEGO (e.g., pixel art, watercolor). This workflow enables seamless interaction between messaging platforms and advanced AI capabilities, making it highly versatile for various creative and business applications. Boriwat Chanruang | 1 month ago | Free", "completion": "{\"meta\":{\"instanceId\":\"c59c4acfed171bdc864e7c432be610946898c3ee271693e0303565c953d88c1d\",\"templateCredsSetupCompleted\":true},\"name\":\"Transform Image to Lego Style Using Line and Dall-E\",\"tags\":[],\"nodes\":[{\"id\":\"82b62d4e-a263-4232-9bae-4c581db2269c\",\"name\":\"Receive a Line Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[0,0],\"webhookId\":\"2a27c148-3977-485f-b197-567c96671023\",\"parameters\":{\"path\":\"lineimage\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":2},{\"id\":\"f861c4eb-3d4f-4253-810f-8032602f079b\",\"name\":\"Receive Line Messages\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[220,0],\"parameters\":{\"url\":\"=https://api-data.line.me/v2/bot/message/{{ $json.body.events[0].message.id }}/content\",\"options\":{},\"jsonHeaders\":\"={\\n\\\"Authorization\\\": \\\"Bearer YOUR_LINE_BOT_TOKEN\\\",\\n\\\"Content-Type\\\": \\\"application/json\\\"\\n}\",\"sendHeaders\":true,\"specifyHeaders\":\"json\"},\"typeVersion\":4.2},{\"id\":\"da3a9188-028d-4c75-b23f-5f1f4e50784c\",\"name\":\"Creating an Image using Dall-E\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[860,0],\"parameters\":{\"prompt\":\"={{ $json.content }}\",\"options\":{\"returnImageUrls\":true},\"resource\":\"image\"},\"credentials\":{\"openAiApi\":{\"id\":\"YOUR_OPENAI_CREDENTIAL_ID\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.7},{\"id\":\"36c826e5-eacd-43ad-b663-4d788005e61a\",\"name\":\"Creating a Prompt for Dall-E (Leg"}
{"prompt": "Create an n8n workflow for: LINE Assistant with Google Calendar and Gmail Integration Who is this for? This workflow is for small business owners, personal assistants, or project managers who rely on multiple platforms for communication and scheduling. Ideal for users managing customer support, personal scheduling, or group event coordination via LINE, Google Calendar, and Gmail. What problem is this workflow solving? Reduces the manual effort needed to manage conversations, schedule events, and handle email communications. Provides an intelligent system for replying to user messages and fetching relevant calendar or email information in real time. Bridges the gap between messaging platforms and productivity tools, improving efficiency. What this workflow does LINE Chatbot Automation**: Automatically processes and responds to messages received via LINE. Google Calendar Management**: Retrieves upcoming events or schedules new events dynamically based on user queries. Email Retrieval**: Fetches recent emails using Gmail and filters them based on user instructions. AI-Powered Replies**: Uses OpenAI GPT to interpret user queries and provide tailored responses. Setup Prerequisites: LINE Developer account and API access. Google Calendar and Gmail accounts with OAuth credentials. An n8n instance with access to environment variables. Steps: Set up environment variables (LINE_API_TOKEN and DYNAMIC_EMAIL). Configure API credentials for Google Calendar and Gmail in n8n. Test the workflow by sending a sample message via LINE. Enhancements: Use sticky notes to provide inline instructions for each node. Include a video walkthrough or a step-by-step document for first-time users. How to customize this workflow to your needs Localization**: Modify responses in the AI Agent node to match the language and tone of your audience. Integration**: Add more integrations like Slack or Microsoft Teams for additional notifications. Advanced Filters**: Add specific conditions to Gmail or Google Calendar nodes to fetch only relevant data, such as events with specific keywords or emails from certain senders. Advanced Use Cases Customer Support**: Automatically schedule meetings with clients based on their messages in LINE. Event Management**: Handle RSVP confirmations, event reminders, and email follow-ups for planned events. Personalized Assistant**: Use the workflow to act as a personal virtual assistant that syncs your schedule, replies to messages, and summarizes emails. Tips for Optimization Edit Fields Node**: Add a centralized node to configure dynamic inputs (e.g., tokens, emails, or thresholds) for easy updates. Fallback Responses**: Use a switch node to handle unrecognized input gracefully and provide clear feedback to users. Logs and Monitoring**: Add nodes to log interactions and track message flows for debugging or analytics. Let me know if you'd like me to expand on any specific section or add more customization ideas! Boriwat Chanruang | 2 months ago | Free", "completion": "{\"id\":\"Z5OgwYfK4reCTv9y\",\"meta\":{\"instanceId\":\"c59c4acfed171bdc864e7c432be610946898c3ee271693e0303565c953d88c1d\"},\"name\":\"LINE Assistant with Google Calendar and Gmail Integration\",\"tags\":[],\"nodes\":[{\"id\":\"9e1e1c11-f406-47de-8f65-9669cf078d3d\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[-1140,120],\"parameters\":{\"text\":\"={{ $json.body.events[0].message.text }}\",\"options\":{\"systemMessage\":\"=You are a helpful assistant.\\n\\nHere is the current date {{ $now }}\"},\"promptType\":\"define\"},\"typeVersion\":1.7},{\"id\":\"fa722820-8804-47da-bb21-02c0d2b5d665\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[-1020,580],\"parameters\":{\"sessionKey\":\"={{ $json.body.events[0].source.userId }}\",\"sessionIdType\":\"customKey\"},\"typeVersion\":1.3},{\"id\":\"5149b91a-5934-4037-a444-dfdb93d0cd16\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-1180,580],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"211a120d-d65f-4708-adc2-66dc8f4a40d6\",\"name\":\"Wikipedia\",\"type\":\"@n8n/n8n-nodes-langchain.toolWikipedia\",\"position\":[-360,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0e03137d-0300-47a4-bbd8-03c87c93d6e2\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[-780,120],\"parameters\":{\"modelId\":{\"__rl"}
{"prompt": "Create an n8n workflow for: Obsidian Notes Read Aloud using AI: Available as a Podcast Feed How it works: Send notes from Obsidian via Webhook to start the audio conversion OpenAI converts your text to natural-sounding audio and generates episode descriptions Audio files are stored in Cloudinary and automatically attached to your notes in Obsidian A professional podcast feed is generated, compatible with all major podcast platforms (Apple, Spotify, Google) Set up steps: Install and configure the Post Webhook Plugin in Obsidian Set up Custom Auth credentials in n8n for Cloudinary using the following JSON: { \"name\": \"Cloudinary API\", \"type\": \"httpHeaderAuth\", \"authParameter\": { \"type\": \"header\", \"key\": \"Authorization\", \"value\": \"Basic {{Buffer.from('your_api_key:your_api_secret').toString('base64')}}\" } } Configure podcast feed metadata (title, author, cover image, etc.) Note: The second flow is a generic Podcast Feed module that can be reused in any '[...]-to-Podcast' workflow. It generates a standard RSS feed from Google Sheets data and podcast metadata, making it compatible with all major podcast platforms. Obsidi8n | 1 month ago | Free", "completion": "{\"id\":\"f9X48gqgIUwyseMM\",\"meta\":{\"instanceId\":\"d47f3738b860eed937a1b18d7345fa2c65cf4b4957554e29477cb064a7039870\"},\"name\":\"Obsidian Notes Read Aloud: Available as a Podcast Feed\",\"tags\":[],\"nodes\":[{\"id\":\"a44b5cb3-6c9f-4227-a45f-a21765ea120c\",\"name\":\"OpenAI1\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[-660,-180],\"parameters\":{\"input\":\"={{ $json.body.content }}\",\"options\":{\"response_format\":\"mp3\"},\"resource\":\"audio\"},\"credentials\":{\"openAiApi\":{\"id\":\"q8L9oWVM7QyzYEE5\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.7},{\"id\":\"9ca589b6-f1c7-44a9-8ff7-4abb979a71c3\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1200,-400],\"parameters\":{\"width\":440,\"height\":540,\"content\":\"## Send Notes to Webhook\\n**Setup:**\\n- Install [Post Webhook Plugin](https://github.com/Masterb1234/obsidian-post-webhook/) in Obsidian\\n- Enter n8n Webhook URL and name in plugin settings\\n\\n**Usage:**\\n- Select text or use full note\\n- Open Command Palette (Ctrl+P)\\n- Choose 'Send Note/Selection to [name]'\\n- Audio file appears in Podcast Feed and note\"},\"typeVersion\":1},{\"id\":\"3ea132e5-8c67-4140-a9b2-607ea256e90f\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1200,240],\"parameters\":{\"width\":440,\"height\":440,\"content\":\"## Generic Podcast Feed Module\\nA reusable module for any 'X-to-Podcast' workflow. Generates standard RSS feed from:\\n- Source data (Google Sheets)\\n- Podcast metadata\\n"}
{"prompt": "Create an n8n workflow for: Screen Applicants With AI, notify HR and save them in a Google Sheet What this workflow does This workflow helps HR teams screen CVs with AI, store compatibility ratings in Google Sheets, and send email notifications to candidates and HR. It simplifies the recruitment process. CV Submission Form: Candidates submit their details and CV (PDF) through a web form, triggering the workflow in n8n. PDF Extraction & AI Rating: The submitted CV is processed to extract text, and AI analyzes it to generate a compatibility rating. Results Storage & Notifications: Ratings are stored in a Google Sheet for easy access and organization. Confirmation emails are automatically sent to both HR and the candidate. Setup Use the provided template to configure your form and connect it to n8n. Ensure your Google Sheets and email service integrations are active. Customization Instructions: Modify the email template to match your organization\u2019s branding. Adjust the AI compatibility rating thresholds based on your requirements. Ensure you have updated the prompt for cv screening. Sarfaraz Muhammad Sajib | 2 months ago | Free", "completion": "{\"id\":\"ES4TSw9HacxoNhLZ\",\"meta\":{\"instanceId\":\"5219bc76ea806909b58e13e2acac1c19192522e70dc3c90467e1800e94864105\",\"templateCredsSetupCompleted\":true},\"name\":\"AI CV Screening Workflow\",\"tags\":[],\"nodes\":[{\"id\":\"e77fbc32-5ee9-49b4-93d5-f2ffda134b08\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[1230,530],\"parameters\":{\"options\":{}},\"credentials\":{\"googlePalmApi\":{\"id\":\"UcdfdADI6w9nkgg5\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"9e24167f-cac6-4b98-95da-30065510d79a\",\"name\":\"Confirmation of CV Submission\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[1780,460],\"webhookId\":\"954756dc-2946-4b78-b208-06f3df612ab5\",\"parameters\":{\"sendTo\":\"={{ $('Application Form').item.json['E-mail'] }}\",\"message\":\"=Dear {{ $('Application Form').item.json['Full Name'] }}, \\n\\nThank you for submitting your CV. We have received it and will review it shortly. \\n\\nBest regards,\\nMediusware\",\"options\":{},\"subject\":\"We Have Received Your CV\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"taFlf0vD5S4QlOKM\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"ff49d370-b4eb-4426-b396-763455e647e7\",\"name\":\"Inform HR New CV Received\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[1760,200],\"webhookId\":\"e969a9f5-631b-4719-a4f6-87e6063cef6a\",\"parameters\":{\"sendTo\":\"sarfaraz@mediusaware.com"}
{"prompt": "Create an n8n workflow for: Deduplicate Scraping AI Grants for Eligibility using AI This n8n template scrapes a list of AI grants from grants.gov and qualifies them using AI; determining interest and eligibility for the business. It then sends an email alert of interesting items to team members in an email. The template also shows how you can use the \"Remove Duplicates\" node to simplify deduplication of external listings without the need to manage this yourself. Not particularly interested in AI Grants? This template works for other tender websites as long as you're able to scrape them. How it works A scheduled trigger is set to fetch a list of AI grants listed on the grants.gov website in the past day. A Remove Duplicates node is used to track Grant IDs to filter out those already processed by the workflow. New grants are summarized and analysed by AI nodes to determine eligibility and interest which is then saved to an Airtable database. Another scheduled trigger starts a little later than the first to collect and summarize the new grants The results are then compiled into an email template using the HTML node, in the form of a newsletter designed to alert and brief team members of new AI grants. This email is then sent to a list of subscribers using the gmail node. How to use Make a copy of sample Airtable here: https://airtable.com/appiNoPRvhJxz9crl/shrRdP6zstgsxjDKL The filters for fetching the grants is currently set to the \"AI\" category. Feel free to change this to include more categories. Not interested in grants, this template can works for other sources of leads just change the endpoint and how you're defining the item ID to track. Requirements Airtable for database OpenAI for LLM Note: These are not hard requirements and can be exchanged for services available to you. customising the workflow \"Eligibility\" criteria at this stage may be better served by identifying hard blockers instead ie. certifications, geographical considerations or certain legal checks. Be sure to mention any hard blockers into the Eligibility prompt. Not particularly interested in AI prompts? This template works for other tender websites as long as you're able to scrape them. Jimleuk | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"c17e444e-0a5e-4bfe-8de6-c3185de4465d\",\"name\":\"Grants to List\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[-240,-180],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"oppHits\"},\"typeVersion\":1},{\"id\":\"9251d39c-6098-42fa-aadd-3a22464dee64\",\"name\":\"Get Grant Details\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[280,-280],\"parameters\":{\"url\":\"https://apply07.grants.gov/grantsws/rest/opportunity/details\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"contentType\":\"form-urlencoded\",\"bodyParameters\":{\"parameters\":[{\"name\":\"oppId\",\"value\":\"={{ $json.id }}\"}]}},\"typeVersion\":4.2},{\"id\":\"ade994d6-a1f8-45bf-a82e-83eb38da08d6\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[440,-120],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"4d81b20e-0038-48d3-840c-3fcf8b798a0d\",\"name\":\"Summarize Synopsis\",\"type\":\"@n8n/n8n-nodes-langchain.informationExtractor\",\"position\":[460,-280],\"parameters\":{\"text\":\"=Agency: {{ $json.synopsis.agencyName }}\\nTitle: {{ $json.opportunityTitle }}\\nSynopsis: {{ $json.synopsis.synopsisDesc }}\",\"options\":{\"systemPromptTemplate\":\"You've been given a grant opportunity listing. Help summarize the opportunity in simple terms.\"},\"schemaType\":\"manual\",\"inputSchema\":\"{\\n\\t\\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n \\\"goal\\\": { \\\"type\\\": [\\\"string\\\", \\\"null\\\"] },\\n \\\"duration\\\": { \\\"type"}
{"prompt": "Create an n8n workflow for: Automate Sales Meeting Prep with AI & APIFY Sent To WhatsApp This n8n template builds a meeting assistant that compiles timely reminders of upcoming meetings filled with email history and recent LinkedIn activity of other people on the invite. This is then discreetly sent via WhatsApp ensuring the user is always prepared, informed and ready to impress! How it works A scheduled trigger fires hourly to check for upcoming personal meetings. When found, the invite is analysed by an AI agent to pull email and LinkedIn details of the other invitees. 2 subworkflows are then triggered for each invitee to (1) search for last email correspondence with them and (2) scrape their LinkedIn profile + recent activity for social updates. Using both available sources, another AI agent is used to summarise this information and generate a short meeting prep message for the user. The notification is finally sent to the user's WhatsApp, allowing them ample time to review. How to use There are a lot of moving parts in this template so in it's current form, it's best to use this for personal rather than team calendars. The LinkedIn scraping method used in this workflow requires you to paste in your LinkedIn cookies from your browser which essentially let's n8n impersonate you. You can retrieve this from dev console or ask someone technical for help! Note: It may be wise to switch to other LinkedIn scraping approaches which do not impersonate your own account for production. Requirements OpenAI for LLM Gmail for Email Google Calendar for upcoming events WhatsApp Business account for notifications Customising this workflow Try adding information sources which are relevant to you and your invitees. Such as company search, other social media sites etc. Create an on-demand version which doesn't rely on the scheduled trigger. Sometimes you want to know prepare for meetings hours or days in advance where this could help immensely. Jimleuk | 3 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"201ef455-2d65-4563-8ec1-318211b1fa6a\",\"name\":\"Get Message Contents\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[2080,500],\"webhookId\":\"fa1d496f-17fa-4e50-bae9-84ca85ed4502\",\"parameters\":{\"simple\":false,\"options\":{},\"messageId\":\"={{ $json.id }}\",\"operation\":\"get\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"Sf5Gfl9NiFTNXFWb\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"ded010af-e977-4c47-87dd-8221d601af74\",\"name\":\"Simplify Emails\",\"type\":\"n8n-nodes-base.set\",\"position\":[2240,500],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"2006c806-42db-4457-84c2-35f59ed39018\",\"name\":\"date\",\"type\":\"string\",\"value\":\"={{ $json.date }}\"},{\"id\":\"872278d2-b97c-45ba-a9d3-162f154fe7dc\",\"name\":\"subject\",\"type\":\"string\",\"value\":\"={{ $json.subject }}\"},{\"id\":\"282f03e9-1d0f-4a17-b9ed-75b44171d4ee\",\"name\":\"text\",\"type\":\"string\",\"value\":\"={{ $json.text }}\"},{\"id\":\"9421776c-ff53-4490-b0e1-1e610534ba25\",\"name\":\"from\",\"type\":\"string\",\"value\":\"={{ $json.from.value[0].name }} ({{ $json.from.value[0].address }})\"},{\"id\":\"3b6716e8-5582-4da3-ae9d-e8dd1afad530\",\"name\":\"to\",\"type\":\"string\",\"value\":\"={{ $json.to.value[0].name }} ({{ $json.to"}
{"prompt": "Create an n8n workflow for: Handling Job Application Submissions with AI and n8n Forms This n8n template leverages n8n's multi-form feature to build a 2 part job application submission journey which aims to eliminate the need for applicants to re-enter data found on their CVs/Resumes. How it works The application submission process starts with an n8n form trigger to accept CV files in the form of PDFs. The PDF is validated using the text classifier node to determine if it is a valid CV else the applicant is asked to reupload. A basic LLM node is used to extract relevant information from the CV as data capture. A copy of the original job post is included to ensure relevancy. Applicant's data is then sent to an ATS for processing. For our demo, we used airtable because we could attach PDFs to rows. Finally, a second form trigger is used for the actual application form. However, it is prefilled to save the applicant's time and allow them to amend any of the generated application fields. How to use Ensure to change the redirect URL in the form ending node to use the host domain of your n8n instance. Requirements OpenAI for LLM Airtable to capture applicant data Customising the workflow Application form is pretty basic for this demonstration but could be extended to ask more in-depth questions. If it fits the job, why not ask applicants to upload portfolio works and have AI describe/caption them. Jimleuk | 3 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"10565888-4a1b-439a-a188-c6ee7990bb63\",\"name\":\"Extract from File\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[860,260],\"parameters\":{\"options\":{},\"operation\":\"pdf\",\"binaryPropertyName\":\"File_Upload\"},\"typeVersion\":1},{\"id\":\"583aff4b-d9f5-44e7-8e91-4938592b5630\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1740,380],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"3a09afd0-0dce-41fd-bec3-783fcb3d01fc\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1920,380],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"Name\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Address\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Email\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Telephone\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Education\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Skills & Technologies\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Years of Experience\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Cover Letter\\\": { \\\"type\\\": \\\"string\\\" }\\n }\\n}\"},\"typeVersion\":1.2},{\"id\":\"541a00d0-1635-48ad-b69e-83b28e178d6e\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\""}
{"prompt": "Create an n8n workflow for: WordPress - AI Chatbot to enhance user experience - with Supabase and OpenAI This is the first version of a template for a RAG/GenAI App using WordPress content. As creating, sharing, and improving templates brings me joy \ud83d\ude04, feel free to reach out on LinkedIn if you have any ideas to enhance this template! How It Works This template includes three workflows: Workflow 1**: Generate embeddings for your WordPress posts and pages, then store them in the Supabase vector store. Workflow 2**: Handle upserts for WordPress content when edits are made. Workflow 3**: Enable chat functionality by performing Retrieval-Augmented Generation (RAG) on the embedded documents. Why use this template? This template can be applied to various use cases: Build a GenAI application that requires embedded documents from your website's content. Embed or create a chatbot page on your website to enhance user experience as visitors search for information. Gain insights into the types of questions visitors are asking on your website. Simplify content management by asking the AI for related content ideas or checking if similar content already exists. Useful for internal linking. Prerequisites Access to Supabase for storing embeddings. Basic knowledge of Postgres and pgvector. A WordPress website with content to be embedded. An OpenAI API key Ensure that your n8n workflow, Supabase instance, and WordPress website are set to the same timezone (or use GMT) for consistency. Workflow 1 : Initial Embedding This workflow retrieves your WordPress pages and posts, generates embeddings from the content, and stores them in Supabase using pgvector. Step 0 : Create Supabase tables Nodes : Postgres - Create Documents Table: This table is structured to support OpenAI embedding models with 1536 dimensions Postgres - Create Workflow Execution History Table These two nodes create tables in Supabase: The documents table, which stores embeddings of your website content. The n8n_website_embedding_histories table, which logs workflow executions for efficient management of upserts. This table tracks the workflow execution ID and execution timestamp. Step 1 : Retrieve and Merge WordPress Pages and Posts Nodes : WordPress - Get All Posts WordPress - Get All Pages Merge WordPress Posts and Pages These three nodes retrieve all content and metadata from your posts and pages and merge them. Important: ** **Apply filters to avoid generating embeddings for all site content. Step 2 : Set Fields, Apply Filter, and Transform HTML to Markdown Nodes : Set Fields Filter - Only Published & Unprotected Content HTML to Markdown These three nodes prepare the content for embedding by: Setting up the necessary fields for content embeddings and document metadata. Filtering to include only published and unprotected content (protected=false), ensuring private or unpublished content is excluded from your GenAI application. Converting HTML to Markdown, which enhances performance and relevance in Retrieval-Augmented Generation (RAG) by optimizing document embeddings. Step 3: Generate Embeddings, Store Documents in Supabase, and Log Workflow Execution Nodes: Supabase Vector Store Sub-nodes: Embeddings OpenAI Default Data Loader Token Splitter Aggregate Supabase - Store Workflow Execution This step involves generating embeddings for the content and storing it in Supabase, followed by logging the workflow execution details. Generate Embeddings: The Embeddings OpenAI node generates vector embeddings for the content. Load Data: The Default Data Loader prepares the content for embedding storage. The metadata stored includes the content title, publication date, modification date, URL, and ID, which is essential for managing upserts. \u26a0\ufe0f Important Note : Be cautious not to store any sensitive information in metadata fields, as this information will be accessible to the AI and may appear in user-facing answers. Token Management: The Token Splitter ensures that content is segmented into manageable sizes to comply with token limits. Aggregate: Ensure the last node is run only for 1 item. Store Execution Details: The Supabase - Store Workflow Execution node saves the workflow execution ID and timestamp, enabling tracking of when each content update was processed. This setup ensures that content embeddings are stored in Supabase for use in downstream applications, while workflow execution details are logged for consistency and version tracking. This workflow should be executed only once for the initial embedding. Workflow 2, described below, will handle all future upserts, ensuring that new or updated content is embedded as needed. Workflow 2: Handle document upserts Content on a website follows a lifecycle\u2014it may be updated, new content might be added, or, at times, content may be deleted. In this first version of the template, the upsert workflow manages: Newly added content** Updated content** Step 1: Retrieve WordPress Content with Regular CRON Nodes: CRON - Every 30 Seconds Postgres - Get Last Workflow Execution WordPress - Get Posts Modified After Last Workflow Execution WordPress - Get Pages Modified After Last Workflow Execution Merge Retrieved WordPress Posts and Pages A CRON job (set to run every 30 seconds in this template, but you can adjust it as needed) initiates the workflow. A Postgres SQL query on the n8n_website_embedding_histories table retrieves the timestamp of the latest workflow execution. Next, the HTTP nodes use the WordPress API (update the example URL in the template with your own website\u2019s URL and add your WordPress credentials) to request all posts and pages modified after the last workflow execution date. This process captures both newly added and recently updated content. The retrieved content is then merged for further processing. Step 2 : Set fields, use filter Nodes : Set fields2 Filter - Only published and unprotected content The same that Step 2 in Workflow 1, except that HTML To Makrdown is used in further Step. Step 3: Loop Over Items to Identify and Route Updated vs. Newly Added Content Here, I initially aimed to use 'update documents' instead of the delete + insert approach, but encountered challenges, especially with updating both content and metadata columns together. Any help or suggestions are welcome! :) Nodes: Loop Over Items Postgres - Filter on Existing Documents Switch Route existing_documents (if documents with matching IDs are found in metadata): Supabase - Delete Row if Document Exists: Removes any existing entry for the document, preparing for an update. Aggregate2: Used to aggregate documents on Supabase with ID to ensure that Set Fields3 is executed only once for each WordPress content to avoid duplicate execution. Set Fields3: Sets fields required for embedding updates. Route new_documents (if no matching documents are found with IDs in metadata): Set Fields4: Configures fields for embedding newly added content. In this step, a loop processes each item, directing it based on whether the document already exists. The Aggregate2 node acts as a control to ensure Set Fields3 runs only once per WordPress content, effectively avoiding duplicate execution and optimizing the update process. Step 4 : HTML to Markdown, Supabase Vector Store, Update Workflow Execution Table The HTML to Markdown node mirrors Workflow 1 - Step 2. Refer to that section for a detailed explanation on how HTML content is converted to Markdown for improved embedding performance and relevance. Following this, the content is stored in the Supabase vector store to manage embeddings efficiently. Lastly, the workflow execution table is updated. These nodes mirros the **Workflow 1 - Step 3 nodes. Workflow 3 : An example of GenAI App with Wordpress Content : Chatbot to be embed on your website Step 1: Retrieve Supabase Documents, Aggregate, and Set Fields After a Chat Input Nodes: When Chat Message Received Supabase - Retrieve Documents from Chat Input Embeddings OpenAI1 Aggregate Documents Set Fields When a user sends a message to the chat, the prompt (user question) is sent to the Supabase vector store retriever. The RPC function match_documents (created in Workflow 1 - Step 0) retrieves documents relevant to the user\u2019s question, enabling a more accurate and relevant response. In this step: The Supabase vector store retriever fetches documents that match the user\u2019s question, including metadata. The Aggregate Documents node consolidates the retrieved data. Finally, Set Fields organizes the data to create a more readable input for the AI agent. Directly using the AI agent without these nodes would prevent metadata from being sent to the language model (LLM), but metadata is essential for enhancing the context and accuracy of the AI\u2019s response. By including metadata, the AI\u2019s answers can reference relevant document details, making the interaction more informative. Step 2: Call AI Agent, Respond to User, and Store Chat Conversation History Nodes: AI Agent** Sub-nodes: OpenAI Chat Model Postgres Chat Memories Respond to Webhook** This step involves calling the AI agent to generate an answer, responding to the user, and storing the conversation history. The model used is gpt4-o-mini, chosen for its cost-efficiency. Dataki | 4 months ago | Free +6", "completion": "{\"id\":\"o8iTqIh2sVvnuWz5\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\"},\"name\":\"RAG & GenAI App With WordPress Content\",\"tags\":[],\"nodes\":[{\"id\":\"c3738490-ed39-4774-b337-bf5ee99d0c72\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[500,940],\"parameters\":{},\"typeVersion\":1},{\"id\":\"3ab719bd-3652-433f-a597-9cd28f8cfcea\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[2580,1320],\"parameters\":{\"model\":\"text-embedding-3-small\",\"options\":{}},\"typeVersion\":1},{\"id\":\"e8639569-2091-44de-a84d-c3fc3ce54de4\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[2800,1260],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"title\",\"value\":\"={{ $json.title }}\"},{\"name\":\"url\",\"value\":\"={{ $json.url }}\"},{\"name\":\"content_type\",\"value\":\"={{ $json.content_type }}\"},{\"name\":\"publication_date\",\"value\":\"={{ $json.publication_date }}\"},{\"name\":\"modification_date\",\"value\":\"={{ $json.modification_date }}\"},{\"name\":\"id\",\"value\":\"={{ $json.id }}\"}]}},\"jsonData\":\"={{ $json.data }}\",\"jsonMode\":\"expressionData\"},\"typeVersion\":1},{\"id\":\"e7f858eb-4dca-40ea-9da9-af953687e63d\",\"name\":\"Token Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter\",\"position\":[2900,1480],\"parameters\":{\"chunkSize\":"}
{"prompt": "Create an n8n workflow for: Telegram Bot with Supabase memory and OpenAI assistant integration Video Guide I prepared a detailed guide that showed the whole process of building an AI bot, from the simplest version to the most complex in a template. .png) Who is this for? This workflow is ideal for developers, chatbot enthusiasts, and businesses looking to build a dynamic Telegram bot with memory capabilities. The bot leverages OpenAI's assistant to interact with users and stores user data in Supabase for personalized conversations. What problem does this workflow solve? Many simple chatbots lack context awareness and user memory. This workflow solves that by integrating Supabase to keep track of user sessions (via What this workflow does This Telegram bot template connects with OpenAI to answer user queries while storing and retrieving user information from a Supabase database. The memory component ensures that the bot can reference past interactions, making it suitable for use cases such as customer support, virtual assistants, or any application where context retention is crucial. 1.Receive New Message: The bot listens for incoming messages from users in Telegram. Check User in Database: The workflow checks if the user is already in the Supabase database using the Create New User (if necessary): If the user does not exist, a new record is created in Supabase with the telegram_id and a unique Start or Continue Conversation with OpenAI: Based on the user\u2019s context, the bot either creates a new thread or continues an existing one using the stored Merge Data: User-specific data and conversation context are merged. Send and Receive Messages: The message is sent to OpenAI, and the response is received and processed. Reply to User: The bot sends OpenAI\u2019s response back to the user in Telegram. Setup Create a Telegram Bot using the Botfather and obtain the bot token. Set up Supabase: Create a new project and generate a Create a new table named create table public.telegram_users ( id uuid not null default gen_random_uuid (), date_created timestamp with time zone not null default (now() at time zone 'utc'::text), telegram_id bigint null, openai_thread_id text null, constraint telegram_users_pkey primary key (id) ) tablespace pg_default; OpenAI Setup: Create an OpenAI assistant and obtain the Customize your assistant\u2019s personality or use cases according to your requirements. Environment Configuration in n8n: Configure the Telegram, Supabase, and OpenAI nodes with the appropriate credentials. Set up triggers for receiving messages and handling conversation logic. Set up OpenAI assistant ID in \"++OPENAI - Run assistant++\" node. Mark Shcherbakov | 4 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"9cc26a42-eb43-40c4-b507-cbaf187a5e15\",\"name\":\"Get New Message\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[1120,500],\"webhookId\":\"464f0a75-56d1-402f-8b12-b358452e9736\",\"parameters\":{\"updates\":[\"message\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"rI0zyfIYVIyXt2fL\",\"name\":\"Telegram Club\"}},\"typeVersion\":1.1},{\"id\":\"098b6fcf-7cb6-4730-8892-949fedc946b3\",\"name\":\"OPENAI - Create thread\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1740,640],\"parameters\":{\"url\":\"https://api.openai.com/v1/threads\",\"method\":\"POST\",\"options\":{},\"sendHeaders\":true,\"authentication\":\"predefinedCredentialType\",\"headerParameters\":{\"parameters\":[{\"name\":\"OpenAI-Beta\",\"value\":\"assistants=v2\"}]},\"nodeCredentialType\":\"openAiApi\"},\"credentials\":{\"openAiApi\":{\"id\":\"zJhr5piyEwVnWtaI\",\"name\":\"OpenAi club\"}},\"typeVersion\":4.2},{\"id\":\"fa157f8c-b776-4b20-bfaf-c17460383505\",\"name\":\"Create User\",\"type\":\"n8n-nodes-base.supabase\",\"position\":[1900,640],\"parameters\":{\"tableId\":\"telegram_users\",\"fieldsUi\":{\"fieldValues\":[{\"fieldId\":\"telegram_id\",\"fieldValue\":\"={{ $('Get New Message').item.json.message.chat.id }}\"},{\"fieldId\":\"openai_thread_id\",\"fieldValue\":\"={{ $('OPENAI - Create thread').item.json.id }}\"}]}},\"credentials\":{\"supabaseApi\":{\"id\":\"QBhcokohbJHfQZ9A\",\"name\":\"Supabase club\"}},\"typeVersion\":1},{\"id\":\"115e417f-5962-409b-8adf-ff236eb9ce2e"}
{"prompt": "Create an n8n workflow for: Daily Podcast Summary What this workflow does Downloads the daily top podcasts of a selected genre Summarizes the content of each podcast in a few paragraphs Sends the summaries and the direct link to each podcast in a formatted email Setup Create a free API key on Taddy here: https://taddy.org/signup/developers Input your user number and API key into the TaddyTopDaily node in the header parameters X-USER-ID and X-API-KEY respectively. Create access credentials for your Gmail as described here: https://developers.google.com/workspace/guides/create-credentials. Use the credentials from your client_secret.json in the Gmail node. In the Genre node, set the genre of podcasts you want a summary for. Valid values are: TECHNOLOGY, NEWS, ARTS, COMEDY, SPORTS, FICTION, etc. Look at api.taddy.org for the full list (they will be displayed in the help docs as PODCASTSERIES_TECHNOLOGY, PODCASTSERIES_NEWS, etc.) Enter your email address in the Gmail node. Change the schedule time for sending email from Schedule to whichever time you want to receive the email. Test: Hit Test Workflow. Check your email for the results. That's it! It should take less than 5 minutes total. Jay Hartley | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"7858a8e25b8fc4dae485c1ef345e6fe74effb1f5060433ef500b4c186c965c18\"},\"nodes\":[{\"id\":\"49ab7596-665e-4a0f-bb8b-9dc04525ce88\",\"name\":\"Gmail\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[2340,1440],\"parameters\":{\"message\":\"={{ $json.html }}\",\"options\":{},\"subject\":\"Podcast Review\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"1MUdv1HbrQUFABiZ\",\"name\":\"Gmail account\"}},\"typeVersion\":2.1},{\"id\":\"40aa23f4-69d6-46e5-84a2-b46a64a3f0af\",\"name\":\"TaddyTopDaily\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1620,820],\"parameters\":{\"url\":\"https://api.taddy.org/\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"sendHeaders\":true,\"bodyParameters\":{\"parameters\":[{\"name\":\"query\",\"value\":\"=query { getTopChartsByGenres( limitPerPage:10, filterByCountry:UNITED_STATES_OF_AMERICA, taddyType:PODCASTEPISODE, genres:PODCASTSERIES_{{ $json.genre }}){ topChartsId podcastEpisodes{ uuid name audioUrl podcastSeries{ uuid name } } } }\"}]},\"headerParameters\":{\"parameters\":[{\"name\":\"X-USER-ID\"},{\"name\":\"X-API-KEY\"}]}},\"typeVersion\":4.2},{\"id\":\"42eea23b-b09c-49ee-af5b-12abb3960390\",\"name\":\"Genre\",\"type\":\"n8n-nodes-base.set\",\"position\":[1420,820],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"e995cd5b-b91c-4a9d-8215-44d7dfe3f52f\",\"name\":\"genre\",\"type\":\"string\",\"value\":\"TECHNOLOGY\"}]}},\"typeVersion\":3.4},{\"id\":\"da256fbf-ed7"}
{"prompt": "Create an n8n workflow for: Automate Image Validation Tasks using AI Vision This n8n workflow shows how using multimodal LLMs with AI vision can tackle tricky image validation tasks which are near impossible to achieve with code and often impractical to be done by humans at scale. You may need image validation when users submitted photos or images are required to meet certain criteria before being accepted. A wine review website may require users only submit photos of wine with labels, a bank may require account holders to submit scanned documents for verification etc. In this demonstration, our scenario will be to analyse a set of portraits to verify if they meet the criteria for valid passport photos according to the UK government website (https://www.gov.uk/photos-for-passports). How it works Our set of portaits are jpg files downloaded from our Google Drive using the Google Drive node. Each image is resized using the Edit Image node to ensure a balance between resolution and processing speed. Using the Basic LLM node, we'll define a \"user message\" option with the type of binary (data). This will allow us to pass our portrait to the LLM as an input. With our prompt containing the criteria pulled off the passport photo requirements webpage, the LLM is able to validate the photo does or doesn't meet its criteria. A structured output parser is used to structure the LLM's response to a JSON object which has the \"is_valid\" boolean property. This can be useful to further extend the workflow. Requirements Google Gemini API key Google Drive account Customising this workflow Not using Gemini? n8n's LLM node works with any compatible multimodal LLM so feel free to swap Gemini out for OpenAI's GPT4o or Antrophic's Claude Sonnet. Don't need to validate portraits? Try other use cases such as document classification, security footage analysis, people tagging in photos and more. Jimleuk | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"6c78b4c7-993b-410d-93e7-e11b3052e53b\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,420],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c2ab6497-6d6d-483b-bd43-494ae95394c0\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1440,600],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n\\t\\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n\\t\\t\\\"is_valid\\\": { \\\"type\\\": \\\"boolean\\\" },\\n \\\"photo_description\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"describe the appearance of the person(s), object(s) if any and the background in the image. Mention any colours of each if possible.\\\"\\n },\\n\\t\\t\\\"reasons\\\": {\\n \\\"type\\\": \\\"array\\\",\\n \\\"items\\\": { \\\"type\\\": \\\"string\\\" }\\n }\\n\\t}\\n}\"},\"typeVersion\":1.2},{\"id\":\"b23f5298-17c7-49ac-a8ca-78e006b2d294\",\"name\":\"Photo URLs\",\"type\":\"n8n-nodes-base.set\",\"position\":[360,380],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"6baa3e08-8957-454e-8ee9-d5414a0ff990\",\"name\":\"data\",\"type\":\"array\",\"value\":\"={{\\n[\\n{\\n \\\"name\\\": \\\"portrait_1\\\",\\n \\\"url\\\": \\\"https://drive.google.com/file/d/1zs963iFkO-3g2rKak8Hcy555h55D8gjF/view?usp=sharing\\\""}
{"prompt": "Create an n8n workflow for: Supabase Insertion & Upsertion & Retrieval This is a demo workflow to showcase how to use Supabase to embed a document, retrieve information from the vector store via chat and update the database. Setup steps: set your credentials for Supabase set your credentials for an AI model of your choice set credentials for any service you want to use to upload documents please follow the guidelines in the workflow itself (Sticky Notes) Feedback & Questions If you have any questions or feedback about this workflow - Feel free to get in touch at ria@n8n.io Ria | 6 months ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"1a23006df50de49624f69e85993be557d137b6efe723a867a7d68a84e0b32704\"},\"nodes\":[{\"id\":\"54065cc9-047c-4741-95f6-cec3e352abd7\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[2700,-1840],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"https://drive.google.com/file/d/xxxxxxxxxxxxxxx/view\"},\"options\":{},\"operation\":\"download\"},\"typeVersion\":3},{\"id\":\"62af57f5-a001-4174-bece-260a1fc595e8\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[3120,-1620],\"parameters\":{\"loader\":\"epubLoader\",\"options\":{},\"dataType\":\"binary\"},\"typeVersion\":1},{\"id\":\"ce3d9c7c-6ce9-421a-b4d0-4235217cf8e6\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[2620,-2000],\"parameters\":{\"width\":749.1276349295781,\"height\":820.5109034066329,\"content\":\"# INSERTING\\n\\n- it's important to use the same embedding model when for any interaction with your vector database (inserting, upserting and retrieval)\"},\"typeVersion\":1},{\"id\":\"81cb3d3e-70af-46c8-bc18-3d076a222d0b\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1720,-1160],\"parameters\":{\"color\":3,\"width\":873.9739981925188,\"height\":534.0012007720542,\"content\":\"# UPSERTING\\n\"},\"typeVersion\":1},{\"id\":\"60ebdb71-c7e0-429b-9394-b680cc000951\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position"}
{"prompt": "Create an n8n workflow for: Telegram chat with PDF What this template does This template serves as a Chatbot that enables you to ask questions about the content of a PDF directly in Telegream. It checks incoming Telegram messages if they contain a document. If they do, it stores the PDF in a Pinecone Vector store. If there's no document, it will search the Vector Store for information and try to answer your question. Setup Open the Telegram app and search for the BotFather user (@BotFather) Start a chat with the BotFather Type /newbot to create a new bot Follow the prompts to name your bot and get a unique API token Save your access token and username Once you set your bot, you can send the pdf, and then ask questions about the content. How to adjust it to your needs You can exchange the Groq chat model with any model that you like Exchange Pinecone with any other vector store tool you like (e.g. Supabase, Postgres or QDrant) #Telegram, #Pinecone, #Openai, #GroQ felipe biava cataneo | 6 months ago | Free +4", "completion": "{\"id\":\"5Ycrm1MuK8htwd96\",\"meta\":{\"instanceId\":\"e5595d8cd58f3a24b5a8cf05dd852846c05423873db868a2b7d01a778210c45a\",\"templateCredsSetupCompleted\":true},\"name\":\"Telegram RAG pdf\",\"tags\":[],\"nodes\":[{\"id\":\"9fbce801-8c42-43a4-bc70-d93042d68b2c\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[-220,240],\"webhookId\":\"b178f034-9997-4832-9bb4-a43c3015506e\",\"parameters\":{\"updates\":[\"message\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"\",\"name\":\"\"}},\"typeVersion\":1.1},{\"id\":\"1bfc1fbd-86b1-4a8a-9301-fe54497f5acd\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[720,460],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"\",\"name\":\"\"}},\"typeVersion\":1},{\"id\":\"d5ad7851-ed40-4b3a-b0d5-aeaf04362f1c\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[860,460],\"parameters\":{\"options\":{},\"dataType\":\"binary\"},\"typeVersion\":1},{\"id\":\"fed803d0-49a2-4b82-8f20-a02a10caa027\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[940,680],\"parameters\":{\"options\":{},\"chunkSize\":3000,\"chunkOverlap\":200},\"typeVersion\":1},{\"id\":\"ab60f36f-fada-4812-8dbd-441ad372cb80\",\"name\":\"Stop and Error\",\"type\":\"n8"}
{"prompt": "Create an n8n workflow for: Advanced AI Demo (Presented at AI Developers #14 meetup) This workflow was presented at the AI Developers meet up in San Fransico on 24 July, 2024. AI workflows Categorize incoming Gmail emails and assign custom Gmail labels. This example uses the Text Classifier node, simplifying this usecase. Ingest a PDF into a Pinecone vector store and chat with it (RAG example) AI Agent example showcasing the HTTP Request tool. We teach the agent how to check availability on a Google Calendar and book an appointment. Max Tkacz | 7 months ago | Free +11", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\",\"templateId\":\"2358\"},\"nodes\":[{\"id\":\"fb774d11-da48-4481-ad4e-8c93274f123e\",\"name\":\"Send message\",\"type\":\"n8n-nodes-base.slack\",\"position\":[2340,580],\"parameters\":{\"text\":\"=Data from webhook: {{ $json.query.email }}\",\"select\":\"channel\",\"channelId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"C079GL6K3U6\",\"cachedResultName\":\"general\"},\"otherOptions\":{},\"authentication\":\"oAuth2\"},\"typeVersion\":2.2},{\"id\":\"5a3ad8f1-eba7-4076-80fc-0c1237aab50b\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"color\":7,\"width\":1163.3132111854613,\"height\":677.0358687053997,\"content\":\"![h](https://i.postimg.cc/9XLvL5dL/slide-sf-talk.png#full-width)\"},\"typeVersion\":1},{\"id\":\"01c59396-0fef-4d1c-aa1f-787669300650\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1860,240],\"parameters\":{\"color\":7,\"width\":437,\"height\":99,\"content\":\"# What is n8n?\\n### Low-code Automation Platform for technical teams\"},\"typeVersion\":1},{\"id\":\"0bdd4a35-7f5c-443c-a14a-4e6f7ed18712\",\"name\":\"Execute JavaScript\",\"type\":\"n8n-nodes-base.code\",\"position\":[2340,380],\"parameters\":{\"jsCode\":\"// Loop over input items and add a new field called 'myNewField' to the JSON of each one\\nfor (const item of $input.all()) {\\n item.json.myNewField ="}
{"prompt": "Create an n8n workflow for: Invoice data extraction with LlamaParse and OpenAI This n8n workflow automates the process of parsing and extracting data from PDF invoices. With this workflow, accounts and finance people can realise huge time and cost savings in their busy schedules. Read the Blog: https://blog.n8n.io/how-to-extract-data-from-pdf-to-excel-spreadsheet-advance-parsing-with-n8n-io-and-llamaparse/ How it works This workflow will watch an email inbox for incoming invoices from suppliers It will download the attached PDFs and processing them through a third party service called LlamaParse. LlamaParse is specifically designed to handle and convert complex PDF data structures such as tables to markdown. Markdown is easily to process for LLM models and so the data extraction by our AI agent is more accurate and reliable. The workflow exports the extracted data from the AI agent to Google Sheets once the job complete. Requirements The criteria of the email trigger must be configured to capture emails with attachments. The gmail label \"invoice synced\" must be created before using this workflow. A LlamaIndex.ai account to use the LlamaParse service. An OpenAI account to use GPT for AI work. Google Sheets to save the output of the data extraction process although this can be replaced for whatever your needs. Customizing this workflow This workflow uses Gmail and Google Sheets but these can easily be swapped out for equivalent services such as Outlook and Excel. Not using Excel? Simple redirect the output of the AI agent to your accounting software of choice. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"7076854e-c7e8-45b5-9e5e-16678bffa254\",\"name\":\"OpenAI Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmOpenAi\",\"position\":[2420,480],\"parameters\":{\"model\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-3.5-turbo-1106\",\"cachedResultName\":\"gpt-3.5-turbo-1106\"},\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"00819f1c-2c60-4b7c-b395-445ec05fd898\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[2600,480],\"parameters\":{\"jsonSchema\":\"{\\n \\\"Invoice date\\\": { \\\"type\\\": \\\"date\\\" },\\n \\\"invoice number\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Purchase order number\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Supplier name\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Supplier address\\\": {\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"address 1\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"address 2\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"city\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"postcode\\\": { \\\"type\\\": \\\"string\\\" }\\n }\\n },\\n \\\"Supplier VAT identification number\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Customer name\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"Customer address\\\": {\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"address 1\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"address 2\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"city\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"post"}
{"prompt": "Create an n8n workflow for: Gmail AI Auto-Responder: Create Draft Replies to incoming emails This workflow automatically generates draft replies in Gmail. It's designed for anyone who manages a high volume of emails or often face writer's block when crafting responses. Since it doesn't send the generated message directly, you're still in charge of editing and approving emails before they go out. How It Works: Email Trigger: activates when new emails reach the Gmail inbox Assessment: uses OpenAI gpt-4o and a JSON parser to determine if a response is necessary. Reply Generation: crafts a reply with OpenAI GPT-4 Turbo Draft Integration: after converting the text to html, it places the draft into the Gmail thread as a reply to the first message Set Up Overview (~10 minutes): OAuth Configuration (follow n8n instructions here): Setup Google OAuth in Google Cloud console. Make sure to add Gmail API with the modify scope. Add Google OAuth credentials in n8n. Make sure to add the n8n redirect URI to the Google Cloud Console consent screen settings. OpenAI Configuration: add OpenAI API Key in the credentials Tweaking the prompt: edit the system prompt in the \"Generate email reply\" node to suit your needs Detailed Walkthrough Check out this blog post where I go into more details on how I built this workflow. Reach out to me here if you need help building automations for your business. Nicolas Chourrout | 9 months ago | Free", "completion": "{\"id\":\"aOQANirVMuWrH0ZD\",\"meta\":{\"instanceId\":\"b78ce2d06ac74b90a581919cf44503cf07404c11eda5c3847597226683145618\"},\"name\":\"Gmail AI auto-responder: create draft replies to incoming emails\",\"tags\":[],\"nodes\":[{\"id\":\"2a9ff08f-919a-41a8-980b-8c2bca3059e4\",\"name\":\"Gmail Trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-332.809175564116,566.0845437534399],\"parameters\":{\"simple\":false,\"filters\":{\"q\":\"-from:me\"},\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"ofvBTX8A0aWfQb2O\",\"name\":\"Gmail account\"}},\"typeVersion\":1},{\"id\":\"3ef14615-0045-404f-a21b-2c65a52f4be8\",\"name\":\"If Needs Reply\",\"type\":\"n8n-nodes-base.if\",\"position\":[240,560],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"53849246-ad32-4845-9976-9f9688f5a6f2\",\"operator\":{\"type\":\"boolean\",\"operation\":\"true\",\"singleValue\":true},\"leftValue\":\"={{ $json.needsReply }}\",\"rightValue\":\"true\"}]}},\"typeVersion\":2},{\"id\":\"36968dd5-8d51-4184-a05a-587b6c95aa82\",\"name\":\"JSON Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[100,720],\"parameters\":{\"jsonSchema\":\"{\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"needsReply\\\": {\\n \\\"type\\\": \\\"boolean\\\"\\n }\\n },\\n \\\"required\\\": [\\\"needsReply\\\"]\\n}\\n\"},\"typeVersion\":1},{\"id\":\"2a64dce8-e2f0"}
{"prompt": "Create an n8n workflow for: Write a WordPress post with AI (starting from a few keywords) This n8n workflow template allows you to write WordPress posts by just providing a few keywords. It uses AI technology to write the text and to create the post's featured image. The text includes an introduction, chapters, and conclusions. Each chapter is written independently and this allows you to create also very long articles. The workflow uses technologies provided by Open AI: Chat GPT for the text and Dall-E for the image. I suggest reviewing the created posts before publishing them on your WordPress website. The article generation might take some minutes as each chapter is created independently. Features Easy to use:** Easy web interface to start the generation of the WordPress post AI-powered:** Text and image generation is done by artificial intelligence Long-text ready:** Possibility to create very long articles Configurable:** Possibility to provide as many keywords as you want, to choose the number of chapters and the length of the article Plugs into your WordPress:** Easily integrates with your WordPress website Tweak it as you want:** Fine-tune the Open AI prompts and the workflow as you want Workflow Steps User form:** An n8n form is used to trigger the post creation Settings:** This node is used to set your WordPress URL (which is used later in the workflow) Article structure:** First AI action that writes the introduction, the conclusions, and the chapter structure. Data check:** Check that the data provided by the AI is valid Chapters split/Chapters text:** Splits the data for each chapter in a separate item and generates each chapter's text with AI Content preparation:** Prepares the text for posting merging the introduction, the chapters, and the conclusions. Adds some basic HTML formatting Draft on WordPress:** Creates the draft post on WordPress Featured image:** Creates a featured image and adds it to the post on WordPress User feedback:** Sends a feedback to the user on the n8n form Getting Started To deploy and use this template: Import the workflow into your n8n workspace Set your WordPress URL in the wordpress_url field in the \"Settings\" node. Include the slash (/) at the end of the URL Set up your Open AI n8n credentials by following this guide. The Open AI credentials are used by the Open AI nodes (\"Create post title and structure\", \"Create chapters text\", and \"Generate featured image\") Set up your WordPress n8n credentials by following this guide. The WordPress credentials are used by the WordPress and HTTP Request nodes (\"Post on Wordpress\", \"Upload media\", and \"Set image ID for the post\"). Pay attention that the \"Password\" in the WordPress credentials is not the user's password by the Application Password How use the workflow to create a WordPress post Activate the workflow Open the \"Form\" node and copy the \"Production URL\". This is the public URL of the form to AI-write the post Open the URL in a browser and fill in the form Wait a few minutes till you get the feedback in the form that the post was created Go to WordPress and check the newly created draft post. Review and publish your post! Giulio | 11 months ago | Free", "completion": "{\"id\":\"mKGMYXJottl0PDtM\",\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"name\":\"Write a WordPress post with AI (starting from a few keywords)\",\"tags\":[],\"nodes\":[{\"id\":\"a4f19a81-6101-48c2-9560-9cf231bc240b\",\"name\":\"Form\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-580,320],\"webhookId\":\"4b937814-e829-4df7-aaba-31192babf7e1\",\"parameters\":{\"path\":\"create-wordpress-post\",\"formTitle\":\"Create a WordPress post with AI\",\"formFields\":{\"values\":[{\"fieldLabel\":\"Keywords (comma-separated)\",\"requiredField\":true},{\"fieldType\":\"dropdown\",\"fieldLabel\":\"Number of chapters\",\"fieldOptions\":{\"values\":[{\"option\":\"1\"},{\"option\":\"2\"},{\"option\":\"3\"},{\"option\":\"4\"},{\"option\":\"5\"},{\"option\":\"6\"},{\"option\":\"7\"},{\"option\":\"8\"},{\"option\":\"9\"},{\"option\":\"10\"}]},\"requiredField\":true},{\"fieldType\":\"number\",\"fieldLabel\":\"Max words count\",\"requiredField\":true}]},\"responseMode\":\"responseNode\",\"formDescription\":\"Fill this form with the required information to create a draft post on WordPress\"},\"typeVersion\":2},{\"id\":\"e4cf75f7-00e7-473a-a944-af635581715f\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[209.98769233621147,140],\"parameters\":{\"color\":4,\"width\":301.3874093724939,\"height\":371.765663140765,\"content\":\"## Data check\"},\"typeVersion\":1},{\"id\":\"e949a487-6701-4650-b9be-08146b4e93ad\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[225.20535922952297,200],\"parameters\":{\"color\":7,\"width\":272.8190508599808,\"height\":80,\"content\":\"Checks that the data returned by OpenAI is correct"}
{"prompt": "Create an n8n workflow for: Chat with PDF docs using AI (quoting sources) This workflow allows you to ask questions about a PDF document. The answers are provided by an AI model of your choice, and the answer includes a citation pointing to the information it used. You can use n8n\u2019s built-in chat interface to ask the questions, or you could customise this workflow to use another one (e.g. Slack, Teams, etc.) Example The workflow is set up with the Bitcoin whitepaper. So you could ask things like: Question: \u201cWhich email provider does the creator of Bitcoin use?\u201c Answer: \u201cGMX [Bitcoin whitepaper.pdf, lines 1-35]\u201d Requirements A Pinecone account (they have a free tier at the time of writing that is easily enough for this workflow) Access to a large language model (e.g. an OpenAI account) Customizing this workflow The workflow only reads in one document, but you could customise it to read in all the documents in a folder (or more). The workflow is set up to use GPT 3.5, but you could swap that out for any other model (including self-hosted ones). David Roberts | 12 months ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\",\"templateId\":\"1960\"},\"nodes\":[{\"id\":\"296a935f-bd02-44bc-9e1e-3e4d6a307e38\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[260,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"61a38c00-f196-4b01-9274-c5e0f4c511bc\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1060,460],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"VQtv7frm7eLiEDnd\",\"name\":\"OpenAi account 7\"}},\"typeVersion\":1},{\"id\":\"816066bd-02e8-4de2-bcee-ab81d890435a\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[426.9261940355327,60.389291053299075],\"parameters\":{\"color\":7,\"width\":1086.039382705461,\"height\":728.4168721167887,\"content\":\"## 1. Setup: Fetch file from Google Drive, split it into chunks and insert into a vector database\\nNote that running this part multiple times will insert multiple copies into your DB\"},\"typeVersion\":1},{\"id\":\"30cd81ad-d658-4c33-9a38-68e33b74cdae\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[1240,460],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"file_url\",\"value\":\"={{ $json.file_url }}\"},{\"name\":\"file_name\",\"value\":\"={{ $('Add in metadata').item.json.file_name }}\"}]}},\"dataType\":\"binary\"},\"typeVersion\":1},{\"id\":\"718f09e"}
{"prompt": "Create an n8n workflow for: Analyze Suspicious Email Contents with ChatGPT Vision Phishing Email Detection and Reporting with n8n Who is this for? This workflow is designed for IT teams, security professionals, and managed service providers (MSPs) looking to automate the process of detecting, analyzing, and reporting phishing emails. What problem is this workflow solving? Phishing emails are a significant cybersecurity threat, and manually detecting and reporting them is time-consuming and prone to errors. This workflow streamlines the process by automating email analysis, generating detailed reports, and logging incidents in a centralized system like Jira. What this workflow does This workflow automates phishing email detection and reporting by integrating Gmail and Microsoft Outlook email triggers, analyzing the content and headers of incoming emails, and generating Jira tickets for flagged phishing emails. Here\u2019s what happens: Email Triggers: Captures incoming emails from Gmail or Microsoft Outlook. Email Analysis: Extracts email content, headers, and metadata for analysis. HTML Screenshot: Converts the email\u2019s HTML body into a visual screenshot. AI Phishing Detection: Leverages ChatGPT to analyze the email and detect potential phishing indicators. Jira Integration: Automatically creates a Jira ticket with detailed analysis and attaches the email screenshot for review by the security team. Customizable Reports: Includes options to customize ticket descriptions and adapt the workflow to organizational needs. Setup Authentication: Set up Gmail and Microsoft Outlook OAuth credentials in n8n to access your email accounts securely. API Keys: Add API credentials for the HTML screenshot service (hcti.io) and ChatGPT. Jira Integration: Configure your Jira project and issue types in the workflow. Workflow Configuration: Update sticky notes and nodes to include any additional setup or configuration details unique to your system. How to customize this workflow to your needs Email Filters**: Modify email triggers to filter specific subjects or sender addresses. Analysis Scope**: Adjust the ChatGPT prompt to refine phishing detection logic. Integration**: Replace Jira with your preferred ticketing system or modify the ticket fields to include additional information. This workflow provides an end-to-end automated solution for phishing email management, enhancing efficiency and reducing security risks. It\u2019s perfect for teams looking to minimize manual effort and improve incident response times. Angel Menendez | 2 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"03e9d14e9196363fe7191ce21dc0bb17387a6e755dcc9acc4f5904752919dca8\"},\"nodes\":[{\"id\":\"1bad6bfc-9ec9-48a5-b8f7-73c4de3d08cf\",\"name\":\"Gmail Trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[1480,160],\"parameters\":{\"simple\":false,\"filters\":{},\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"kkhNhqKpZt6IUZd0\",\"name\":\" Gmail\"}},\"typeVersion\":1.2},{\"id\":\"9ac747a1-4fd8-46ba-b4c1-75fd17aab2ed\",\"name\":\"Microsoft Outlook Trigger\",\"type\":\"n8n-nodes-base.microsoftOutlookTrigger\",\"disabled\":true,\"position\":[1480,720],\"parameters\":{\"fields\":[\"body\",\"toRecipients\",\"subject\",\"bodyPreview\"],\"output\":\"fields\",\"filters\":{},\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"microsoftOutlookOAuth2Api\":{\"id\":\"vTCK0oVQ0WjFrI5H\",\"name\":\" Outlook Credential\"}},\"typeVersion\":1},{\"id\":\"5bf9b0e8-b84e-44a2-aad2-45dde3e4ab1b\",\"name\":\"Screenshot HTML\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[2520,480],\"parameters\":{\"url\":\"https://hcti.io/v1/image\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"sendQuery\":true,\"authentication\":\"genericCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"html\",\"value\":\"={{ $json.htmlBody }}\"}]},\"genericAuthType\":\"httpBasicAuth\",\"queryParameters\":{\"parameters\":[{}]}},\"credentials\":{\"httpBasicAuth\":{\"id\":\"8tm8mUWmPvtmPFPk\",\"name\":\"hcti.io\"}},\"typeVersion\":4.2},{\"id\":\"fc770d1d-6c"}
{"prompt": "Create an n8n workflow for: Vector Database as a Big Data Analysis Tool for AI Agents [3/3 - anomaly] Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar \"Build production-ready AI Agents with Qdrant and n8n\". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection The first pipeline to upload an image dataset to Qdrant. The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. 3. This is the third pipeline --- the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification The first pipeline to upload an image dataset to Qdrant. The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] Anomaly Detection Tool This is the tool that can be used directly for anomalous images (crops) detection. It takes as input (any) image URL and returns a text message telling if whatever this image depicts is anomalous to the crop dataset stored in Qdrant. An Image URL is received via the Execute Workflow Trigger, which is used to generate embedding vectors using the Voyage AI Embeddings API. The returned vectors are used to query the Qdrant collection to determine if the given crop is known by comparing it to threshold scores of each image class (crop type). If the image scores lower than all thresholds, then the image is considered an anomaly for the dataset. Jenny | 2 months ago | Free", "completion": "{\"id\":\"G8jRDBvwsMkkMiLN\",\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"name\":\"[3/3] Anomaly detection tool (crops dataset)\",\"tags\":[{\"id\":\"spMntyrlE9ydvWFA\",\"name\":\"anomaly-detection\",\"createdAt\":\"2024-12-08T22:05:15.945Z\",\"updatedAt\":\"2024-12-09T12:50:19.287Z\"}],\"nodes\":[{\"id\":\"e01bafec-eb24-44c7-b3c4-a60f91666350\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1200,180],\"parameters\":{\"color\":6,\"width\":400,\"height\":740,\"content\":\"We are working here with crops dataset: \\nExisting (so not anomalies) crops images in dataset are:\\n- 'pearl_millet(bajra)',\\n- 'tobacco-plant',\\n- 'cherry',\\n- 'cotton',\\n- 'banana',\\n- 'cucumber',\\n- 'maize',\\n- 'wheat',\\n- 'clove',\\n- 'jowar',\\n- 'olive-tree',\\n- 'soyabean',\\n- 'coffee-plant',\\n- 'rice',\\n- 'lemon',\\n- 'mustard-oil',\\n- 'vigna-radiati(mung)',\\n- 'coconut',\\n- 'gram',\\n- 'pineapple',\\n- 'sugarcane',\\n- 'sunflower',\\n- 'chilli',\\n- 'fox_nut(makhana)',\\n- 'jute',\\n- 'papaya',\\n- 'tea',\\n- 'cardamom',\\n- 'almond'\\n\"},\"typeVersion\":1},{\"id\":\"b9943781-de1f-4129-9b81-"}
{"prompt": "Create an n8n workflow for: Vector Database as a Big Data Analysis Tool for AI Agents [1/3 anomaly][1/2 KNN] Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar \"Build production-ready AI Agents with Qdrant and n8n\". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection 1. This is the first pipeline to upload an image dataset to Qdrant. The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification 1. This is the first pipeline to upload an image dataset to Qdrant. The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] Batch Uploading Images Dataset to Qdrant This template imports dataset images from Google Could Storage, creates Voyage AI embeddings for them in batches, and uploads them to Qdrant, also in batches. In this particular template, we work with crops dataset. However, it's analogous to uploading lands dataset, and in general, it's adaptable to any dataset consisting of image URLs (as the following pipelines are). First, check for an existing Qdrant collection to use; otherwise, create it here. Additionally, when creating the collection, we'll create a payload index, which is required for a particular type of Qdrant requests we will use later. Next, import all (dataset) images from Google Cloud Storage but keep only non-tomato-related ones (for anomaly detection testing). Create (per batch) embeddings for all imported images using the Voyage AI multimodal embeddings API. Finally, upload the resulting embeddings and image descriptors to Qdrant via batch upload. Jenny | 2 months ago | Free", "completion": "{\"id\":\"pPtCy6qPfEv1qNRn\",\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"name\":\"[1/3 - anomaly detection] [1/2 - KNN classification] Batch upload dataset to Qdrant (crops dataset)\",\"tags\":[{\"id\":\"n3zAUYFhdqtjhcLf\",\"name\":\"qdrant\",\"createdAt\":\"2024-12-10T11:56:59.987Z\",\"updatedAt\":\"2024-12-10T11:56:59.987Z\"}],\"nodes\":[{\"id\":\"53831410-b4f3-4374-8bdd-c2a33cd873cb\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-640,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e303ccea-c0e0-4fe5-bd31-48380a0e438f\",\"name\":\"Google Cloud Storage\",\"type\":\"n8n-nodes-base.googleCloudStorage\",\"position\":[820,160],\"parameters\":{\"resource\":\"object\",\"returnAll\":true,\"bucketName\":\"n8n-qdrant-demo\",\"listFilters\":{\"prefix\":\"agricultural-crops\"},\"requestOptions\":{}},\"credentials\":{\"googleCloudStorageOAuth2Api\":{\"id\":\"fn0sr7grtfprVQvL\",\"name\":\"Google Cloud Storage account\"}},\"typeVersion\":1},{\"id\":\"737bdb15-61cf-48eb-96af-569eb5986ee8\",\"name\":\"Get fields for Qdrant\",\"type\":\"n8n-nodes-base.set\",\"position\":[1080,160],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"10d9147f-1c0c-4357-8413-3130829c2e24\",\"name\":\"=publicLink\",\"type\":\"string\",\"value\":\"=https://storage.googleapis.com/{{ $json.bucket }}/{{ $json.selfLink.split('/"}
{"prompt": "Create an n8n workflow for: Intelligent Web Query and Semantic Re-Ranking Flow using Brave and Google Gemini Workflow Description This workflow is a powerful, fully automated web query and semantic reranking system that allows users to perform precise, detailed searches, intelligently rank search results and provide high-quality, structured output. Built with AI-powered components, the workflow leverages semantic query generation, result re-ranking, and real-time reporting to deliver actionable insights. It is particularly well-suited for real-time data retrieval, market research, and any domain requiring automated yet customizable search result processing. How It Works Webhook Integration for Input: The workflow begins with a Webhook Node that captures the user's search query as input, enabling seamless integration with other systems. Step 1: Semantic Query Generation (Powered by \"Semantic Search - Query Maker\"): Using AI (Google Gemini), the initial query is refined and transformed into a context-aware, expert-level search query. The process ensures that the search engine retrieves the most relevant and precise results. Step 2: Web Search Execution: A free Brave Search API processes the refined query to fetch search results, ensuring speed and cost efficiency. Step 3: Semantic Re-Ranking of Results (Powered by \"Semantic Search - Result Re-Ranker\"): The workflow reranks the search results based on relevance to the original question, prioritizing the most relevant URLs dynamically. Results are passed through AI-powered intelligent reranking to ensure the final output reflects optimal relevance and quality. Step 4: Structured Output Generation: Results are converted into a well-structured, organized JSON format, ranking the top 10 search results with their titles, links, and descriptions. Missing ranks (if fewer than 10 results) are handled gracefully with placeholders, ensuring consistency. Step 5: Real-Time Reporting: The reranked search results are sent back to the user or integrated system via the Webhook Node in a JSON-formatted response. Reports are highly structured and ready for downstream processing or consumption. Key Features AI-Powered Query Refinement: Transforms basic queries into detailed, expert-level search terms for optimal results. Dual-Stage Semantic Search: Combines query generation and result reranking for precise, high-relevance outputs. Top 10 Result Reranking: Dynamically ranks and organizes the top 10 results based on semantic relevance to the query. Customizable Integration: Fully modifiable for alternative APIs or integrations, such as other search engines or custom ranking logic. JSON-Formatted Structured Results: Outputs reranked results in a standardized format, ideal for integration into systems requiring machine-readable data. Webhook-Based Flexibility: Works seamlessly with Webhook inputs for easy deployment in diverse workflows. Cost-Effective API Usage: Pre-integrated with the free Brave Search API, minimizing operational costs while delivering accurate search results. Instructions for API Setup Brave Search API: Visit api.search.brave.com to obtain a free-tier API key for web search. AI Integration (Google Gemini): Visit Google AI Studio and generate an API key for semantic query generation and reranking. Webhook Configuration: Set up the input Webhook to capture search queries and the output Webhook to deliver reranked results. Why Choose This Workflow? Precision and Relevance**: Combines AI-based query generation with advanced reranking for accurate results. Fully Customizable**: Easily adapt the workflow to alternative APIs, search engines, or ranking logic. Real-Time Insights**: Provides structured, real-time output ready for immediate use. Scalable and Modular**: Ideal for businesses, researchers, and data analysts needing a robust, repeatable solution. Tags AI Workflow, Semantic Search, Query Refinement, Search Result Reranking, Real-Time Search, Web Search Automation, Google Search, Brave Search, News Search, API Integration, Market Research, Competitive Intelligence, Business Intelligence,Google Gemini, Anthropic Claude, OpenAI, GPT, LLM Mind-Front | 2 months ago | Free +3", "completion": "{\"id\":\"wa2uEnSIowqSrHoY\",\"meta\":{\"instanceId\":\"cca06617664f52c5a019ea575691fdbce675dd95dc0452af5f13dbe76d615b69\"},\"name\":\"Intelligent Web Query and Semantic Re-Ranking Flow\",\"tags\":[],\"nodes\":[{\"id\":\"8e7dc5cb-6822-4ef6-9e5a-2b350a1526bf\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-640,-620],\"parameters\":{\"color\":5,\"width\":1172,\"height\":970,\"content\":\"\\n## Step 1. Set Up a Free Brave Web Search Query API Key\\n\\nTo attain the free web search API tier from Brave, follow these steps:\\n\\n1. Visit api.search.brave.com\\n2. Create an account\\n3. Subscribe to the free plan (no charge)\\n4. Navigate to the API Keys section\\n5. Generate an API key. For the subscription type, choose \\\"Free\\\".\\n6. Go to the \\\"Query\\\" Nodes and change the \\\"X-Subscription-Token\\\" value to your API Key.\\n\"},\"typeVersion\":1},{\"id\":\"5bb3e68f-7693-4d4b-b794-843f2c3535e0\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1580,-420],\"parameters\":{\"color\":4,\"width\":680,\"height\":360,\"content\":\"## If you require to change this Node to Webhook Or any Other Item:\\n\\n- In case you want to change the input type from Webhook to any other item, Make sure to go to the Query 1 and Query 1 Ranker and replace the Webhook Input to your Node's input.\"},\"typeVersion\":1},{\"id\":\"f2fc02f9-a78a-4e87-be85-0032492a9f3f\",\"name\":\"Date & Time\",\"type\":\"n8n-nodes-base.dateTime\",\"position\":[-820,-240],\"parameters\":{\"options\":{}},\"typeVersion\":2},{\"id\":\"6f18ebbd-83db-"}
{"prompt": "Create an n8n workflow for: Extract insights & analyse YouTube comments via AI Agent chat Video Guide I prepared a detailed guide to help you set up your workflow effectively, enabling you to extract insights from YouTube for content generation using an AI agent. Youtube Link Who is this for? This workflow is ideal for content creators, marketers, and analysts looking to enhance their YouTube strategies through data-driven insights. It\u2019s particularly beneficial for individuals wanting to understand audience preferences and improve their video content. What problem does this workflow solve? Navigating the content generation and optimization process can be complex, especially without significant audience insight. This workflow automates insights extraction from YouTube videos and comments, empowering users to create more engaging and relevant content effectively. What this workflow does The workflow integrates various APIs to gather insights from YouTube videos, enabling automated commentary analysis, video transcription, and thumbnail evaluation. The main functionalities include: Extracting user preferences from comments. Transcribing video content for enhanced understanding. Analyzing thumbnails via AI for maximum viewer engagement insights. AI Insights Extraction: Automatically pulls comments and metrics from selected YouTube creators to evaluate trends and gaps. Dynamic Video Planning: Uses transcriptions to help creators outline video scripts and topics based on audience interest. Thumbnail Assessment: Provides analysis on thumbnail designs to improve click-through rates and viewer attraction. Setup N8N Workflow API Setup: Create a Google Cloud project and enable the YouTube Data API. Generate an API key to be included in your workflow requests. YouTube Creator and Video Selection: Start by defining a request to identify top creators based on their video views. Capture the YouTube video IDs for further analysis of comments and other video metrics. Comment Analysis: Gather comments associated with the selected videos and analyze them for user insights. Video Transcription: Utilize the insights from transcriptions to formulate content plans. Thumbnail Analysis: Evaluate your video thumbnails by submitting the URL through the OpenAI API to gain insights into their effectiveness. Mark Shcherbakov | 2 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6a2a7715680b8313f7cb4676321c5baa46680adfb913072f089f2766f42e43bd\"},\"nodes\":[{\"id\":\"f4b3833b-cf25-4bbc-927c-080586c5713c\",\"name\":\"Sticky Note9\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[700,760],\"parameters\":{\"color\":7,\"width\":330.5152611046425,\"height\":239.5888196628349,\"content\":\"### ... or watch set up video [13 min]\\n[![Youtube Thumbnail](https://cflobdhpqwnoisuctsoc.supabase.co/storage/v1/object/public/my_storage/Youtube%20AI%20Agent%20Blur.png)](https://youtu.be/6RmLZS8Yl4E)\\n\"},\"typeVersion\":1},{\"id\":\"64d96c53-b3e2-4aea-9a29-9b9e5c729f4f\",\"name\":\"Sticky Note7\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[400,240],\"parameters\":{\"color\":7,\"width\":636.2128494576581,\"height\":497.1532689930921,\"content\":\"![5min Logo](https://cflobdhpqwnoisuctsoc.supabase.co/storage/v1/object/public/my_storage/banner.png)\\n## AI Agent To Chat With Youtube\\n**Made by [Mark Shcherbakov](https://www.linkedin.com/in/marklowcoding/) from community [5minAI](https://www.skool.com/5minai)**\\n\\nNavigating the content generation and optimization process can be complex, especially without significant audience insight. This workflow automates insights extraction from YouTube videos and comments, empowering users to create more engaging and relevant content effectively.\\n\\nThe workflow integrates various APIs to gather insights from YouTube videos, enabling automated commentary analysis, video transcription, and thumbnail evaluation. The main functionalities include:\\n- Extracting user preferences from comments.\\n- Transcribing video content for enhanced understanding."}
{"prompt": "Create an n8n workflow for: Time logging on Clockify using Slack Time Logging on Clockify Using Slack How it works This workflow simplifies time tracking for teams and agencies by integrating Slack with Clockify. It enables users to log, update, or delete time entries directly within Slack, leveraging an AI-powered assistant for seamless and conversational interactions. Key features include: Effortless Time Logging**: Create and manage time entries in Clockify without leaving Slack. AI-Powered Assistant**: Get step-by-step guidance to ensure accurate and efficient time logging. Project and Client Management**: Retrieve project and client information from Clockify effortlessly. Overlap Prevention**: Avoid overlapping entries with built-in time validation. Automated Descriptions**: Generate ethical, grammatically correct descriptions for time logs. Set up steps 1. Prepare your integrations Ensure you have active accounts for both Slack and Clockify. Generate your Clockify API credentials for integration. 2. Import the workflow Download and import the workflow template into your n8n instance. Configure the workflow to connect with your Slack and Clockify accounts. 3. Configure the workflow Add your Clockify API credentials in the workflow settings. Set up the Slack Trigger to listen for app mentions or specific commands. 4. Test the workflow Use Slack to create a time entry and verify it in Clockify. Test updating and deleting existing entries to ensure smooth functionality. Check for any overlapping time logs or incorrect data entries. Why use this workflow? Efficiency**: Eliminate the need to switch between tools for time tracking. Accuracy**: AI-driven validation ensures error-free entries. Automation**: Simplify repetitive tasks like updating or deleting time logs. Proactive Guidance**: Conversational assistant ensures smooth operations. Blockia Labs | 3 months ago | Free +2", "completion": "{\"id\":\"WsksMHrmAQrG32db\",\"meta\":{\"instanceId\":\"92fc20bda79393649a623da4b0a65937bcc52015ab24e5a11633573bf81c05ba\"},\"name\":\"ClockifyBlockiaWorkflow\",\"tags\":[{\"id\":\"0zJNrNQJD49aoFYO\",\"name\":\"Clockify\",\"createdAt\":\"2024-12-02T21:53:54.940Z\",\"updatedAt\":\"2024-12-02T21:53:54.940Z\"},{\"id\":\"JvuYX1WC7uT4mYl7\",\"name\":\"Slack\",\"createdAt\":\"2024-12-02T21:53:56.825Z\",\"updatedAt\":\"2024-12-02T21:53:56.825Z\"}],\"nodes\":[{\"id\":\"98efbcb6-7d13-436d-bb78-22999944b4da\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[-800,260],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"oMvpfPJuOGdwct9R\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"1e32c1a0-3fb6-4fb6-b706-805b16f95528\",\"name\":\"Calculator\",\"type\":\"@n8n/n8n-nodes-langchain.toolCalculator\",\"position\":[-20,260],\"parameters\":{},\"typeVersion\":1},{\"id\":\"363b9823-5c97-4da6-889f-4fa83fac539f\",\"name\":\"Create New Time Entry\",\"type\":\"@n8n/n8n-nodes-langchain.toolHttpRequest\",\"position\":[-120,260],\"parameters\":{\"url\":\"=https://api.clockify.me/api/v1/workspaces/6735b75fe9244e75e2123fba/time-entries\",\"method\":\"POST\",\"jsonBody\":\"{\\n \\\"billable\\\": true,\\n \\\"description\\\": \\\"{logDescription}\\\",\\n \\\"end\\\": \\\"{endTime}\\\",\\n \\\""}
{"prompt": "Create an n8n workflow for: Call analyzer with AssemblyAI transcription and OpenAI assistant integration Video Guide I prepared a detailed guide that showed the whole process of building a call analyzer. .png) Who is this for? This workflow is ideal for sales teams, customer support managers, and online education services that conduct follow-up calls with clients. It\u2019s designed for those who want to leverage AI to gain deeper insights into client needs and upsell opportunities from recorded calls. What problem does this workflow solve? Many follow-up sales calls lack structured analysis, making it challenging to identify client needs, gauge interest levels, or uncover upsell opportunities. This workflow enables automated call transcription and AI-driven analysis to generate actionable insights, helping teams improve sales performance, refine client communication, and streamline upselling strategies. What this workflow does This workflow transcribes and analyzes sales calls using AssemblyAI, OpenAI, and Supabase to store structured data. The workflow processes recorded calls as follows: Transcribe Call with AssemblyAI: Converts audio into text with speaker labels for clarity. Analyze Transcription with OpenAI: Using a predefined JSON schema, OpenAI analyzes the transcription to extract metrics like client intent, interest score, upsell opportunities, and more. Store and Access Results in Supabase: Stores both transcription and analysis data in a Supabase database for further use and display in interfaces. Setup Preparation Create Accounts: Set up accounts for N8N, Supabase, AssemblyAI, and OpenAI. Get Call Link: Upload audio files to public Supabase storage or Dropbox to generate a direct link for transcription. Prepare Artifacts for OpenAI: Define Metrics: Identify business metrics you want to track from call analysis, such as client needs, interest score, and upsell potential. Generate JSON Schema: Use GPT to design a JSON schema for structuring OpenAI\u2019s responses, enabling efficient storage, analysis, and display. Create Analysis Prompt: Write a detailed prompt for GPT to analyze calls based on your metrics and JSON schema. Scenario 1: Transcribe Call with AssemblyAI Set Up Request: Header Authentication: Set Authorization with AssemblyAI API key. URL: POST to https://api.assemblyai.com/v2/transcript/. Parameters: audio_url: Direct URL of the audio file. webhook_url: URL for an N8N webhook to receive the transcription result. Additional Settings: speaker_labels (true/false): Enables speaker diarization. speakers_expected: Specify expected number of speakers. language_code: Set language (default: en_us). Scenario 2: Process Transcription with OpenAI Webhook Configuration: Set up a POST webhook to receive AssemblyAI\u2019s transcription data. Get Transcription: Header Authentication: Set Authorization with AssemblyAI API key. URL: GET https://api.assemblyai.com/v2/transcript/&lt;transcript_id&gt;. Send to OpenAI: URL: POST to https://api.openai.com/v1/chat/completions. Header Authentication: Set Authorization with OpenAI API key. Body Parameters: Model: Use gpt-4o-2024-08-06 for JSON Schema support, or gpt-4o-mini for a less costly option. Messages: system: Contains the main analysis prompt. user: Combined speakers\u2019 utterances to analyze in text format. Response Format: type: json_schema. json_schema: JSON schema for structured responses. Save Results in Supabase: Operation: Create a new record. Table Name: demo_calls. Fields: Input: Transcription text, audio URL, and transcription ID. Output: Parsed JSON response from OpenAI\u2019s analysis. Mark Shcherbakov | 3 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"061bf344-8e0d-46df-a097-dfc000b63bbd\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[1240,1020],\"parameters\":{},\"typeVersion\":1},{\"id\":\"3d66c4ce-e150-4f51-a9f9-4e7f61981ba4\",\"name\":\"Set link to audio\",\"type\":\"n8n-nodes-base.set\",\"position\":[1440,1020],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"745e32ae-61e6-429b-a107-f20c9b17e65e\",\"name\":\"url\",\"type\":\"string\",\"value\":\"https://cflobdhpqwnoisuctsoc.supabase.co/storage/v1/object/public/5minai/OUTBOUNDSAMPLE_01.mp3\"}]}},\"typeVersion\":3.4},{\"id\":\"f6fe5922-1140-499f-b38b-3f0bc0b398cc\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1620,895],\"parameters\":{\"width\":224.61210598590444,\"height\":80,\"content\":\"**Replace API key in Header, webhook from 2nd scenario, change settings if needed**\"},\"typeVersion\":1},{\"id\":\"37454706-c012-45b1-83ca-a618a28c27d5\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1400,900],\"parameters\":{\"width\":167.86310443717323,\"height\":80,\"content\":\"**Replace this link with direct link on your audio**\"},\"typeVersion\":1},{\"id\":\"a52fc9df-20b5-4b2e-956d-66604be12660\",\"name\":\"AssemblyAI - Transcribe\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1680,1020],\"parameters\":{\"url\":\"=https://api.assemblyai.com/v2/transcript\",\"method\":\"POST"}
{"prompt": "Create an n8n workflow for: Create Content from Form Inputs and Save it to Google Drive using AI AI Content Generator Workflow Introduction This workflow automates the process of creating high-quality articles using AI, organizing them in Google Drive, and tracking their progress in Google Sheets. It's perfect for marketers, bloggers, and businesses looking to streamline content creation. With minimal setup, you can have a fully operational system to generate, save, and manage your articles in one cohesive workflow. How It Works Collect Inputs: Users fill out a form with details like article title, keywords, and instructions. Generate Content: AI creates an outline and writes the article based on user inputs. Organize Files: Saves the outline and final article in Google Drive for easy access. Track Progress: Updates Google Sheets with links to the generated content for tracking. Set Up Steps Time Required**: Approximately 15\u201320 minutes to connect all integrations and test the workflow. Steps**: Connect Google Drive and Google Sheets: Authorize access to store files and update the spreadsheet. Set Up OpenAI Integration: Add your OpenAI API key for generating the outline and article content. Customize the Form: Modify the form fields to match the details you want to collect for each article. Test the Workflow: Run the workflow with sample inputs to ensure everything works smoothly. This workflow not only simplifies the process of article creation but also sets a foundation for expanding into additional automations, like posting to social media platforms. Francis Njenga | 3 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"be27b2af86ae3a5dc19ef2a1947644c0aec45fd8c88f29daa7dea6f0ce537691\"},\"nodes\":[{\"id\":\"11abe711-000c-4960-9f07-4e124532ba83\",\"name\":\"create_folder\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[20,440],\"parameters\":{\"name\":\"={{ $('topic_variables').item.json.Title }}\",\"driveId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"My Drive\"},\"options\":{},\"folderId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"root\",\"cachedResultUrl\":\"https://drive.google.com/drive\",\"cachedResultName\":\"/ (Root folder)\"},\"resource\":\"folder\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"MHcgKR744VHXSe3X\",\"name\":\"Drive n8n\"}},\"typeVersion\":3},{\"id\":\"8198bcdb-3082-43d8-84aa-77e292b56e05\",\"name\":\"input_brief\",\"type\":\"n8n-nodes-base.set\",\"position\":[1040,440],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"eff28505-c438-4c44-8db4-188797b1e5f3\",\"name\":\"content\",\"type\":\"string\",\"value\":\"={{ $('create_outline').item.json.message.content }}\"}]}},\"typeVersion\":3.4},{\"id\":\"9b2be845-91c5-4fa8-9007-c0cee4058ddd\",\"name\":\"new_lines\",\"type\":\"n8n-nodes-base.set\",\"position\":[1260,440],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"dda6ee09-0629-4ebc-a4cf-80ebe0172dee\",\"name\":\"content\",\"type\":\"array\",\"value\":\"={{ $json.content.split(/(?:\\\\r?\\\\n){2}## /) }}\"}]}},\"typeVersion\":3.4},{\"id\":\"e5228041-32e7-4834-"}
{"prompt": "Create an n8n workflow for: \ud83d\udcc4\ud83c\udf10PDF2Blog - Create Blog Post on Ghost CRM from PDF Document From PDF to Powerful Blog Posts: AI-Powered Content Transformation Turn complex documents into engaging digital content that drives results. This n8n Workflow uses AI to transforms lengthy PDFs into compelling blog posts that attract and retain readers while you focus on strategic initiatives. Time-Saving Innovation \ud83d\ude85Lightning-Fast Processing Transform lengthy documents into polished blog content in under 1 minute, eliminating hours of manual work. Our system handles the heavy lifting, delivering up to a 95% reduction in content production time. \ud83d\udcf1Intelligent Analysis The AI engine identifies and extracts key insights, organizing information for maximum impact. Each document undergoes comprehensive analysis to ensure no valuable content is overlooked. Advanced Content Optimization \u270d\ufe0fDynamic Writing Styles Possible Adjust the prompt for multiple tone options: Professional for corporate communications Conversational for engaging blogs Thought leadership for industry authority \ud83d\udccaSEO-Ready Content Potential Adjust the prompt to automatically optimized for search engines, incorporating relevant keywords and semantic structure to improve visibility and drive organic traffic. Ideal Applications \ud83e\udd3cContent Marketing Teams Scale content production without sacrificing quality or consistency. Perfect for teams looking to maintain a robust publishing schedule while maximizing resource efficiency. \ud83c\udfebAcademic Communication Help researchers and institutions share complex findings with broader audiences through accessible, engaging content that maintains academic integrity. \ud83e\uddd1\u200d\ud83d\udcbbDigital Publishers Streamline the content transformation process while ensuring each piece meets modern digital standards and reader expectations. Transform your content strategy with an intelligent system that delivers consistent, high-quality results while dramatically reducing production time. Joseph LePage | 4 months ago | Free", "completion": "{\"id\":\"dMiUunCiaMsCr1Wu\",\"meta\":{\"instanceId\":\"03907a25f048377a8789a4332f28148522ba31ee907fababf704f1d88130b1b6\",\"templateCredsSetupCompleted\":true},\"name\":\"\ud83d\udcc4\ud83d\udee0\ufe0fPDF2Blog\",\"tags\":[],\"nodes\":[{\"id\":\"58a4923b-3e8f-4abd-bebc-6488f8b04101\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[580,1340],\"parameters\":{\"color\":3,\"width\":461,\"height\":359.27075107113785,\"content\":\"## Upload PDF and Extract Text\"},\"typeVersion\":1},{\"id\":\"eb0ec98a-9c9d-4203-9586-9e81e23e7232\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1087.027580347215,1340],\"parameters\":{\"color\":4,\"width\":508.8267597424673,\"height\":532.0416571599118,\"content\":\"## Create Blog Post\"},\"typeVersion\":1},{\"id\":\"b2d3cb70-4335-43a2-80db-889559ebf020\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[2020,1340],\"parameters\":{\"color\":6,\"width\":370.4721755771028,\"height\":352.3823858238478,\"content\":\"## Publish Draft Blog Post to Ghost\"},\"typeVersion\":1},{\"id\":\"766daeb8-23e8-4ee9-acee-ea2a787bbfda\",\"name\":\"Upload PDF\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[660,1480],\"webhookId\":\"6c4a4180-7206-469f-a645-f41824ccbf42\",\"parameters\":{\"path\":\"pdf\",\"options\":{},\"formTitle\":\"PDF2Blog\",\"formFields\":{\"values\":[{\"fieldType\":\"file\",\"fieldLabel\":\"Upload PDF File\",\"multipleFiles\":false,\"requiredField\":true,\"acceptFileTypes"}
{"prompt": "Create an n8n workflow for: Create LinkedIn Contributions with AI and Notify Users On Slack This workflow automates the process of gathering LinkedIn advice articles, extracting their content, and generating unique contributions for each article using an AI model. The contributions are then posted to a Slack channel and a NocoDB database for record-keeping. The workflow is triggered weekly to ensure new articles are continuously collected and responded to. Who is this for? This workflow is designed for professionals, marketers, and content creators looking to boost their LinkedIn presence by regularly engaging with LinkedIn advice articles. It\u2019s especially useful for those who want to be seen as a \"thought leader\" or \"top voice\" in their niche by contributing relevant and unique advice to trending topics. What problem is this workflow solving? Manually searching for relevant LinkedIn articles, reading through them, and crafting thoughtful contributions can be time-consuming. This workflow solves that by automating the process of finding new articles, extracting key content, and generating AI-powered contributions. It helps users stay consistently active on LinkedIn, contributing value to trending discussions. What this workflow does Triggers Weekly: The workflow is set to run every Monday at 8:00 AM. Search Google for LinkedIn Advice Articles: Uses a predefined Google search URL to find the latest LinkedIn advice articles based on the user's area of expertise. Extract LinkedIn Article Links: A code node extracts all LinkedIn advice article links from the search results. Retrieve Article Content: For each article link, the workflow retrieves the HTML content and extracts the article title, topics, and existing contributions. Generate AI-Powered Contributions: The workflow sends the extracted article content to an AI model, which generates unique, helpful advice for each topic within the article. Post to Slack & NocoDB: The AI-generated contributions, along with the article links, are posted to a designated Slack channel and stored in a NocoDB database for future reference. Setup Google Search URL: Update the Google search URL with the relevant LinkedIn advice query for your field (e.g., \"site:linkedin.com/advice 'marketing automation'\"). Slack Integration: Connect your Slack account and specify the Slack channel where you want the contributions to be posted. NocoDB Integration: Set up your NocoDB project to store the generated contributions along with the article titles and links. How to customize this workflow Change Search Terms**: Modify the Google search URL to focus on a different LinkedIn topic or expertise area. Adjust Trigger Frequency**: The workflow is set to run weekly, but you can adjust the frequency by changing the schedule trigger. Enhance Contribution Quality**: Customize the AI model's prompt to generate contributions that align with your brand voice or content strategy. Workflow Summary This workflow helps users maintain a consistent presence on LinkedIn by automating the discovery of new advice articles and generating unique contributions using AI. It is ideal for professionals who want to engage with LinkedIn content regularly without spending too much time manually searching and drafting responses. Darryn | 4 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"38d37c49298b42c645e6a7693766d7c3522b24e54454034f955422b5d7af611c\"},\"nodes\":[{\"id\":\"dd9e2f9c-225a-4b6b-9904-293206a477e4\",\"name\":\"Get advice articles from a Google search\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1040,360],\"parameters\":{\"url\":\"=https://www.google.com/search?q=site%3Alinkedin.com%2Fadvice+{{ $json.Topic }}\",\"options\":{\"batching\":{\"batch\":{\"batchSize\":25}}}},\"typeVersion\":4.2},{\"id\":\"0e2bcaeb-65a0-400a-a15e-0840723d8144\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[980,320],\"parameters\":{\"color\":2,\"width\":621.7044818991839,\"height\":566.8592254014303,\"content\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## 1. Get advice articles from a Google search**\\nUses an HTTP request to perform a Google search for LinkedIn advice articles based on a predefined query.\\n\\n## 2. Extract Article links for LinkedIn advice articles\\nThis Code node extracts LinkedIn article URLs from the Google search results by using a regular expression. It pulls all article links related to LinkedIn advice.\\n\\n## 3. Split Out all links for LinkedIn advice articles\\nSplits the list of extracted LinkedIn article links into individual items. This allows each article to be processed one at a time in the following steps.\\n\"},\"typeVersion\":1},{\"id\":\"68eefc93-6c82-4687-bb4d-52345e5a5094\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[520,80],\"parameters\":{},\"typeVersion\":1},{\"id\":\"af3fdd03-d28a-4f49-b"}
{"prompt": "Create an n8n workflow for: Narrating over a Video using Multimodal AI This n8n template takes a video and extracts frames from it which are used with a multimodal LLM to generate a script. The script is then passed to the same multimodal LLM to generate a voiceover clip. This template was inspired by Processing and narrating a video with GPT's visual capabilities and the TTS API How it works Video is downloaded using the HTTP node. Python code node is used to extract the frames using OpenCV. Loop node is used o batch the frames for the LLM to generate partial scripts. All partial scripts are combined to form the full script which is then sent to OpenAI to generate audio from it. The finished voiceover clip is uploaded to Google Drive. Sample the finished product here: https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing Requirements OpenAI for LLM Ideally, a mid-range (16GB RAM) machine for acceptable performance! Customising this workflow For larger videos, consider splitting into smaller clips for better performance Use a multimodal LLM which supports fully video such as Google's Gemini. Jimleuk | 4 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"6d16b5be-8f7b-49f2-8523-9b84c62f2759\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1960,660],\"parameters\":{\"model\":\"gpt-4o-2024-08-06\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"a6084f09-9a4f-478a-ac1a-ab1413628c1f\",\"name\":\"Capture Frames\",\"type\":\"n8n-nodes-base.code\",\"position\":[720,460],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"language\":\"python\",\"pythonCode\":\"import cv2\\nimport numpy as np\\nimport base64\\n\\ndef extract_evenly_distributed_frames_from_base64(base64_string, max_frames=90):\\n # Decode the Base64 string into bytes\\n video_bytes = base64.b64decode(base64_string)\\n \\n # Write the bytes to a temporary file\\n video_path = '/tmp/temp_video.mp4'\\n with open(video_path, 'wb') as video_file:\\n video_file.write(video_bytes)\\n \\n # Open the video file using OpenCV\\n video_capture = cv2.VideoCapture(video_path)\\n \\n # Get the total number of frames in the video\\n total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\\n \\n # Calculate the step size to take 'max_frames' evenly distributed frames\\n step_size = max(1, total_frames // (max_frames - 1))\\n \\n # List to store selected frames as base64\\n selected_frames_base64 = []"}
{"prompt": "Create an n8n workflow for: Enrich Company Data from Google Sheet with OpenAI Agent and ScrapingBee This workflow demonstrates how to enrich data from a list of companies in a spreadsheet. While this workflow is production-ready if all steps are followed, adding error handling would enhance its robustness. Important notes Check legal regulations**: This workflow involves scraping, so make sure to check the legal regulations around scraping in your country before getting started. Better safe than sorry! Mind those tokens**: OpenAI tokens can add up fast, so keep an eye on usage unless you want a surprising bill that could knock your socks off! \ud83d\udcb8 Main Workflow Node 1 - Webhook This node triggers the workflow via a webhook call. You can replace it with any other trigger of your choice, such as form submission, a new row added in Google Sheets, or a manual trigger. Node 2 - Get Rows from Google Sheet This node retrieves the list of companies from your spreadsheet. here is the Google Sheet Template you can use. The columns in this Google Sheet are: Company**: The name of the company Website**: The website URL of the company These two fields are required at this step. Business Area**: The business area deduced by OpenAI from the scraped data Offer**: The offer deduced by OpenAI from the scraped data Value Proposition**: The value proposition deduced by OpenAI from the scraped data Business Model**: The business model deduced by OpenAI from the scraped data ICP**: The Ideal Customer Profile deduced by OpenAI from the scraped data Additional Information**: Information related to the scraped data, including: Information Sufficiency: Description: Indicates if the information was sufficient to provide a full analysis. Options: \"Sufficient\" or \"Insufficient\" Insufficient Details: Description: If labeled \"Insufficient,\" specifies what information was missing or needed to complete the analysis. Mismatched Content: Description: Indicates whether the page content aligns with that of a typical company page. Suggested Actions: Description: Provides recommendations if the page content is insufficient or mismatched, such as verifying the URL or searching for alternative sources. Node 3 - Loop Over Items This node ensures that, in subsequent steps, the website in \"extra workflow input\" corresponds to the row being processed. You can delete this node, but you'll need to ensure that the \"query\" sent to the scraping workflow corresponds to the website of the specific company being scraped (rather than just the first row). Node 4 - AI Agent This AI agent is configured with a prompt to extract data from the content it receives. The node has three sub-nodes: OpenAI Chat Model: The model used is currently gpt4-o-mini. Call n8n Workflow: This sub-node calls the workflow to use ScrapingBee and retrieves the scraped data. Structured Output Parser: This parser structures the output for clarity and ease of use, and then adds rows to the Google Sheet. Node 5 - Update Company Row in Google Sheet This node updates the specific company's row in Google Sheets with the enriched data. Scraper Agent Workflow Node 1 - Tool Called from Agent This is the trigger for when the AI Agent calls the Scraper. A query is sent with: Company name Website (the URL of the website) Node 2 - Set Company URL This node renames a field, which may seem trivial but is useful for performing transformations on data received from the AI Agent. Node 3 - ScrapingBee: Scrape Company's Website This node scrapes data from the URL provided using ScrapingBee. You can use any scraper of your choice, but ScrapingBee is recommended, as it allows you to configure scraper behavior directly. Once configured, copy the provided \"curl\" command and import it into n8n. Node 4 - HTML to Markdown This node converts the scraped HTML data to Markdown, which is then sent to OpenAI. The Markdown format generally uses fewer tokens than HTML. Improving the Workflow It's always a pleasure to share workflows, but creators sometimes want to keep some magic to themselves \u2728. Here are some ways you can enhance this workflow: Handle potential errors Configure the scraper tool to scrape other pages on the website. Although this will cost more tokens, it can be useful (e.g., scraping \"Pricing\" or \"About Us\" pages in addition to the homepage). Instead of Google Sheets, connect directly to your CRM to enrich company data. Trigger the workflow from form submissions on your website and send the scraped data about the lead to a Slack or Teams channel. Dataki | 4 months ago | Free", "completion": "{\"id\":\"TfwQRZkTBtykx1rM\",\"meta\":{\"instanceId\":\"\",\"templateCredsSetupCompleted\":true},\"name\":\"Enrich Company Data from Google Sheet with OpenAI Agent and Scraper Tool\",\"tags\":[],\"nodes\":[{\"id\":\"90c02c5e-228e-478b-b06d-424dc0c4f9b9\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1500,240],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n \\\"Business Area\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Summary of the company's core activities or industry focus.\\\"\\n },\\n \\\"Offers or Product\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Summary of the company's main products or services.\\\"\\n },\\n \\\"Value Proposition\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Catchphrase or tagline that represents the company\u2019s value proposition.\\\"\\n },\\n \\\"Business Model\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Description of the company's business model, including revenue generation, key partnerships, or unique aspects.\\\"\\n },\\n \\\"Ideal Customer Profile\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Description of the ideal customer profile, based on available information.\\\"\\n },\\n \\\"Additional Information\\\": {\\n \\\"type\\\": \\\"object\\\",\\n \\\"description\\\": \\\"Additional insights or actions if there is insufficient information or if the content does not match a company page.\\\",\\n \\\"properties\\\": {\\n \\\"Information Sufficiency\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"Indicate if the information was sufficient to provide a full analysis.\\\",\\n \\\"enum\\\": [\\\"Sufficient\\\", \\\"Insufficient\\\"]\\n },\\n \\\"Insufficient Details\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"If 'Insufficient', specify what information was missing or would be needed to complete the analysis.\\\",\\n \\\"optional\\\": true\\n },\\n \\\"Mismatched Content\\\": {\\n \\\"type\\\": \\\"boolean\\\",\\n \\\"description\\\": \\\"Indicate whether the page content aligns with that of a typical company"}
{"prompt": "Create an n8n workflow for: Voice Activated Multi-Agent Demo for Vagent.io using Notion and Google Calendar Purpose Use a lightweight Voice Interface, for you and your entire organization, to interact with an AI Supervisor, a personal AI Assistant, which has access to your custom workflows. You can also connect the supervisor to your already existing Agents. Demo & Explanation How it works After recording a message in the Vagent App, it gets transcribed and sent in combination with a session ID to the registered webhook The Main Agent acts as a router. I interprets the message while using the stored chat history (bound to the session ID) and chooses which tool to use to perform the required action and. Tools on this level are workflows, which contain subordinated Agents. Since the Main Agent interprets the original message, the raw input is passed to the Tools/Sub-Agents as a separate parameter Within the Sub-Agents the actual processing takes place. Each of those has it\u2019s separate chat memory (with a suffix to the main session ID), to achieve a clear separation of concerns Depending on the required action an HTTP Request Tool is called. The result is being formatted in Markdown and returned to the Main Agent with an additional short prompt, so it does not get interpreted by the Main Agent. Drafts are separated from a short message by added indentation (angle brackets). If some information is missing, no tool is called just yet, instead a message is returned back to the user The Main Agent then outputs the result from the called Sub-Agent. If a draft is included, it gets separated from the spoken output Finally the formatted output is returned as response to the webhook. The message is split into a spoken and a text version, which enables the App to read out loud unnecessary information like drafts in this example See the full documentation of Vagent: https://vagent.io/docs Setup Import this workflow into your n8n instance Follow the instructions given in the sticky notes on the canvas Setup your credentials. OpenAI can be replaced by another LLM in the workflow, but is required for the App to work. Google Calendar and Notion are required for all scenarios to work Copy the Webhook URL from the Webhook node of the main workflow Download the Vagent App from https://vagent.io In the settings paste your OpenAI API Token, the Webhook URL and the password defined for Header Auth Now you can use the App to interact with the Multi-Agent using your Voice by tapping the Mic symbol in the App to record your message. To use the chat trigger (for testing) properly, temporarily disable the nodes after the Tools Agent. Mario | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\",\"templateId\":\"2446\"},\"nodes\":[{\"id\":\"af0765f4-75b5-445c-80d7-51b0aa180fe5\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[820,620],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0,\"responseFormat\":\"text\"}},\"typeVersion\":1},{\"id\":\"497c534e-e117-4592-b76f-bef424a7fd5a\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1500,400],\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"5b358850-cbc3-4a8c-b2b8-12e3b7aa1e44\",\"name\":\"calendarAgent\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1060,620],\"parameters\":{\"name\":\"calendarAgent\",\"fields\":{\"values\":[{\"name\":\"sessionId\",\"stringValue\":\"={{ $json.sessionId }}\"},{\"name\":\"prompt\",\"stringValue\":\"={{ $json.chatInput }}\"}]},\"workflowId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"yPCMz4zxB291oM31\",\"cachedResultName\":\"Google Calendar Agent\"},\"description\":\"Call this workflow to do handle every request regarding calendar management.\",\"responsePropertyName\":\"output\"},\"typeVersion\":1.2},{\"id\":\"8bcc4b27-59b9-4ce3-8525-34221c10f11a\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[460,480],\"webhookId\":\"96e410fe-ef91-4767-aa9a-bf95ba50f972\",\"parameters\":{\"public\":true,\"options\":{}},\"typeVersion\":1.1},{\"id\":\"0aa"}
{"prompt": "Create an n8n workflow for: Transcribing Bank Statements To Markdown Using Gemini Vision AI This n8n workflow demonstrates an approach to parsing bank statement PDFs with multimodal LLMs as an alternative to traditional OCR. This allows for much more accurate data extraction from the document especially when it comes to tables and complex layouts. Multimodal Parsing is better than traditiona OCR because: It reduces complexity and overhead by avoiding the need to preprocess the document into text format such as markdown before passing to the LLM. It handles non-standard PDF formats which may produce garbled output via traditional OCR text conversion. It's orders of magnitude cheaper than premium OCR models that still require post-processing cleanup and formatting. LLMs can format to any schema or language you desire! How it works You can use the example bank statement created specifically for this workflow here: https://drive.google.com/file/d/1wS9U7MQDthj57CvEcqG_Llkr-ek6RqGA/view?usp=sharing A PDF bank statement is imported via Google Drive. For this demo, I've created a mock bank statement which includes complex table layouts of 5 columns. Typically, OCR will be unable to align the columns correctly and mistake some deposits for withdrawals. Because multimodal LLMs do not accept PDFs directly, well have to convert the PDF to a series of images. We can achieve this by using a tool such as Stirling PDF. Stirling PDF is self-hostable which is handy for sensitive data such as bank statements. Stirling PDF will return our PDF as a series of JPGs (one for each page) in a zipped file. We can use n8n's decompress node to extract the images and ensure they are ordered by using the Sort node. Next, we'll resize each page using the Edit Image node to ensure the right balance between resolution limits and processing speed. Each resized page image is then passed into the Basic LLM node which will use our multimodal LLM of choice - Gemini 1.5 Pro. In the LLM node's options, we'll add a \"user message\" of type binary (data) which is how we add our image data as an input. Our prompt will instruct the multimodal LLM to transcribe each page to markdown. Note, you do not need to do this - you can just ask for data points to extract directly! Our goal for this template is to demonstrate the LLMs ability to accurately read the page. Finally, with our markdown version of all pages, we can pass this to another LLM node to extract required data such as deposit line items. Requirements Google Gemini API for Multimodal LLM. Google Drive access for document storage. Stirling PDF instance for PDF to Image conversion Customising the workflow At time of writing, Gemini 1.5 Pro is the most accurate in text document parsing with a relatively low cost. If you are not using Google Gemini however you can switch to other multimodal LLMs such as OpenAI GPT or Antrophic Claude. If you don't need the markdown, simply asking what to extract directly in the LLM's prompt is also acceptable and would save a few extra steps. Not parsing any bank statements any time soon? This template also works for Invoices, inventory lists, contracts, legal documents etc. Jimleuk | 5 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"490493d1-e9ac-458a-ac9e-a86048ce6169\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-700,260],\"parameters\":{},\"typeVersion\":1},{\"id\":\"116f1137-632f-4021-ad0f-cf59ed1776fd\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[980,440],\"parameters\":{\"options\":{},\"modelName\":\"models/gemini-1.5-pro-latest\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"dSxo6ns5wn658r8N\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"44695b4f-702c-4230-9ec3-e37447fed38e\",\"name\":\"Sort Pages\",\"type\":\"n8n-nodes-base.sort\",\"position\":[400,320],\"parameters\":{\"options\":{},\"sortFieldsUi\":{\"sortField\":[{\"fieldName\":\"fileName\"}]}},\"typeVersion\":1},{\"id\":\"f2575b2c-0808-464e-b982-1eed8e0d9df7\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1280,0],\"parameters\":{\"width\":437.0502325581392,\"height\":430.522325581395,\"content\":\"## Try Me Out!\\n\\n### This workflow converts a bank statement to markdown, faithfully capturing the details using the power of Vision Language Models (\\\"VLMs\\\"). The resulting markdown can then be parsed again by your standard LLM to extract data such as identifying all deposit table rows in the document.\\n\\nThis workflow is able to handle both downloaded PDFs as well as scanned PDFs. Be sure to protect sensitive data before running this workflow.\\n\\n### Need Help?"}
{"prompt": "Create an n8n workflow for: CV Resume PDF Parsing with Multimodal Vision AI This n8n workflow demonstrates how we can use Multimodal LLMs to parse and extract from PDF documents in n8n. In this particular scenario, we're passing a candidate's CV/resume to an AI which filters out unqualified applications. However, this sneaky candidate has added in hidden prompt to bypass our bot! Whatever will we do? No fret, using AI Vision is one approach to solve this problem... read on! How it works Our candidate's CV/Resume is a PDF downloaded via Google Drive for this demonstration. The PDF is then converted into an image PNG using a tool called Stirling PDF. Since the hidden prompt has a white font color, it is is invisible in the converted image. The image is then forwarded to a Basic LLM node to process using our multimodal model - in this example, we'll use Google's Gemini 1.5 Pro. In the Basic LLM node, we'll need to set a User Message with the type of Binary. This allows us to directly send the image file in our request. The LLM is now immune to the hidden prompt and its response is has expected. The example CV/Resume with hidden prompt can be found here: https://drive.google.com/file/d/1MORAdeev6cMcTJBV2EYALAwll8gCDRav/view?usp=sharing Requirements Google Gemini API Key. Alternatively, GPT4 will also work for this use-case. Stirling PDF or another service which can convert PDFs into images. Note for data privacy, this example uses a public API and it is recommended that you self-host and use a private instance of Stirling PDF instead. Customising the workflow Swap out the manual trigger for another trigger such as a webhook to integrate into your existing services. This example demonstrates a validation use-case ie. \"does the candidate look qualified?\". You can try additionally extracting data points instead such as years of experiences, previous companies etc. Jimleuk | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"38da57b7-2161-415d-8473-783ccdc7b975\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-260,840],\"parameters\":{},\"typeVersion\":1},{\"id\":\"2cd46d91-105d-4b5e-be43-3343a9da815d\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-780,540],\"parameters\":{\"width\":365.05232558139534,\"height\":401.24529475392126,\"content\":\"## Try me out!\\n\\n### This workflow converts a Candidate Resume PDF to an image which is then \\\"read\\\" by a Vision Language Model (VLM). The VLM assesses if the candidate's CV is a fit for the desired role.\\n\\nThis approach can be employed to combat \\\"hidden prompts\\\" planted in resumes to bypass and/or manipulate automated ATS systems using AI.\\n\\n\\n### Need Help?\\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\\n\"},\"typeVersion\":1},{\"id\":\"40bab53a-fcbc-4acc-8d59-c20b3e1b2697\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1200,980],\"parameters\":{\"jsonSchemaExample\":\"{\\n\\t\\\"is_qualified\\\": true,\\n\\t\\\"reason\\\": \\\"\\\"\\n}\"},\"typeVersion\":1.2},{\"id\":\"d75fb7ab-cfbc-419d-b803-deb9e99114ba\",\"name\":\"Should Proceed To Stage 2?\",\"type\":\"n8n-nodes-base.if\",\"position\":[1360,820],\"parameters\":{\"options\":{},\"conditions\":{\""}
{"prompt": "Create an n8n workflow for: AI Agent with charts capabilities using OpenAI Structured Output and Quickchart This workflow is an experiment to integrate charts in AI Agents, using the new Structured Output from OpenAI and Quickchart.io. How it works Users chat with an AI Agent. Anytime the AI Agent considers a chart is needed, it calls a tool to generate a chart OpenAI generates a chart using the Quickchart definition This object is added at the end of a Quickchart.io URL (see documentation) The url is added in the conversation via the AI Agent as markdown. Set up steps Create an OpenAI API Key Create the OpenAI credentials Use the credentials for the HTTP Request node (as Predefined Credential type) Activate your workflow Start chatting For example, you can ask the AI Agent to generate a chart about the top 5 movies at the box office Start exploring the limits Shout-out Quickchart.io is an amazing open source project that provides a free API to test. Go check them out! Example of chart Agent Studio | 6 months ago | Free", "completion": "{\"id\":\"6yNJxDjV9rSiOkj9\",\"meta\":{\"instanceId\":\"f4f5d195bb2162a0972f737368404b18be694648d365d6c6771d7b4909d28167\",\"templateCredsSetupCompleted\":true},\"name\":\"AI Agent with charts capabilities using OpenAI Structured Output\",\"tags\":[{\"id\":\"9tRfTc35T5pruw03\",\"name\":\"experiment\",\"createdAt\":\"2024-03-18T15:32:10.504Z\",\"updatedAt\":\"2024-03-18T15:32:10.504Z\"}],\"nodes\":[{\"id\":\"4b7c314a-d7c5-46cb-af6f-b3ff02a182b7\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[980,600],\"parameters\":{\"model\":\"gpt-4o-mini-2024-07-18\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"WqzqjezKh8VtxdqA\",\"name\":\"OpenAi account - Baptiste\"}},\"typeVersion\":1},{\"id\":\"cf4ffa49-8830-4db2-9a7d-b8931e806947\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1120,600],\"parameters\":{},\"typeVersion\":1.2},{\"id\":\"22d36226-ca37-4ccc-a2d6-826b78c2f1f3\",\"name\":\"Generate a chart\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1260,600],\"parameters\":{\"name\":\"generate_a_chart\",\"schemaType\":\"manual\",\"workflowId\":\"={{ $workflow.id }}\",\"description\":\"Call this tool whenever you need to generate a chart.\",\"inputSchema\":\"{\\n\\\"type\\\": \\\"object\\\",\\n\\\"properties\\\": {\\n\\t\\\"query\\\": {\\n\\t\\t\\\"type\\\": \\\"string\\\",\\n\\t\\t\\\"description\\\": \\\"a query"}
{"prompt": "Create an n8n workflow for: IT Ops AI SlackBot Workflow - Chat with your knowledge base Video Demo: Click here to see a video of this workflow in action. Summary Description: The \"IT Department Q&A Workflow\" is designed to streamline and automate the process of handling IT-related inquiries from employees through Slack. When an employee sends a direct message (DM) to the IT department's Slack channel, the workflow is triggered. The initial step involves the \"Receive DMs\" node, which listens for new messages. Upon receiving a message, the workflow verifies the webhook by responding to Slack's challenge request, ensuring that the communication channel is active and secure. Once the webhook is verified, the workflow checks if the message sender is a bot using the \"Check if Bot\" node. If the sender is identified as a bot, the workflow terminates the process to avoid unnecessary actions. If the sender is a human, the workflow sends an acknowledgment message back to the user, confirming that their query is being processed. This is achieved through the \"Send Initial Message\" node, which posts a simple message like \"On it!\" to the user's Slack channel. The core functionality of the workflow is powered by the \"AI Agent\" node, which utilizes the OpenAI GPT-4 model to interpret and respond to the user's query. This AI-driven node processes the text of the received message, generating an appropriate response based on the context and information available. To maintain conversation context, the \"Window Buffer Memory\" node stores the last five messages from each user, ensuring that the AI agent can provide coherent and contextually relevant answers. Additionally, the workflow includes a custom Knowledge Base (KB) tool (see that tool template here) that integrates with the AI agent, allowing it to search the company's internal KB for relevant information. After generating the response, the workflow cleans up the initial acknowledgment message using the \"Delete Initial Message\" node to keep the conversation thread clean. Finally, the generated response is sent back to the user via the \"Send Message\" node, providing them with the information or assistance they requested. This workflow effectively automates the IT support process, reducing response times and improving efficiency. To quickly deploy the Knowledge Ninja app in Slack, use the app manifest below and don't forget to replace the two sample urls: { \"display_information\": { \"name\": \"Knowledge Ninja\", \"description\": \"IT Department Q&A Workflow\", \"background_color\": \"#005e5e\" }, \"features\": { \"bot_user\": { \"display_name\": \"IT Ops AI SlackBot Workflow\", \"always_online\": true } }, \"oauth_config\": { \"redirect_urls\": [ \"Replace everything inside the double quotes with your slack redirect oauth url, for example: https://n8n.domain.com/rest/oauth2-credential/callback\" ], \"scopes\": { \"user\": [ \"search:read\" ], \"bot\": [ \"chat:write\", \"chat:write.customize\", \"groups:history\", \"groups:read\", \"groups:write\", \"groups:write.invites\", \"groups:write.topic\", \"im:history\", \"im:read\", \"im:write\", \"mpim:history\", \"mpim:read\", \"mpim:write\", \"mpim:write.topic\", \"usergroups:read\", \"usergroups:write\", \"users:write\", \"channels:history\" ] } }, \"settings\": { \"event_subscriptions\": { \"request_url\": \"Replace everything inside the double quotes with your workflow webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad\", \"bot_events\": [ \"message.im\" ] }, \"org_deploy_enabled\": false, \"socket_mode_enabled\": false, \"token_rotation_enabled\": false } } Angel Menendez | 6 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"96ef3bfe-a493-4377-b090-6b2d02d87480\",\"name\":\"Verify Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1420,800],\"parameters\":{\"options\":{\"responseCode\":200,\"responseHeaders\":{\"entries\":[{\"name\":\"Content-type\",\"value\":\"application/json\"}]}},\"respondWith\":\"json\",\"responseBody\":\"={\\\"challenge\\\":\\\"{{ $json.body.challenge }}\\\"}\"},\"typeVersion\":1},{\"id\":\"38db6da6-13bf-47a1-b5cb-f06403b309ac\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2120,1220],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"QpFZ2EiM3WGl6Zr3\",\"name\":\"Marketing OpenAI\"}},\"typeVersion\":1},{\"id\":\"139b606d-29ae-480d-bde7-458ef45dba01\",\"name\":\"No Operation, do nothing\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[1840,700],\"parameters\":{},\"typeVersion\":1},{\"id\":\"64acd4c6-cd53-46e5-a29e-40884044b186\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[2800,1220],\"parameters\":{\"sessionKey\":\"={{ $('Receive DMs').item.json[\\\"body\\\"][\\\"event\\\"][\\\"channel\\\"] }}\",\"sessionIdType\":\"customKey\",\"contextWindowLength\":10},\"typeVersion\":1.2},{\"id\":\"e605864f-198e-4358-8333-50ed962d4e50\",\"name\":\"Check if Bot\",\"type\":\"n8n-nodes-base.if\",\"position\":[1640"}
{"prompt": "Create an n8n workflow for: Dynamically generate a webpage from user request using OpenAI Structured Output This workflow is a experiment to build HTML pages from a user input using the new Structured Output from OpenAI. How it works: Users add what they want to build as a query parameter The OpenAI node generate an interface following a structured output defined in the body The JSON output is then converted to HTML along with a title The HTML is encapsulated in an HTML node (where the Tailwind css script is added) The HTML is rendered to the user via the Webhook response. Set up steps Create an OpenAI API Key Create the OpenAI credentials Use the credentials for both nodes HTTP Request (as Predefined Credential type) and OpenAI Activate your workflow Once active, go to the production URL and add what you'd like to build as the parameter \"query\" Example: https://production_url.com?query=a%20signup%20form Example of generated page Agent Studio | 6 months ago | Free", "completion": "{\"id\":\"eXiaTDyKfXpMeyLh\",\"meta\":{\"instanceId\":\"f4f5d195bb2162a0972f737368404b18be694648d365d6c6771d7b4909d28167\",\"templateCredsSetupCompleted\":true},\"name\":\"Dynamically generate HTML page from user request using OpenAI Structured Output\",\"tags\":[],\"nodes\":[{\"id\":\"b1d9659f-4cd0-4f87-844d-32b2af1dcf13\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[2160,380],\"parameters\":{\"options\":{\"responseHeaders\":{\"entries\":[{\"name\":\"Content-Type\",\"value\":\"text/html; charset=UTF-8\"}]}},\"respondWith\":\"text\",\"responseBody\":\"={{ $json.html }}\"},\"typeVersion\":1.1},{\"id\":\"5ca8ad3e-7702-4f07-af24-d38e94fdc4ec\",\"name\":\"Open AI - Using Structured Output\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1240,380],\"parameters\":{\"url\":\"https://api.openai.com/v1/chat/completions\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n \\\"messages\\\": [\\n {\\n \\\"role\\\": \\\"system\\\",\\n \\\"content\\\": \\\"You are a user interface designer and copy writter. Your job is to help users visualize their website ideas. You design elegant and simple webs, with professional text. You use Tailwind framework\\\"\\n },\\n {\\n \\\"role\\\": \\\"user\\\",\\n \\\"content\\\": \\\"{{ $json.query.query }}\\\"\\n }\\n ],\\n \\\"response_format\\\":\\n{\\n \\\"type\\\": \\\"json_schema\\\",\\n \\\"json_schema\\\": {\\n \\\"name\\\": \\\"ui\\\",\\n \\\"description\\\": \\\"Dynamically generated UI\\\",\\n \\\"strict\\\": true,\\n \\\"schema\\\": {\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"type\\\": {\\n \\\"type\\\": \\\"string\\\",\\n \\\"description\\\":"}
{"prompt": "Create an n8n workflow for: Automate Competitor Research with Exa.ai, Notion and AI Agents This n8n workflow demonstrates a simple multi-agent setup to perform the task of competitor research. It showcases how using the HTTP request tool could reduce the number of nodes needed to achieve a workflow like this. How it works For this template, a source company is defined by the user which is sent to Exa.ai to find competitors. Each competitor is then funnelled through 3 AI agents that will go out onto the internet and retrieve specific datapoints about the competitor; company overview, product offering and customer reviews. Once the agents are finished, the results are compiled into a report which is then inserted in a notion database. Check out an example output here: https://jimleuk.notion.site/2d1c3c726e8e42f3aecec6338fd24333?v=de020fa196f34cdeb676daaeae44e110&pvs=4 Requirements An OpenAI account for the LLM. Exa.ai account for access to their AI search engine. SerpAPI account for Google search. Firecrawl.dev account for webscraping. Notion.com account for database to save final reports. Customising the workflow Add additional agents to gather more datapoints such as SEO keywords and metrics. Not using notion? Feel free to swap this out for your own database. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"d26b0190-c683-45fc-ac5b-0654af78f080\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1000,-620],\"parameters\":{\"width\":377.7154173079816,\"height\":511.2813260861502,\"content\":\"## Try It Out!\\n\\n### This workflow builds a competitor research agent using Exa.ai as a starting point. The HTTP Request tool is used to demonstrate how you can build powerful agents with minimal effort.\\n\\n* Using Exa's findSimilar search, we ask it to look for similar companies ie. competitors, to our source company.\\n* This list of competitors is sent to 3 agents to scour the internet to find company overview, product offering and customer reviews.\\n* A report is then compiled from the output of all 3 agents into a notion table.\\n\\n### Need Help?\\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\\n\\nHappy Hacking!\"},\"typeVersion\":1},{\"id\":\"747d2f04-1e9c-45bb-b2ad-68da81524f4f\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-520,-420],\"parameters\":{},\"typeVersion\":1},{\"id\":\"5cb5f5a1-bc2d-4557-aff4-1993d8dcb99b\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1020,20],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"type"}
{"prompt": "Create an n8n workflow for: Actioning Your Meeting Next Steps using Transcripts and AI This n8n workflow demonstrates how you can summarise and automate post-meeting actions from video transcripts fed into an AI Agent. Save time between meetings by allowing AI handle the chores of organising follow-up meetings and invites. How it works This workflow scans for the calendar for client or team meetings which were held online. * Attempts will be made to fetch any recorded transcripts which are then sent to the AI agent. The AI agent summarises and identifies if any follow-on meetings are required. If found, the Agent will use its Calendar Tool to to create the event for the time, date and place for the next meeting as well as add known attendees. Requirements Google Calendar and the ability to fetch Meeting Transcripts (There is a special OAuth permission for this action!) OpenAI account for access to the LLM. Customising the workflow This example only books follow-on meetings but could be extended to generate reports or send emails. Jimleuk | 7 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"bec5c6c1-52d4-4665-b814-56a6bb82ea6b\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[800,660],\"parameters\":{\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"d3e057d1-df44-4ac3-ac46-fc2b04e3de78\",\"name\":\"Get Meeting ConferenceRecords\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[20,580],\"parameters\":{\"url\":\"https://meet.googleapis.com/v2/conferenceRecords\",\"options\":{},\"sendQuery\":true,\"authentication\":\"predefinedCredentialType\",\"queryParameters\":{\"parameters\":[{\"name\":\"filter\",\"value\":\"=space.meeting_code={{ $json.conferenceData.conferenceId }}\"}]},\"nodeCredentialType\":\"googleOAuth2Api\"},\"credentials\":{\"googleOAuth2Api\":{\"id\":\"kgVOfvlBIWTWXthG\",\"name\":\"Google Meets Oauth2 API\"}},\"typeVersion\":4.2},{\"id\":\"831668fd-04ab-4144-bec0-c733902f2a13\",\"name\":\"Get Meeting Transcript Location\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[200,580],\"parameters\":{\"url\":\"=https://meet.googleapis.com/v2/{{ $json.conferenceRecords[0].name }}/transcripts\",\"options\":{},\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"googleOAuth2Api\"},\"credentials\":{\"googleOAuth2Api\":{\"id\":\"kgVOfvlBIWTWXthG\",\"name\":\"Google Meets Oauth2 API\"}},\"typeVersion\":4.2},{\"id\":\"0a1c"}
{"prompt": "Create an n8n workflow for: Chat with OpenAI Assistant (by adding a memory) OpenAI Assistant is a powerful tool, but at the time of writing it doesn't automatically remember past messages from a conversation. This workflow demonstrates how to get around this, by managing the chat history in n8n and passing it to the assistant when required. This makes it possible to use OpenAI Assistant for chatbot use cases. Note that to use this template, you need to be on n8n version 1.28.0 or later. David Roberts | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"087ae6e2-b333-4a30-9010-c78050203961\",\"name\":\"OpenAI Assistant\",\"type\":\"@n8n/n8n-nodes-langchain.openAiAssistant\",\"position\":[1340,460],\"parameters\":{\"text\":\"=## Our Previous Conversation:\\n{{ $json[\\\"messages\\\"].map(m => `\\nHuman: ${m.human}\\nAI Assistant: ${m.ai}\\n`) }}\\n## Current message:\\n{{ $('Chat Trigger').item.json.chatInput }}\",\"options\":{},\"assistantId\":\"asst_HDSAnzsp4WqY4UC1iI9auH5z\"},\"credentials\":{\"openAiApi\":{\"id\":\"VQtv7frm7eLiEDnd\",\"name\":\"OpenAi account 7\"}},\"typeVersion\":1},{\"id\":\"3793b10a-ebb7-42ec-8b9b-7fa3a353d9a3\",\"name\":\"Calculator\",\"type\":\"@n8n/n8n-nodes-langchain.toolCalculator\",\"position\":[1500,640],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7bee2882-bb9e-402e-ba42-9b1ed0e1264b\",\"name\":\"Chat Memory Manager\",\"type\":\"@n8n/n8n-nodes-langchain.memoryManager\",\"position\":[760,460],\"parameters\":{},\"typeVersion\":1,\"alwaysOutputData\":true},{\"id\":\"5c66e482-819e-47e7-90be-779e92364e2a\",\"name\":\"Chat Memory Manager1\",\"type\":\"@n8n/n8n-nodes-langchain.memoryManager\",\"position\":[1720,460],\"parameters\":{\"mode\":\"insert\",\"messages\":{\"messageValues\":[{\"type\":\"user\",\"message\":\"={{ $('Chat Trigger').item.json.chatInput }}\"},{\"type\":\"ai\",\"message\":\"={{ $json.output }}\"}]}},\"typeVersion\":1,\"alwaysOutputData"}
{"prompt": "Create an n8n workflow for: Prompt-based Object Detection with Gemini 2.0 This n8n template demonstrates how to get started with Gemini 2.0's new Bounding Box detection capabilities in your workflows. The key difference being this enables prompt-based object detection for images which is pretty powerful for things like contextual search over an image. eg. \"Put a bounding box around all adults with children in this image\" or \"Put a bounding box around cars parked out of bounds of a parking space\". How it works An image is downloaded via the HTTP node and an \"Edit Image\" node is used to extract the file's width and height. The image is then given to the Gemini 2.0 API to parse and return coordinates of the bounding box of the requested subjects. In this demo, we've asked for the AI to identify all bunnies. The coordinates are then rescaled with the original image's width and height to correctl align them. Finally to measure the accuracy of the object detection, we use the \"Edit Image\" node to draw the bounding boxes onto the original image. How to use Really up to the imagination! Perhaps a form of grounding for evidence based workflows or a higher form of image search can be built. Requirements Google Gemini for LLM Customising the workflow This template is just a demonstration of an experimental version of Gemini 2.0. It is recommended to wait for Gemini 2.0 to come out of this stage before using in production. Jimleuk | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"bae5d407-9210-4bd0-99a3-3637ee893065\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-1440,-280],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c5a14c8e-4aeb-4a4e-b202-f88e837b6efb\",\"name\":\"Get Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[-200,-180],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"b455afe0-2311-4d3f-8751-269624d76cf1\",\"name\":\"coords\",\"type\":\"array\",\"value\":\"={{ $json.candidates[0].content.parts[0].text.parseJson() }}\"},{\"id\":\"92f09465-9a0b-443c-aa72-6d208e4df39c\",\"name\":\"width\",\"type\":\"string\",\"value\":\"={{ $('Get Image Info').item.json.size.width }}\"},{\"id\":\"da98ce2a-4600-46a6-b4cb-159ea515cb50\",\"name\":\"height\",\"type\":\"string\",\"value\":\"={{ $('Get Image Info').item.json.size.height }}\"}]}},\"typeVersion\":3.4},{\"id\":\"f24017c9-05bc-4f75-a18c-29efe99bfe0e\",\"name\":\"Get Test Image\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1260,-280],\"parameters\":{\"url\":\"https://www.stonhambarns.co.uk/wp-content/uploads/jennys-ark-petting-zoo-for-website-6.jpg\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"c0f6a9f7-ba65-48a3-8752-ce5d80fe33cf\",\"name\":\"Gemini 2.0 Object Detection\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-680,-180],\"parameters\":{\"url\":\"https://generativelanguage.googleapis."}
{"prompt": "Create an n8n workflow for: Sentiment Analysis Tracking on Support Issues with Linear and Slack This n8n template monitors active support issues in Linear.app to track the mood of their ongoing conversation between reporter and assignee using Sentiment Analysis. When sentiment dips into the negative, a notification is sent via Slack to alert the team. How it works A scheduled trigger is used to fetch recently updated issues in Linear using the GraphQL node. Each issue's comments thread is passed into a simple Information Extractor node to identify the overall sentiment. The resulting sentiment analysis combined with the some issue details are uploaded to Airtable for review. When the template is re-run at a later date, each issue is re-analysed for sentiment Each issue's new sentiment state is saved to the airtable whilst its previous state is moved to the \"previous sentiment\" column. An Airtable trigger is used to watch for recently updated rows Each matching Airtable row is filtered to check if it has a previous non-negative state but now has a negative state in its current sentiment. The results are sent via notification to a team slack channel for priority. Check out the sample Airtable here: https://airtable.com/appViDaeaFw4qv9La/shrq6HgeYzpW6uwXL How to use Modify the GraphQL filter to fetch issues to a relevant issue type, team or person. Update the Slack channel to ensure messages are sent to the correct location or persons. The Airtable also serves to give a snapshot of Sentiment across support tickets for a given period. It's possible to use this to assess the daily operations. Requirements Linear for issue tracking (but feel free to use another system if preferred) Airtable for Database OpenAI for LLM and Sentiment Analysis Customising the workflow Add more granular levels of sentiment to reduce the number of alerts. Explore different types of sentiment based on issue types and customer types. This may help prioritise alerts and response. Run across teams or categories of issues to get an overview of sentiment across the support organisation. Jimleuk | 2 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"82fd6023-2cc3-416e-83b7-fda24d07d77a\",\"name\":\"Issues to List\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[40,-100],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"data.issues.nodes\"},\"typeVersion\":1},{\"id\":\"9cc77786-e14f-47c6-a3cf-60c2830612e6\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[360,80],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"821d4a60-81a4-4915-9c13-3d978cc0114b\",\"name\":\"Combine Sentiment Analysis\",\"type\":\"n8n-nodes-base.set\",\"position\":[700,-80],\"parameters\":{\"mode\":\"raw\",\"options\":{},\"jsonOutput\":\"={{\\n{\\n ...$('Issues to List').item.json,\\n ...$json.output\\n}\\n}}\"},\"typeVersion\":3.4},{\"id\":\"fe6560f6-2e1b-4442-a2af-bd5a1623f213\",\"name\":\"Sentiment over Issue Comments\",\"type\":\"@n8n/n8n-nodes-langchain.informationExtractor\",\"position\":[360,-80],\"parameters\":{\"text\":\"={{\\n$json.comments.nodes.map(node => [\\n `${node.user.displayName} commented on ${node.createdAt}:`,\\n node.body\\n].join('\\\\n')).join('---\\\\n')\\n}}\",\"options\":{},\"attributes\":{\"attributes\":[{\"name\":\"sentiment\",\"required\":true,\"description\":\"One of positive, negative or neutral\"},{\"name\":\"sentimentSummary\",\"description\":\"Describe the sentiment of the conversation\"}]}},\"typeVersion\":1},{\"id\":\"4fd0345d-e5bf-426d-8403-e2217e19bbea\",\"name\":\"Copy of Issue\",\""}
{"prompt": "Create an n8n workflow for: Email Subscription Service with n8n Forms, Airtable and AI This n8n template shows how anyone can build a simple newsletter-like subscription service where users can enrol themselves to receive messages/content on a regular basis. It uses n8n forms for data capture, Airtable for database, AI for content generation and Gmail for email sending. How it works An n8n form is setup up to allow users to subscribe with a desired topic and interval of which to recieve messages via n8n forms which is then added to the Airtable. A scheduled trigger is executed every morning and searches for subscribers to send messages for based on their desired intervals. Once found, Subscribers are sent to a subworkflow which performs the text content generation via an AI agent and also uses a vision model to generate an image. Both are attached to an email which is sent to the subscriber. This email also includes an unsubscribe link. The unsubscribe flow works similarly via n8n form interface which when submitted disables further scheduled emails to the user. How to use Make a copy of sample Airtable here: https://airtable.com/appL3dptT6ZTSzY9v/shrLukHafy5bwDRfD Make sure the workflow is \"activated\" and the forms are available and reachable by your audience. Requirements Airtable for Database OpenAI for LLM (but compatible with others) Gmail for Email (but can be replaced with others) Customising this workflow This simple use can be extended to deliver any types of content such as your company newsletter, promotions, social media posts etc. Doesn't have to be limited to just email - try social messaging, Whatsapp, Telegram and others. Jimleuk | 2 months ago | Free +4", "completion": "{\"nodes\":[{\"id\":\"4dd52c72-9a9b-4db4-8de5-5b12b1e5c4be\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[180,1480],\"parameters\":{\"rule\":{\"interval\":[{\"triggerAtHour\":9}]}},\"typeVersion\":1.2},{\"id\":\"9226181c-b84c-4ea1-a5b4-eedb6c62037b\",\"name\":\"Search daily\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[440,1480],\"parameters\":{\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appL3dptT6ZTSzY9v\",\"cachedResultUrl\":\"https://airtable.com/appL3dptT6ZTSzY9v\",\"cachedResultName\":\"Scheduled Emails\"},\"table\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"tblzR9vSuFUzlQNMI\",\"cachedResultUrl\":\"https://airtable.com/appL3dptT6ZTSzY9v/tblzR9vSuFUzlQNMI\",\"cachedResultName\":\"Table 1\"},\"options\":{},\"operation\":\"search\",\"filterByFormula\":\"AND({Status} = 'active', {Interval} = 'daily')\"},\"credentials\":{\"airtableTokenApi\":{\"id\":\"Und0frCQ6SNVX3VV\",\"name\":\"Airtable Personal Access Token account\"}},\"typeVersion\":2.1},{\"id\":\"1a3b6224-2f66-41c6-8b3d-be286cf16370\",\"name\":\"Search weekly\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[440,1660],\"parameters\":{\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appL3dptT6ZTSzY9v\",\"cachedResultUrl\":\"https://airtable.com/appL3dptT6ZTSzY9v\",\"cachedResultName\":\"Scheduled Emails\"},\"table\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"tblzR9vSuFUzlQNMI\",\"cachedResultUrl\":\"https://airtable.com/app"}
{"prompt": "Create an n8n workflow for: Send Google analytics data to A.I. to analyze then save results in Baserow Who's this for? If you own a website and need to analyze your Google analytics data If you need to create an SEO report on which pages are getting most traffic or how your google search terms are performing If you want to grow your site based on suggestions from data Use case Instead of hiring an SEO expert, I run this report weekly. It checks compares the data from this week to the week before: Views based on countries The top performing pages Google search console performance Read my detailed case study here How it works The workflow gathers google analytics for the past 7 days then it gathers the data for the week before for comparison. It does this 3 times to get: views per country, engagement per page and google search console results for organic search results. The google analytics nodes has already chosen the correct dimensions and metrics. At the end, it passes the data to openrouter.ai for A.I. analyse. Finally it saves to baserow. How to use this Input your Google analytics credentials Input your property ID Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Name, Country Views, Page Views, Search Report, Blog (name of your blog). Created by Rumjahn Keith Rumjahn | 4 months ago | Free", "completion": "{\"id\":\"K3uf8aY8wipScEay\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Google analytics template\",\"tags\":[],\"nodes\":[{\"id\":\"6a9fc442-d0a3-48be-8dff-94f8d9cd5cf1\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[460,460],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\"}]}},\"typeVersion\":1.2},{\"id\":\"484cbc41-f57d-4c3d-a458-e439d480d290\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[460,640],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b1b66e9b-5fea-407b-9c1e-39bd2a9d4a90\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[460,100],\"parameters\":{\"width\":714.172987012987,\"content\":\"## Send Google analytics to A.I. and save results to baserow\\n\\nThis workflow will check for country views, page engagement and google search console results. It will take this week's data and compare it to last week's data.\\n\\n[You can read more about this workflow here](https://rumjahn.com/how-i-used-a-i-to-be-an-seo-expert-and-analyzed-my-google-analytics-data-in-n8n-and-make-com/)\"},\"typeVersion\":1},{\"id\":\"adde29fc-ddb5-4b50-aa78-313ac9ede879\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[633.6540259740264,320],\"parameters\":{\"color\":4,\"width\":2097.92831168831,\"height\":"}
{"prompt": "Create an n8n workflow for: Summarize your emails with A.I. (via Openrouter) and send to Line messenger Who is this template for? Anyone who is drowning in emails Busy parents who has alot of school emails Busy executives with too many emails Case Study I get too many emails from my kid's school about soccer practice, lunch orders and parent events. I use this workflow to read all the emails and tell me what is important and what requires actioning. Read more -&gt; How I used A.I. to read all my emails What this workflow does It uses IMAP to read the emails from your email account (i.e. Gmail). It then passes the email to Openrouter.ai and uses a free A.I. model to read and summarize the email. It then sends the summary as a message to your messenger (i.e. Line). Setup You need to find your email server IMAP credentials. Input your openrouter.ai API credentials or replace the HTTP request node with an A.I. node such as OpenAI. Input your messenger credentials. I use Line but you can change the node to another messenger line Telegram. You need to change the message ID to your ID inside the http request. You can find your user ID inside the https://developers.line.biz/console/. Change the \"to\": {insert your user ID}. How to adjust it to your needs You can change the A.I. prompt to fit your needs by telling it to mark emails from a certain address as important. You can change the A.I. model from the current meta-llama/llama-3.1-70b-instruct:free to a paid model or other free models. You can change the messenger node to telegram or any other messenger app you like. Keith Rumjahn | 4 months ago | Free", "completion": "{\"id\":\"QnVdtKiTf3nbrNkh\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Summarize emails with A.I. then send to messenger\",\"tags\":[],\"nodes\":[{\"id\":\"50e12e63-df28-45ac-9208-48cbf5116d09\",\"name\":\"Read emails (IMAP)\",\"type\":\"n8n-nodes-base.emailReadImap\",\"position\":[340,260],\"parameters\":{\"options\":{},\"postProcessAction\":\"nothing\"},\"credentials\":{\"imap\":{\"id\":\"gXtdakU9M02LBQc3\",\"name\":\"IMAP account\"}},\"typeVersion\":2},{\"id\":\"6565350b-2269-44e3-8f36-8797f32d3e09\",\"name\":\"Send email to A.I. to summarize\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[700,260],\"parameters\":{\"url\":\"https://openrouter.ai/api/v1/chat/completions\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"model\\\": \\\"meta-llama/llama-3.1-70b-instruct:free\\\",\\n \\\"messages\\\": [\\n {\\n \\\"role\\\": \\\"user\\\",\\n \\\"content\\\": \\\"I want you to read and summarize all the emails. If it's not rimportant, just give me a short summary with less than 10 words.\\\\n\\\\nHighlight as important if it is, add an emoji to indicate it is urgent:\\\\nFor the relevant content, find any action items and deadlines. Sometimes I need to sign up before a certain date or pay before a certain date, please highlight that in the summary for me.\\\\n\\\\nPut the deadline in BOLD at the top. If the email is not important, keep the summary short to 1 sentence only.\\\\n\\\\nHere's the email content for you to read:\\\\nSender email address: {{ encodeURIComponent($json.from) }}\\\\nSubject: {{ encodeURIComponent($json.subject) }}\\\\n{{"}
{"prompt": "Create an n8n workflow for: Scale Deal Flow with a Pitch Deck AI Vision, Chatbot and QDrant Vector Store Are you a popular tech startup accelerator (named after a particular higher order function) overwhelmed with 1000s of pitch decks on a daily basis? Wish you could filter through them quickly using AI but the decks are unparseable through conventional means? Then you're in luck! This n8n template uses Multimodal LLMs to parse and extract valuable data from even the most overly designed pitch decks in quick fashion. Not only that, it'll also create the foundations of a RAG chatbot at the end so you or your colleagues can drill down into the details if needed. With this template, you'll scale your capacity to find interesting companies you'd otherwise miss! Requires n8n v1.62.1+ How It Works Airtable is used as the pitch deck database and PDF decks are downloaded from it. An AI Vision model is used to transcribe each page of the pitch deck into markdown. An Information Extractor is used to generate a report from the transcribed markdown and update required information back into pitch deck database. The transcribed markdown is also uploaded to a vector store to build an AI chatbot which can be used to ask questions on the pitch deck. Check out the sample Airtable here: https://airtable.com/appCkqc2jc3MoVqDO/shrS21vGqlnqzzNUc How To Use This template depends on the availability of the Airtable - make a duplicate of the airtable (link) and its columns before running the workflow. When a new pitchdeck is received, enter the company name into the Name column and upload the pdf into the File column. Leave all other columns blank. If you have the Airtable trigger active, the execution should start immediately once the file is uploaded. Otherwise, click the manual test trigger to start the workflow. When manually triggered, all \"new\" pitch decks will be handled by the workflow as separate executions. Requirements OpenAI for LLM Airtable For Database and Interface Qdrant for Vector Store Customising This Workflow Extend this starter template by adding more AI agents to validate claims made in the pitch deck eg. Linkedin Profiles, Page visits, Reviews etc. Jimleuk | 4 months ago | Free +10", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"9ce4eadf-7eef-43bd-bbe9-e25bc5a42df7\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-1076,594],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b8d12c00-4004-44b4-b793-e9608fd36d5d\",\"name\":\"Sort Pages\",\"type\":\"n8n-nodes-base.sort\",\"position\":[1440,777],\"parameters\":{\"options\":{},\"sortFieldsUi\":{\"sortField\":[{\"fieldName\":\"fileName\"}]}},\"typeVersion\":1},{\"id\":\"27520282-af95-415e-a3d3-3cf9e4373813\",\"name\":\"Split PDF into Images\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[900,777],\"parameters\":{\"url\":\"http://stirlingpdf.io/api/v1/convert/pdf/img\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"contentType\":\"multipart-form-data\",\"bodyParameters\":{\"parameters\":[{\"name\":\"fileInput\",\"parameterType\":\"formBinaryData\",\"inputDataFieldName\":\"data\"},{\"name\":\"imageFormat\",\"value\":\"jpg\"},{\"name\":\"singleOrMultiple\",\"value\":\"multiple\"},{\"name\":\"dpi\",\"value\":\"300\"}]}},\"typeVersion\":4.2},{\"id\":\"e3862292-3261-4876-b53e-acea88810afb\",\"name\":\"Extract Zip File\",\"type\":\"n8n-nodes-base.compression\",\"position\":[1080,777],\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"2d949fb6-980f-409a-9b71-bf12927eaa6d\",\"name\":\"Images To List\",\"type\":\"n8n-nodes-base.code\",\"position\":[1260,777],\"parameters\":{\"jsCode\":\"let results = [];\\n\\nfor (item of items) {\\n for (key of Object.keys(item"}
{"prompt": "Create an n8n workflow for: Convert image to text using GROQ LLaVA V1.5 7B What this template does This template uses GROQ LLAVA V1.5 7B API that offers fast inference for multimodal models with vision capabilities for understanding and interpreting visual data from images. . The users send a image and get a description of the image from the model. Setup Open the Telegram app and search for the BotFather user (@BotFather) Start a chat with the BotFather Type /newbot to create a new bot Follow the prompts to name your bot and get a unique API token Save your access token and username Once you set your bot, you can send the image, and get the descriptions. felipe biava cataneo | 5 months ago | Free", "completion": "{\"id\":\"aDPpPIaeM7zfUCdJ\",\"meta\":{\"instanceId\":\"e5595d8cd58f3a24b5a8cf05dd852846c05423873db868a2b7d01a778210c45a\",\"templateCredsSetupCompleted\":true},\"name\":\"GROQ LLAVA V1.5 7B\",\"tags\":[],\"nodes\":[{\"id\":\"d831f75e-0385-482c-b2d5-e8da58216f4c\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[540,280],\"webhookId\":\"6021108f-f0e8-4d7a-918b-f49baa28ba85\",\"parameters\":{\"updates\":[\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"JLNFgAyYUUyvLhcv\",\"name\":\"Bot 2\"}},\"typeVersion\":1.1},{\"id\":\"0fd97c36-3e3d-45a3-929d-975d17baf1fb\",\"name\":\"Telegram send the text\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1640,280],\"parameters\":{\"text\":\"={{ $json.choices[0].message.content }}\",\"chatId\":\"={{ $('Telegram Trigger').item.json.message.chat.id }}\",\"additionalFields\":{\"appendAttribution\":false}},\"credentials\":{\"telegramApi\":{\"id\":\"\",\"name\":\"\"}},\"typeVersion\":1.2},{\"id\":\"bd39b29f-e128-4891-bc6a-3eb75de29182\",\"name\":\"Get only the text\",\"type\":\"n8n-nodes-base.set\",\"position\":[1420,280],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"52a2f0d9-3137-4f6e-a2c1-8285694f6159\",\"name\":\"choices[0].message.content\",\"type\":\"string\",\"value\":\"={{ $json.choices[0].message.content }}\"}]}},\"typeVersion\":3.4},{\"id\":\"f1a960"}
{"prompt": "Create an n8n workflow for: Enrich FAQ sections on your website pages at scale with AI This n8n workflow template lets you easily generate comprehensive FAQ (Frequently Asked Questions) content for multiple services (or any items or pages you need to add the FAQs to). Simply provide the Google Sheets document containing the items to scrape, and the workflow automatically creates detailed, AI-enhanced FAQ documents. How it works The workflow reads data from a Google Sheets document containing information about different services and categories (again, in your case - whatever objects you need). For each service and category, it generates a set of standard questions and answers covering setup, permissions, integrations, use cases, and pricing benefits. An AI model (OpenAI's GPT) is used to enhance or complete some of the answers, making the content more comprehensive and natural-sounding. The workflow formats the Q&A pairs, combining AI-generated content with predefined answers where applicable. It creates a text file (JSON) for each service or category, containing the formatted Q&A pairs. The generated files are saved to specific folders in Google Drive, organized by the type of integration (native, credential-only, non-native) or category. After processing each service or category, it updates the status in the original Google Sheets document to mark it as completed. Ideal for: Marketing teams: Rapidly create comprehensive FAQ documents for multiple products or services. Customer support: Generate consistent and detailed answers for common customer queries. Product managers: Easily maintain up-to-date documentation as products evolve. Content creators: Streamline the process of creating informative content about various offerings. Accounts required Google account (for Google Sheets and Google Drive) OpenAI API account (for AI-enhanced content generation) n8n.io account (for workflow execution) Set up instructions Set up the required credentials for Google Sheets, Google Drive, and OpenAI when you first open the workflow. Prepare your Google Sheets document with the service/category information. Here's an example of Google Sheet. Fill the \"Define Sheets\" node with your sheets Adjust the folder IDs in the \"Prepare Job\" node to match your Google Drive structure. Configure the OpenAI model settings in the \"OpenAI Chat Model\" node if needed. Test the workflow with a small subset of data before running it on your entire dataset. Adjust the questions asked in the \"Create your Q&A templates\" section After testing, activate your workflow for automated FAQ generation. \ud83d\ude4f Big, big kudos to Jim Le for his ideas, input and support when building this workflow. Your approach to AI workflows is always super helpful! Polina Medvedieva | 5 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"ff412ab2a6cd55af5dedbbab9b8e43f0f3a0cb16fb794fa8d3837f957b771ad2\"},\"nodes\":[{\"id\":\"9c3c06eb-8b48-4229-9b16-7fe7c4f886c3\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[78.44447107090468,520],\"parameters\":{},\"typeVersion\":1},{\"id\":\"2a8d8297-18de-4e1f-b44b-93842f7c1709\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1678.4444710709047,2020],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"typeVersion\":1},{\"id\":\"a6c24857-ad3b-4561-b40b-8520064e861b\",\"name\":\"Format QA Pair1\",\"type\":\"n8n-nodes-base.set\",\"position\":[2018.4444710709047,1880],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"2c1bd408-29f0-487b-9a33-7513d5bbfe23\",\"name\":\"question\",\"type\":\"string\",\"value\":\"={{ $('Needs AI Completion?1').item.json.question }}\"},{\"id\":\"02ffc3b7-3d77-4dfe-ba3f-2052f5cc9e83\",\"name\":\"answer\",\"type\":\"string\",\"value\":\"={{\\n[\\n $('Needs AI Completion?1').item.json.answer,\\n $json.text\\n ? $json.text[0].toLowerCase() + $json.text.substring(1, $json.text.length)\\n : '',\\n $('Needs AI Completion?1').item.json.append || '',\\n].join(' ').trim()\\n}}\"}]}},\"typeVersion\":3.4},{\"id\":\"2b"}
{"prompt": "Create an n8n workflow for: Easy Image Captioning with Gemini 1.5 Pro This n8n workflow demonstrates how to automate image captioning tasks using Gemini 1.5 Pro - a multimodal LLM which can accept and analyse images. This is a really simple example of how easy it is to build and leverage powerful AI models in your repetitive tasks. How it works For this demo, we'll import a public image from a popular stock photography website, Pexel.com, into our workflow using the HTTP request node. With multimodal LLMs, there is little do preprocess other than ensuring the image dimensions fit within the LLMs accepted limits. Though not essential, we'll resize the image using the Edit image node to achieve fast processing. The image is used as an input to the basic LLM node by defining a \"user message\" entry with the binary (data) type. The LLM node has the Gemini 1.5 Pro language model attached and we'll prompt it to generate a caption title and text appropriate for the image it sees. Once generated, the generated caption text is positioning over the original image to complete the task. We can calculate the positioning relative to the amount of characters produced using the code node. An example of the combined image and caption can be found here: https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/l5xbb4ze4wyxwwefqmnc Requirements Google Gemini API Key. Access to Google Drive. Customising the workflow Not using Google Gemini? n8n's basic LLM node supports the standard syntax for image content for models that support it - try using GPT4o, Claude or LLava (via Ollama). Google Drive is only used for demonstration purposes. Feel free to swap this out for other triggers such as webhooks to fit your use case. Jimleuk | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"0b64edf1-57e0-4704-b78c-c8ab2b91f74d\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[480,300],\"parameters\":{},\"typeVersion\":1},{\"id\":\"a875d1c5-ccfe-4bbf-b429-56a42b0ca778\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[1280,720],\"parameters\":{\"options\":{},\"modelName\":\"models/gemini-1.5-flash\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"dSxo6ns5wn658r8N\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"a5e00543-dbaa-4e62-afb0-825ebefae3f3\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1480,720],\"parameters\":{\"jsonSchemaExample\":\"{\\n\\t\\\"caption_title\\\": \\\"\\\",\\n\\t\\\"caption_text\\\": \\\"\\\"\\n}\"},\"typeVersion\":1.2},{\"id\":\"bb9af9c6-6c81-4e92-a29f-18ab3afbe327\",\"name\":\"Get Info\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[1100,400],\"parameters\":{\"operation\":\"information\"},\"typeVersion\":1},{\"id\":\"8a0dbd5d-5886-484a-80a0-486f349a9856\",\"name\":\"Resize For AI\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[1100,560],\"parameters\":{\"width\":512,\"height\":512,\"options\":{},\"operation\":\"resize\"},\"typeVersion\":1},{\"id\":\"d29f254a-5fa3"}
{"prompt": "Create an n8n workflow for: Generate audio from text using OpenAI and Webhook | Text to Speech Workflow Who is this for? This workflow is ideal for developers, content creators, or customer support teams looking to automate text-to-speech conversion using OpenAI. What problem does this solve? It automates the process of converting text inputs into speech, reducing manual effort and enhancing productivity. What this workflow does: This workflow triggers when a text input is received via a webhook, converts it into audio using the OpenAI API, and sends the generated speech back through a webhook response. Setup: Ensure you have an OpenAI API key (you can get it from OpenAI website). Set up the webhook URL and parameters. Configure the OpenAI node with your API key (Create New Credentials). set up the responde to webhook node. Ayoub | 6 months ago | Free", "completion": "{\"id\":\"OVSyGmI6YFviPu8Q\",\"meta\":{\"instanceId\":\"fb261afc5089eae952e09babdadd9983000b3d863639802f6ded8c5be2e40067\",\"templateCredsSetupCompleted\":true},\"name\":\"Generate audio from text using OpenAI - text-to-speech Workflow\",\"tags\":[],\"nodes\":[{\"id\":\"c40966a4-1709-4998-ae95-b067ce3496c9\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1320,200],\"parameters\":{\"options\":{},\"respondWith\":\"binary\"},\"typeVersion\":1.1},{\"id\":\"c4e57bb6-79a4-4b26-a179-73e30d681521\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[280,-140],\"parameters\":{\"width\":501.55,\"height\":493.060000000001,\"content\":\"This `Webhook` node triggers the workflow when it receives a POST request.\\n\\n### 1. Test Mode:\\n* Use the test webhook URL\\n* Click the `Test workflow` button on the canvas. (In test mode, the webhook only works for one call after you click this button)\\n\\n### 1. Production Mode:\\n* The workflow must be active for a **Production URL** to run successfully.\\n* You can activate the workflow using the toggle in the top-right of the editor.\\n* Note that unlike test URL calls, production URL calls aren't shown on the canvas (only in the executions list).\"},\"typeVersion\":1},{\"id\":\"1364a4b6-2651-4b38-b335-c36783a25f12\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[825,60],\"parameters\":{\"color\":4,\"width\":388.35000000000025,\"height\":292.71000000000043,\"content\":\"### Configure the OpenAI node with your API key:\\nIf you haven't connected your OpenAI credentials in n8n yet, log in to your OpenAI account to get your API Key. Then"}
{"prompt": "Create an n8n workflow for: Turn Emails into AI-Enhanced Tasks in Notion (Multi-User Support) with Gmail, Airtable and Softr Purpose This workflow automatically creates Tasks from forwarded Emails, similar to Asana, but better. Emails are processed by AI and converted to rather actionable task. In addition this workflow is build in a way, that multiple users can share this single process by setting up their individual configuration through a user friendly portal (internal tool) instead of the need to manage their own workflows. Demo How it works One Gmail account is used to process inbound mails from different users. A custom web portal enables users to define \u201croutes\u201d. Thats where the mapping between an automatically generated Gmail Alias and a Notion Database URL, including the personal API Token, happens. Using a Gmail Trigger, new entries are split by the Email Alias, so the corresponding route can be retrieved from the Database connected to the portal. Every Email then gets processed by AI to get generate an actionable task and get a short summary of the original Email as well as some metadata. Based on a predefined structure a new Page is created in the corresponding Notion Database. Finally the Email is marked as \u201cprocessed\u201d in Gmail. If an error happens, the route gets paused for a possible overflow and the user gets notified by Email. Setup Create a new Google account (alternatively you can use an existing one and set up rules to keep your inbox organized) Create two Labels in Gmail: \u201cProcessed\u201d and \u201cError\u201d Clone this Softr template including the Airtable dataset and publish the application Clone this workflow and choose credentials (Gmail, Airtable) Follow the additional instructions provided within the workflow notes Enable the workflow, so it runs automatically in the background How to use Open published Softr application Register as a new user Create a new route containing the Notion API key and the Notion Database URL Expand the new entry to copy the Email address Save the address as a new contact in your Email provider of choice Forward an Email to it and watch how it gets converted to an actionable task Disclamer Airtable was chosen, so you can setup this template fairly quickly. It is advised to replace the persistence by something you own, like a self hosted SQL server, since we are dealing with sensitive information of multiple users This solution is only meant for building internal tools, unless you own an embed license for n8n. Mario | 6 months ago | Free +3", "completion": "{\"id\":\"30r9acI1XVIIwAMi\",\"meta\":{\"instanceId\":\"378c072a34d9e63949fd9cf26b8d28ff276a486e303f0d8963f23e1d74169c1b\",\"templateCredsSetupCompleted\":true},\"name\":\"mails2notion V2\",\"tags\":[],\"nodes\":[{\"id\":\"3f649e97-e568-47ff-b175-bf63d859d95f\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2560,240],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0,\"responseFormat\":\"json_object\"}},\"credentials\":{\"openAiApi\":{\"id\":\"mrgqM64cM1L88xC6\",\"name\":\"octionicsolutions@gmail.com\"}},\"typeVersion\":1},{\"id\":\"bd60c65f-ba6c-4dcb-8d09-b29f5dd475b7\",\"name\":\"Calculator\",\"type\":\"@n8n/n8n-nodes-langchain.toolCalculator\",\"disabled\":true,\"position\":[2700,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"d052786a-92a0-4f9b-9867-2dd64ada8034\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[2820,240],\"parameters\":{\"jsonSchemaExample\":\"{\\n \\\"summary\\\": \\\"Text\\\",\\n \\\"meta\\\": {\\n \\\"sender\\\": \\\"Text\\\",\\n \\\"subject\\\": \\\"Text\\\",\\n \\\"date\\\": \\\"Text\\\"\\n }\\n}\"},\"typeVersion\":1.2},{\"id\":\"50d396fd-d3b0-4fea-99d7-18bd4773cb20\",\"name\":\"Add Label \\\"Processed\\\"\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[3860,20],\"parameters\":{\"labelIds\":\"={{ $('Globals').item.json.processedLabelID }}\",\"messageId\":\"={{ $('Gmail Trigger"}
{"prompt": "Create an n8n workflow for: Creating a AI Slack Bot with Google Gemini This is an example of how we can build a slack bot in a few easy steps Before you can start, you need to o a few things Create a copy of this workflow Create a slack bot Create a slash command on slack and paste the webhook url to the slack command Note Make sure to configure this webhook using a https:// wrapper and don't use the default http://localhost:5678 as that will not be recognized by your slack webhook. Once the data has been sent to your webhook, the next step will be passing it via an AI Agent to process data based on the queries we pass to our agent. To have some sort of a memory, be sure to set the slack token to the memory node. This way you can refer to other chats from the history. The final message is relayed back to slack as a new message. Since we can not wait longer than 3000 ms for slack response, we will create a new message with reference to the input we passed. We can advance this using the tools or data sources for it to be more custom tailored for your company. Usage To use the slackbot, go to slack and click on your set slash command eg /Bob and send your desired message. This will send the message to your endpoint and get return the processed results as the message. If you would like help setting this up, feel free to reach out to zacharia@effibotics.com Imperol | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\",\"templateId\":\"2370\"},\"nodes\":[{\"id\":\"2ce91ec6-0a8c-438a-8a18-216001c9ee07\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"width\":407.6388140161723,\"height\":490.24769122000794,\"content\":\"## This is a POST Webhook endpoint\\n\\nMake sure to configure this webhook using a https:// wraper and dont use the default http://localhost:5678 as that will not be recognized by your slack webhook\\n\\n\\nOnce the data has been sent to your webhook, the next step will be passing it via an AI Agent to process data based on the queries we pass to our agent.\\n\\nTo have some sort of a memory, be sure to set the slack token to the memory node. This way you can refer to other chats from the history.\\n\\nThe final message is relayed back to slack as a new message. Since we can not wait longer than 3000 ms for slack response, we will create anew message with reference to the input we passed.\\n\\nWe can advance this using the tools or data sources for it to be more custom tailored for your company.\\n\"},\"typeVersion\":1},{\"id\":\"7a0c84a8-90ef-4de8-b120-700c94c35a51\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1180,560],\"parameters\":{\"color\":4,\"width\":221.73584905660368,\"height\":233,\"content\":\"### Conversation history is stored in memory using the body token as the chatsession id\"},\"typeVersion\":1},{\"id\":\"9b843e0e-42a6-4125-8c59-a7d5620a15f7\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[942.5229110512129,560],\"parameters\":{\"color\":4"}
{"prompt": "Create an n8n workflow for: Handling Appointment Leads and Follow-up With Twilio, Cal.com and AI This n8n workflow builds an appointment scheduling AI agent which can Take enquiries from prospective customers and help them book an appointment by checking appointment availability Where no appointment is booked, the Agent is able to send follow-up messages to re-engage leads. After an appointment is booked, the agent is able reschedule or even cancel the booking for the user without human intervention. For small outfits, this workflow could contribute the necessary \"man-power\" required to increase business sales. The sample Airtable can be found here: https://airtable.com/appO2nHiT9XPuGrjN/shroSFT2yjf87XAox 2024-10-22 Updated to Cal.com API v2. How it works The customer sends an enquiry via SMS to trigger our workflow. For this trigger, we'll use a Twilio webhook. The prospective or existing customer's number is logged in an Airtable Base which we'll be using to track all our enquries. Next, the message is sent to our AI Agent who can reply to the user and decide if an appointment booking can be made. The reply is made via SMS using Twilio. A scheduled trigger which runs every day, checks our chat logs for a list of prospective customers who have yet to book an appointment but still show interest. This list is sent to our AI Agent to formulate a personalised follow-up message to each lead and ask them if they want to continue with the booking. The follow-up interaction is logged so as to not to send too many messages to the customer. Requirements A Twilio account to receive customer messages. An Airtable account and Base to use as our datastore for enquiries. Cal.com account to use as our scheduling service. OpenAI account for our AI model. Customising this workflow Not using Airtable? Swap this out for your CRM of choice such as hubspot or your own service. Not using Cal.com? Swap this out for API-enabled services such as Acuity Scheduling or your own service. Jimleuk | 7 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"f55b3110-f960-4d89-afba-d47bc58102eb\",\"name\":\"Twilio Trigger\",\"type\":\"n8n-nodes-base.twilioTrigger\",\"position\":[100,180],\"webhookId\":\"bfc8f587-8183-46f8-9e76-3576caddf8c0\",\"parameters\":{\"updates\":[\"com.twilio.messaging.inbound-message.received\"]},\"credentials\":{\"twilioApi\":{\"id\":\"TJv4H4lXxPCLZT50\",\"name\":\"Twilio account\"}},\"typeVersion\":1},{\"id\":\"8472f5b0-329f-45ac-b35f-c42558daa7c7\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1140,1360],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"4b3e8a26-c808-46e5-bbcf-2e1279989a0b\",\"name\":\"Find Follow-Up Candidates\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[720,1240],\"parameters\":{\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appO2nHiT9XPuGrjN\",\"cachedResultUrl\":\"https://airtable.com/appO2nHiT9XPuGrjN\",\"cachedResultName\":\"Twilio-Scheduling-Agent\"},\"table\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"tblokH7uw63RpIlQ0\",\"cachedResultUrl\":\"https://airtable.com/appO2nHiT9XPuGrjN/tblokH7uw63RpIlQ0\",\"cachedResultName\":\"Lead Tracker\"},\"options\":"}
{"prompt": "Create an n8n workflow for: Breakdown Documents into Study Notes using Templating MistralAI and Qdrant This n8n workflow takes in a document such as a research paper, marketing or sales deck or company filings, and breaks them down into 3 templates: study guide, briefing doc and timeline. These templates are designed to help a student, associate or clerk quickly summarise, learn and understand the contents to be more productive. Study guide - a short quiz of questions and answered generated by the AI Agent using the contents of the document. Briefing Doc - key information and insights are extracted by the AI into a digestable form. Timeline - key events, durations and people are identified and listed into a simple to understand timeline by the AI How it works A local file trigger watches a local network directory for new documents. New documents are imported into the workflow, its contents extracted and vectorised into a Qdrant vector store to build a mini-knowledgebase. The document then passes through a series of template generating prompts where the AI will perform \"research\" on the knowledgebase to generate the template contents. Generated study guide, briefing and timeline documents are exported to a designated folder for the user. Requirements Self-hosted version of n8n. Qdrant instance for knowledgebase. Mistral.ai account for embeddings and AI model. Customising your workflow Try adding your own templates or adjusting the existing templates to suit your unique use-case. Anything is quite possible and limited only by your imagination! Want to go fully local? A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/1VV5R2nW-IhVcFP_k8uEks4LsLRZrHSNG/view?usp=sharing Jimleuk | 7 months ago | Free +5", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"a3af309b-d24c-42fe-8bcd-f330927c7a3c\",\"name\":\"Local File Trigger\",\"type\":\"n8n-nodes-base.localFileTrigger\",\"position\":[140,260],\"parameters\":{\"path\":\"/home/node/storynotes/context\",\"events\":[\"add\"],\"options\":{\"usePolling\":true,\"followSymlinks\":true},\"triggerOn\":\"folder\"},\"typeVersion\":1},{\"id\":\"048f9d67-6519-4dea-97df-aaddfefbfea2\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[1300,720],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"project\",\"value\":\"={{ $('Settings').item.json.project }}\"},{\"name\":\"filename\",\"value\":\"={{ $('Settings').item.json.filename }}\"}]}},\"jsonData\":\"={{ $json.data }}\",\"jsonMode\":\"expressionData\"},\"typeVersion\":1},{\"id\":\"9e9047c9-4428-4afb-8c74-d6eb1075a65a\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[1300,860],\"parameters\":{\"options\":{},\"chunkSize\":2000},\"typeVersion\":1},{\"id\":\"e42e3f82-6cd9-40c4-9da2-8f87ee5b3956\",\"name\":\"Embeddings Mistral Cloud\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsMistralCloud\",\"position\":[1180,720],\"parameters\":{\"options\":{}},\"credentials\":{\"mistralCloudApi\":{\"id\":\"EIl2QxhXAS9Hkg37\",\"name\":\"Mistral Cloud account\"}},\"typeVersion\":1},{\"id\":\"578c63db-4f6e-4341-ab0d-111debd519be"}
{"prompt": "Create an n8n workflow for: Build a Financial Documents Assistant using Qdrant and Mistral.ai This n8n workflow demonstrates how to manage your Qdrant vector store when there is a need to keep it in sync with local files. It covers creating, updating and deleting vector store records ensuring our chatbot assistant is never outdated or misleading. Disclaimer This workflow depends on local files accessed through the local filesystem and so will only work on a self-hosted version of n8n at this time. It is possible to amend this workflow to work on n8n cloud by replacing the local file trigger and read file nodes. How it works A local directory where bank statements are downloaded to is monitored via a local file trigger. The trigger watches for the file create, file changed and file deleted events. When a file is created, its contents are uploaded to the vector store. When a file is updated, its previous records are replaced. When the file is deleted, the corresponding records are also removed from the vector store. A simple Question and Answer Chatbot is setup to answer any questions about the bank statements in the system. Requirements A self-hosted version of n8n. Some of the nodes used in this workflow only work with the local filesystem. Qdrant instance to store the records. Customising the workflow This workflow can also work with remote data. Try integrating accounting or CRM software to build a managed system for payroll, invoices and more. Want to go fully local? A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/189F1fNOiw6naNSlSwnyLVEm_Ho_IFfdM/view?usp=sharing Jimleuk | 7 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"c5525f47-4d91-4b98-87bb-566b90da64a1\",\"name\":\"Local File Trigger\",\"type\":\"n8n-nodes-base.localFileTrigger\",\"position\":[660,700],\"parameters\":{\"path\":\"/home/node/host_mount/local_file_search\",\"events\":[\"add\",\"change\",\"unlink\"],\"options\":{\"awaitWriteFinish\":true},\"triggerOn\":\"folder\"},\"typeVersion\":1},{\"id\":\"804334d6-e34d-40d1-9555-b331ffe66f6f\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[664.5766613599001,881.8474780113352],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7ab0e284-b667-4d1f-8ceb-fb05e4081a06\",\"name\":\"Set Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[840,700],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"35ea70c4-8669-4975-a68d-bbaa094713c0\",\"name\":\"directory\",\"type\":\"string\",\"value\":\"/home/node/BankStatements\"},{\"id\":\"1d081d19-ff4e-462a-9cbe-7af2244bf87f\",\"name\":\"file_added\",\"type\":\"string\",\"value\":\"={{ $json.event === 'add' && $json.path || ''}}\"},{\"id\":\"18f8dc03-51ca-48c7-947f-87ce8e1979bf\",\"name\":\"file_changed\",\"type\":\"string\",\"value\":\"={{ $json.event === 'change' && $json.path || '' }}\"},{\"id\":\"65074ff7-037b-4b3b-b2c3-8a61755ab43b\",\"name\":\"file_deleted\",\"type\":\"string\",\"value\":\"={{ $json.event === 'un"}
{"prompt": "Create an n8n workflow for: Talk to your SQLite database with a LangChain AI Agent \ud83e\udde0\ud83d\udcac This n8n workflow demonstrates how to create an agent using LangChain and SQLite. The agent can understand natural language queries and interact with a SQLite database to provide accurate answers. \ud83d\udcaa \ud83d\ude80 Setup Run the top part of the workflow once. It downloads the example SQLite database, extracts from a ZIP file and saves locally (chinook.db). \ud83d\udde3\ufe0f Chatting with Your Data Send a message in a chat window. Locally saved SQLite database loads automatically. User's chat input is combined with the binary data. The LangChain Agend node gets both data and begins to work. The AI Agent will process the user's message, perform necessary SQL queries, and generate a response based on the database information. \ud83d\uddc4\ufe0f \ud83c\udf1f Example Queries Try these sample queries to see the AI Agent in action: \"Please describe the database\" - Get a high-level overview of the database structure, only one or two queries are needed. \"What are the revenues by genre?\" - Retrieve revenue information grouped by genre, LangChain agent iterates several time before producing the answer. The AI Agent will store the final answer in its memory, allowing for context-aware conversations. \ud83d\udcac Read the full article: \ud83d\udc49 https://blog.n8n.io/ai-agents/ Yulia | 8 months ago | Free", "completion": "{\"id\":\"AQJ6QnF2yVdCWMnx\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\",\"templateCredsSetupCompleted\":true},\"name\":\"SQL agent with memory\",\"tags\":[],\"nodes\":[{\"id\":\"3544950e-4d8e-46ca-8f56-61c152a5cae3\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1220,500],\"parameters\":{\"contextWindowLength\":10},\"typeVersion\":1.2},{\"id\":\"743cc4e7-5f24-4adc-b872-7241ee775bd0\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1000,500],\"parameters\":{\"model\":\"gpt-4-turbo\",\"options\":{\"temperature\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"rveqdSfp7pCRON1T\",\"name\":\"Ted's Tech Talks OpenAi\"}},\"typeVersion\":1},{\"id\":\"cc30066c-ad2c-4729-82c1-a6b0f4214dee\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[500,-80],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0deacd0d-45cb-4738-8da0-9d1251858867\",\"name\":\"Get chinook.zip example\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[700,-80],\"parameters\":{\"url\":\"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"61f34708-f8ed-44a9-8522-6042d28511ae\",\"name\":\"Extract zip file\",\"type\":\"n8n-nodes-base.compression\",\"position\":[900,-80"}
{"prompt": "Create an n8n workflow for: Automate LinkedIn Outreach with Notion and OpenAI This template is based on the following template. Thank you for the groundwork, Matheus. How it works: Store your snippets of text in a Notion table. Each snippet should have an image associated with it (copy + pasted into the text) Connect to your table via a Notion \"integration\", from which N8N can then query your pre-meditated posts The text is fed through an OpenAI assistant to boost engagement via formatting The re-formatted text along with the image pulled from the Notion snippet are combined into a post for your LinkedIn The row in the original Notion table from step 1 containing this post is set to a status of \"Done\" Set up steps: You will need to create a Notion \"integration\", which will yield a \"secret key\" which you enter into your N8N as a \"Credential\". You will need to create a LinkedIn \"app\" in order to post on your behalf. When creating your LinkedIn \"app\", you will be required to link this \"app\" to a company page on LinkedIn. If you are doing this for yourself, seach for the \"Default Company Payge (for API testing)\", and select this page as it is provided by LinkedIn for individuals. You can find your LinkedIn apps here, and if you get stuck, further instructions on setting up this workflow (including this LinkedIn OAuth piece) can be found in this YouTube Video Aide to these instructions. Lastly, you will need to create an OpenAI API key, found on your OpenAI Playground Dashboard. Once you created an API key, make sure you have an assistant created from the \"Assistants\" tab on the OpenAI dashboard. This assistant and its instructions will be needed for carrying out the re-formatting of your post. Paul Mikulskis | 8 months ago | Free", "completion": "{\"id\":\"mb2MU4xOaT3NrvqN\",\"meta\":{\"instanceId\":\"e7a28cc5c8c9de1976820e0f309940cf456344d9daf5360a4975186f3d8a107f\",\"templateCredsSetupCompleted\":true},\"name\":\"Automate LinkedIn Posts with AI\",\"tags\":[],\"nodes\":[{\"id\":\"7e8ec5cc-0216-4897-8a40-c44f9bbe5a9b\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[580,540],\"parameters\":{\"rule\":{\"interval\":[{\"triggerAtHour\":15}]}},\"typeVersion\":1.2},{\"id\":\"dbde804d-9c84-4023-9e05-7506cd38a460\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[760,225.26841303066982],\"parameters\":{\"color\":6,\"width\":652.1201853643956,\"height\":542.0867486896091,\"content\":\"## Fetch the day's post from my Notion database\\nA Notion _\\\"database\\\"_ is just a table on a Notion Page.\\nThis table will have various rows, for which a minimum of three columns are required:\\n- Name\\n- Status\\n- Date\\n\\nThe Date column is the most important, which will dictate when that row from your Notion table containing the text should be posted.\\n\\nNOTE: each post is required to have a copy and pasted image!\"},\"typeVersion\":1},{\"id\":\"95205e81-e28d-48f9-b3fb-bcf361f7799e\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1520,220],\"parameters\":{\"width\":860.9829802912225,\"height\":540.7357881640437,\"content\":\"## Format Post\\nSend the post to OpenAI, where it will attempt to ask your assistant how to take the incoming blob of text, and soup it up into something more palpable for LinkedIn engagement.\"},\"typeVersion\":1},{\"id\":\"4bc"}
{"prompt": "Create an n8n workflow for: Generate Text-to-Speech Using Elevenlabs via API \ud83c\udf89 Do you want to master AI automation, so you can save time and build cool stuff? I\u2019ve created a welcoming Skool community for non-technical yet resourceful learners. \ud83d\udc49\ud83c\udffb Join the AI Atelier \ud83d\udc48\ud83c\udffb This workflow provides an API endpoint to generate speech from text using Elevenlabs.io, a popular text-to-speech service. Step 1: Configure Custom Credentials in n8n To set up your credentials in n8n, create a new custom authentication entry with the following JSON structure: { \"headers\": { \"xi-api-key\": \"your-elevenlabs-api-key\" } } Replace \"your-elevenlabs-api-key\" with your actual Elevenlabs API key. Step 2: Send a POST Request to the Webhook Send a POST request to the workflow's webhook endpoint with these two parameters: voice_id: The ID of the voice from Elevenlabs that you want to use. text: The text you want to convert to speech. This workflow has been a significant time-saver in my video production tasks. I hope it proves just as useful to you! Happy automating! The n8Ninja Emmanuel Bernard | 10 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"73b64763-5e18-4ff1-bb52-ba25a08d3c3a\",\"name\":\"If params correct\",\"type\":\"n8n-nodes-base.if\",\"position\":[500,200],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"2e968b41-88f7-4b28-9837-af50ae130979\",\"operator\":{\"type\":\"string\",\"operation\":\"exists\",\"singleValue\":true},\"leftValue\":\"voice_id\",\"rightValue\":\"\"},{\"id\":\"ad961bc9-6db8-4cac-8c63-30930e8beca7\",\"operator\":{\"type\":\"string\",\"operation\":\"exists\",\"singleValue\":true},\"leftValue\":\"text\",\"rightValue\":\"\"}]}},\"typeVersion\":2},{\"id\":\"39079dec-54c5-458e-afa1-56ee5723f3a3\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[960,180],\"parameters\":{\"options\":{},\"respondWith\":\"binary\"},\"typeVersion\":1.1},{\"id\":\"b6a344f4-28ac-41a7-8e6a-a2782a5d1c68\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[300,200],\"webhookId\":\"5acc6769-6c0f-42a8-a69c-b05e437e18a9\",\"parameters\":{\"path\":\"generate-voice\",\"options\":{},\"httpMethod\":\"POST\",\"responseMode\":\"responseNode\"},\"typeVersion\":2},{\"id\":\"a25dec72-152b-4457-a18f-9cbbd31840ec\",\"name\":\"Generate voice\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[740,180],\"parameters\":{\"url\":\"=https://api.elevenlabs.io/v1/text-to-speech/{{ $json.body.voice_id }}\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"text\\\": \\\"{{ $json.body.text }}"}
{"prompt": "Create an n8n workflow for: Auto-label incoming Gmail messages with AI nodes This workflow uses AI to analyze the content of every new message in Gmail and then assigns specific labels, according to the context of the email. Default configuration of the workflow includes 3 labels: \u201ePartnership\u201d - email about sponsored content or cooperation, \u201eInquiry\u201d - email about products, services, \u201eNotification\u201d - email that doesn't require response. You can add or edit labels and descriptions according to your use case. \ud83c\udfac See this workflow in action in my YouTube video about automating Gmail. How it works? Gmail trigger performs polling every minute for new messages (you can change the trigger interval according to your needs). The email content is then downloaded and forwarded to an AI chain. \ud83d\udca1 The prompt in the AI chain node includes instructions for applying labels according to the email content - change label names and instructions to fit your use case. Next, the workflow retrieves all labels from the Gmail account and compares them with the label names returned from the AI chain. Label IDs are aggregated and applied to processed email messages. \u26a0\ufe0f Label names in the Gmail account and workflow (prompt, JSON schema) must be the same. Set up steps Set credentials for Gmail and OpenAI. Add labels to your Gmail account (e.g. \u201ePartnership\u201d, \u201eInquiry\u201d and \u201eNotification\u201d). Change prompt in AI chain node (update list of label names and instructions). Change list of available labels in JSON schema in parser node. Optionally: change polling interval in Gmail trigger (by default interval is 1 minute). If you like this workflow, please subscribe to my YouTube channel and/or my newsletter. Oskar | 11 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"8141ffad-df2a-403b-a869-799c036f9733\",\"name\":\"Gmail trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-600,580],\"parameters\":{\"simple\":false,\"filters\":{},\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"uBcIMfsTtKjexw7I\",\"name\":\"Gmail (workfloowstutorial@gmail.com)\"}},\"typeVersion\":1},{\"id\":\"6d9aa398-e2de-4fd0-b939-2a12d0c9fe14\",\"name\":\"Get message content\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[-340,580],\"parameters\":{\"simple\":false,\"options\":{},\"messageId\":\"={{ $json.id }}\",\"operation\":\"get\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"uBcIMfsTtKjexw7I\",\"name\":\"Gmail (workfloowstutorial@gmail.com)\"}},\"typeVersion\":2.1},{\"id\":\"cd86bc09-8c7f-4c85-9cb3-6dbd42420672\",\"name\":\"Set label values\",\"type\":\"n8n-nodes-base.set\",\"position\":[300,580],\"parameters\":{\"fields\":{\"values\":[{\"name\":\"labels\",\"type\":\"arrayValue\",\"arrayValue\":\"={{ $json.labels }}\"}]},\"options\":{}},\"typeVersion\":3.2},{\"id\":\"329435a6-51d1-416e-9aa9-5fe9a8dce74f\",\"name\":\"Get all labels\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[580,460],\"parameters\":{\"resource\":\"label\",\"returnAll\":true},\"credentials\":{\"gmailOAuth2\":{\"id\":\"uBcIMfsTtKjexw7I\",\"name\":\"Gmail (workfloowstutorial@gmail.com)\"}},\"typeVersion\":2.1},{\"id\":\"7ae2dd15-472d-4a4b-b036-f80ebd7e3c28\",\"name\":\"Split out assigned labels\",\"type\":\"n"}
{"prompt": "Create an n8n workflow for: AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow How it works: Using a Crew of AI agents (Senior Researcher, Visionary, and Senior Editor), this crew will automatically determine the right questions to ask to produce a detailed fundamental stock analysis. This application has two components: a front-end and a Stock Q&A engine. The front end is the team of agents automatically figuring out the questions to ask, and the back-end part is the ability to answer those questions with the SEC 10K data. This template implements the Stock Q&A engine. For the front-end of the application, you can choose one of two options: using CrewAI with the Replit environment (code approach) fully visual approach with n8n template (AI-powered automated stock analysis) Setup steps: Use first workflow in template to upsert a company annual report PDF (such as from SEC 10K filling) Get URL for Webhook in second workflow template CrewAI front-end: Youtube overview video Fork this AI Agent environment Crew Agent Environment Set the webhook URL into N8N_WEBHOOK_URL variable Set OpenAI_API_KEY variable Derek Cheung | 11 months ago | Free +3", "completion": "{\"id\":\"tMiRJYDrXzpKysTX\",\"meta\":{\"instanceId\":\"2723a3a635131edfcb16103f3d4dbaadf3658e386b4762989cbf49528dccbdbd\",\"templateId\":\"1960\"},\"name\":\"Stock Q&A Workflow\",\"tags\":[],\"nodes\":[{\"id\":\"ec3b86be-4113-4fd5-8365-02adb67693e9\",\"name\":\"Embeddings OpenAI1\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1960,720],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"fOF5kro9BJ6KMQ7n\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"42fd8020-3861-4d0f-a7a2-70e2c35f0bed\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"disabled\":true,\"position\":[1620,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"a9b48d04-691e-4537-90f8-d7a4aa6153af\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1560,120],\"parameters\":{\"color\":6,\"width\":903.0896125323785,\"height\":733.5099670584011,\"content\":\"## Step 2: Setup the Q&A \\n### The incoming message from the webhook is queried from the Supabase Vector Store. The response is provided in the response webhook. \"},\"typeVersion\":1},{\"id\":\"472b4800-745a-4337-9545-163247f7e9ae\",\"name\":\"Retrieval QA Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainRetrievalQa\",\"position\":[1880,240],\"parameters\":{\"query\":\"={{ $json.body.input }}\"},\"typeVersion\":1},{\"id\":\"e58bd82d-abc6-44ed-8e93-ec5436126"}
{"prompt": "Create an n8n workflow for: Chat with OpenAIs GPT via a simple Telegram Bot Use case LLMs have provided a lot of value for several use cases. Especially some OpenAI models are proving to be quite valuable. However, it's sometimes not super accessible to chat with these models. This workflow enables you to chate directly with OpenAI's GPT-3.5 via Telegram. How it works A simple telegram bot that connects to your botfather bot to give AI responses, using OpenAI's GPT 3.5 model, to a user's messages with emojis. What to do Add your telegram API key and your OpenAI api key and have fun! Mike | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"014363851c6b81282e1489df62d7f66bb7c99af5dcb6c1032b3a83a1d72baee4\"},\"nodes\":[{\"id\":\"0b4eb8e4-e98b-4f67-b134-914a5aa46b4d\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[960,400],\"webhookId\":\"9c8b833c-7aa7-430d-8fc0-47936f695ddf\",\"parameters\":{\"updates\":[\"message\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"4lzd2F9cNrnR7j0j\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"339246f2-76cb-44c4-8828-da0cb5d3ad5e\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1100,600],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"m3YyjGXFLLWwcnk7\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"70a981e2-7833-473b-a27a-fedf860901cb\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[1200,400],\"parameters\":{\"text\":\"=Respond to this as a helpful assistant with emojis: {{ $json.message.text }}\",\"options\":{}},\"typeVersion\":1.2},{\"id\":\"fb6ff65b-56b4-44c4-978a-b9a5c3d535d6\",\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1560,400],\"parameters\":{\"text\":\"={{ $json.output }}\",\"chatId\":\"={{ $('Telegram Trigger').item.json.message.chat.id }}\",\"additionalFields\":{\"appendAttribution\":false}},\"credentials\":{\"te"}
{"prompt": "Create an n8n workflow for: Chat with a Google Sheet using AI This workflow allows you to ask questions about the data in a Google Sheet over a chat interface. It uses n8n's built-in chat, but could be modified to work with Slack, Teams, WhatsApp, etc. Behind the scenes, the workflow uses GPT4, so you'll need to have an OpenAI API key that supports it. How it works The workflow uses an AI agent with custom tools that call a sub-workflow. That sub-workflow reads the Google Sheet and returns information from it. Because models have a context window (and therefore a maximum number of characters they can accept), we can't pass the whole Google Sheet to GPT - at least not for big sheets. So we provide three ways of querying less data, that can be used in combination to answer questions. Those three functions are: List all the columns in the sheet Get all values of a single column Get all values of a single row Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free", "completion": "{\"id\":\"ZVUQL1bUQ8gBCZTl\",\"meta\":{\"instanceId\":\"23e6ce638471979c8a2c72a9fb50e44f4f2bfd5a9fc2f5b7f5c842b9abeb9393\"},\"name\":\"Chat with Google Sheet\",\"tags\":[],\"nodes\":[{\"id\":\"89af21df-1125-4df6-9d43-a643e02bb53f\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[540,1240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f571d0cc-eb43-46c9-bdd5-45abc51dfbe7\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[461.9740563285368,970.616715060075],\"parameters\":{\"color\":7,\"width\":1449.2963504228514,\"height\":612.0936015224503,\"content\":\"### Sub-workflow: Custom tool\\nThis can be called by the agent above. It returns three different types of data from the Google Sheet, which can be used together for more complex queries without returning the whole sheet (which might be too big for GPT to handle)\"},\"typeVersion\":1},{\"id\":\"8761e314-c1f2-4edd-88ea-bfeb02dc8f1a\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[460,460],\"parameters\":{\"color\":7,\"width\":927.5,\"height\":486.5625,\"content\":\"### Main workflow: AI agent using custom tool\"},\"typeVersion\":1},{\"id\":\"e793b816-68d9-42ef-b9b0-6fe22aa375e8\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[300,540],\"parameters\":{\"width\":185.9375,\"height\":183.85014518022527,\"content\":\"## Try me out\\n\\nClick the 'Chat' button at the bottom and enter:\\n\\n_Which is our"}
{"prompt": "Create an n8n workflow for: Telegram AI bot with LangChain nodes This workflow connects Telegram bots with LangChain nodes in n8n. The main AI Agent Node is configured as a Conversation Agent. It has a custom System Prompt which explains the reply formatting and provides some additional instructions. The AI Agent has several connections: OpenAI GPT-4 model is called to generate the replies Window Buffer Memory stores the history of conversation with each user separately There is an additional Custom n8n Workflow tool (Dall-E 3 Tool). AI Agent uses this tool when the user requests an image generation. In the lower part of the workflow, there is a series of nodes that call Dall-E 3 model with the user Telegram ID and a prompt for a new image. Once image is ready, it is sent back to the user. Finally, there is an extra Telegram node that masks HTML syntax for improved stability in case the AI Agent replies using the unsupported format. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"ax8PJlp1UDb6EGFt\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"Telegram AI Langchain bot\",\"tags\":[],\"nodes\":[{\"id\":\"e275f31f-6a5f-4444-8bf7-6c003a8e53df\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1100,600],\"parameters\":{\"model\":\"gpt-4-1106-preview\",\"options\":{\"temperature\":0.7,\"frequencyPenalty\":0.2}},\"credentials\":{\"openAiApi\":{\"id\":\"63\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"f25a6666-ff23-4372-afd0-4920a99aab6a\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[1220,600],\"parameters\":{\"sessionKey\":\"=chat_with_{{ $('Listen for incoming events').first().json.message.chat.id }}\",\"contextWindowLength\":10},\"typeVersion\":1},{\"id\":\"96faef5d-0349-47fe-a7cf-150953490e90\",\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"onError\":\"continueErrorOutput\",\"position\":[1500,380],\"parameters\":{\"text\":\"={{ $json.output }}\",\"chatId\":\"={{ $('Listen for incoming events').first().json.message.from.id }}\",\"additionalFields\":{\"parse_mode\":\"HTML\",\"appendAttribution\":false}},\"credentials\":{\"telegramApi\":{\"id\":\"70\",\"name\":\"Telegram sdfsdfsdfsdfsfd_bot\"}},\"typeVersion\":1.1},{\"id\":\"5ad43039-aaa6-43cd-9b0f-1d02f4d9c4ff\",\"name\":\"Correct errors\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1700,380],\"parameters"}
{"prompt": "Create an n8n workflow for: AI chat with any data source (using the n8n workflow tool) This AI agent can access data provided by another n8n workflow. Since that workflow can be used to retrieve any data from any service, this template can be used give an agent access to any data. Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"4c52efcf-039b-4550-8a63-3d3d4dde488b\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[740,300],\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"adb528f1-b87b-4bb2-99e1-776fd839522e\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[680,940],\"parameters\":{},\"typeVersion\":1},{\"id\":\"092cf737-5b53-4fc8-82f5-c775b77ea0bd\",\"name\":\"Hacker News\",\"type\":\"n8n-nodes-base.hackerNews\",\"position\":[900,940],\"parameters\":{\"limit\":50,\"resource\":\"all\",\"additionalFields\":{}},\"typeVersion\":1},{\"id\":\"a0805137-630c-4065-826e-88afa000660f\",\"name\":\"Clean up data\",\"type\":\"n8n-nodes-base.set\",\"position\":[1120,940],\"parameters\":{\"fields\":{\"values\":[{\"name\":\"title\",\"stringValue\":\"={{ $json._highlightResult.title.value }}\"},{\"name\":\"points\",\"type\":\"numberValue\",\"numberValue\":\"={{ $json.points }}\"},{\"name\":\"url\",\"stringValue\":\"={{ $json.url }}\"},{\"name\":\"created_at\",\"stringValue\":\"={{ $json.created_at }}\"},{\"name\":\"author\",\"stringValue\":\"={{ $json.author }}\"}]},\"include\":\"none\",\"options\":{}},\"typeVersion\":3.2},{\"id\":\"e1b255f4-e970-42d6-9870-4e302bf2da83\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[960,300],\"parameters\":{\"options\":{\"maxIterations\":10}},\"typeVersion\":1.1},{\"id\":\"91e33"}
{"prompt": "Create an n8n workflow for: Ask questions about a PDF using AI The workflow first populates a Pinecone index with vectors from a Bitcoin whitepaper. Then, it waits for a manual chat message. When received, the chat message is turned into a vector and compared to the vectors in Pinecone. The most similar vectors are retrieved and passed to OpenAI for generating a chat response. Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"62b3b6db4f4d3641a1fa1da6dfb9699a19380a1f60cbc18fc75d6d145f35552b\"},\"nodes\":[{\"id\":\"40bb5497-d1d2-4eb7-b683-78b88c8d9230\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[496.83478320435574,520],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"https://drive.google.com/file/d/11Koq9q53nkk0F5Y8eZgaWJUVR03I4-MM/view\"},\"options\":{},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"20\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"1323d520-1528-4a5a-9806-8f4f45306098\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[996.8347832043557,920],\"parameters\":{\"chunkSize\":3000,\"chunkOverlap\":200},\"typeVersion\":1},{\"id\":\"796b155a-64e6-4a52-9168-a37c68077d99\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[836.8347832043557,740],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"JCgD7807AQpe8Xge\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"dbe42c28-6f0b-4999-8372-0b42f6fb5916\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[260,420],\"parameters\":{\"color\":7,\"width\":978.0454109366399,\"height\":806.6556079800943,\"content\":\"### Load data into database\\n"}
{"prompt": "Create an n8n workflow for: AI chatbot that can search the web This workflow is designed for dynamic and intelligent conversational capabilities. It incorporates OpenAI's GPT-4o model for natural language understanding and generation. Additional tools include SerpAPI and Wikipedia for enriched, data-driven responses. The workflow is triggered manually, and utilizes a 'Window Buffer Memory' to maintain the context of the last 20 interactions for better conversational continuity. All these components are orchestrated through n8n nodes, ensuring seamless interconnectivity. To use this template, you need to be on n8n version 1.50.0 or later. n8n Team | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"3a3bcb2d-cb94-40d8-8b9e-322ea9d27f6e\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1000,640],\"parameters\":{\"width\":300,\"height\":185,\"content\":\"### The conversation history(last 20 messages) is stored in a buffer memory\"},\"typeVersion\":1},{\"id\":\"e279af43-b003-4499-b221-58716e735379\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[740,340],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f4f8bf03-a43e-4a1f-a592-cd0f8408f552\",\"name\":\"Chat OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[840,653],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{\"temperature\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"66b60f68-bae8-4958-ac81-03883f563ab3\",\"name\":\"Wikipedia\",\"type\":\"@n8n/n8n-nodes-langchain.toolWikipedia\",\"position\":[1480,693],\"parameters\":{},\"typeVersion\":1},{\"id\":\"70f6b43b-9290-4fbc-992f-0895d4578c9f\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1340,633],\"parameters\":{\"width\":300,\"height\":185,\"content\":\"### Tools which agent can use to accomplish the task\"},\"typeVersion\":1},{\"id\":\"8696269f-6556-41f1-bbe4-5597e4e"}
{"prompt": "Create an n8n workflow for: Suggest meeting slots using AI The purpose of this n8n workflow is to automate the process of identifying incoming Gmail emails that are requesting an appointment, evaluating their content, checking calendar availability, and then composing and sending a response email. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free +2", "completion": "{\"id\":\"slP122GjD9meGkS6\",\"meta\":{\"instanceId\":\"178ef8a5109fc76c716d40bcadb720c455319f7b7a3fd5a39e4f336a091f524a\"},\"name\":\"Calendar_scheduling\",\"tags\":[],\"nodes\":[{\"id\":\"bd1dae81-daea-4539-bf1d-38eb9a2bd2f0\",\"name\":\"Gmail Trigger\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[500,560],\"parameters\":{\"filters\":{\"readStatus\":\"unread\",\"includeSpamTrash\":false},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"kLFedNEM8Zwkergv\",\"name\":\"Gmail account\"}},\"typeVersion\":1},{\"id\":\"a97c3ab1-6fbc-441e-af11-3c746936013b\",\"name\":\"Chat OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[720,740],\"parameters\":{\"model\":\"gpt-4\",\"options\":{\"temperature\":0.1}},\"credentials\":{\"openAiApi\":{\"id\":\"wJtZwsVKW5v6R2Iy\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"a1205598-7cd4-4278-ad53-0cfc7c7947ff\",\"name\":\"Workflow Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1580,759],\"parameters\":{\"name\":\"Calendar_Availability\",\"workflowId\":\"={{ $workflow.id }}\",\"description\":\"Call this tool to get my calendar availability as stringified JSON array.\"},\"typeVersion\":1},{\"id\":\"5ba2c2b0-2218-45d2-a417-f86c80643397\",\"name\":\"Chat OpenAI1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1420,759],\"parameters\":{\"model\":\"gpt-4\",\"options\":{\"temperature\":0}},\""}
{"prompt": "Create an n8n workflow for: Telegram AI Chatbot The workflow starts by listening for messages from Telegram users. The message is then processed, and based on its content, different actions are taken. If it's a regular chat message, the workflow generates a response using the OpenAI API and sends it back to the user. If it's a command to create an image, the workflow generates an image using the OpenAI API and sends the image to the user. If the command is unsupported, an error message is sent. Throughout the workflow, there are additional nodes for displaying notes and simulating typing actions. Eduard | 1 year ago | Free", "completion": "{\"id\":\"177\",\"meta\":{\"instanceId\":\"dfdeafd1c3ed2ee08eeab8c2fa0c3f522066931ed8138ccd35dc20a1e69decd3\"},\"name\":\"Telegram AI-bot\",\"tags\":[{\"id\":\"15\",\"name\":\"tutorial\",\"createdAt\":\"2022-10-04T20:07:25.607Z\",\"updatedAt\":\"2022-10-04T20:07:25.607Z\"}],\"nodes\":[{\"id\":\"ea71a467-a646-4aca-b72e-cef1249c74e2\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[20,340],\"webhookId\":\"51942fbb-ca0e-4ec4-9423-5fcc7d3c4281\",\"parameters\":{\"updates\":[\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"70\",\"name\":\"Telegram bot\"}},\"typeVersion\":1},{\"id\":\"1cbe43d4-ea8b-4178-bc10-4bfad7abe143\",\"name\":\"CheckCommand\",\"type\":\"n8n-nodes-base.switch\",\"position\":[980,360],\"parameters\":{\"rules\":{\"rules\":[{\"value2\":\"/\",\"operation\":\"notStartsWith\"},{\"output\":1,\"value2\":\"/start\",\"operation\":\"startsWith\"},{\"output\":2,\"value2\":\"=/image \",\"operation\":\"startsWith\"}]},\"value1\":\"={{ $json.message?.text }}\",\"dataType\":\"string\",\"fallbackOutput\":3},\"typeVersion\":1},{\"id\":\"074e907f-634b-4242-b669-33fa064f8472\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1600,581.661764705882],\"parameters\":{\"width\":316.1071428571428,\"height\":231.22373949579838,\"content\":\"## Error fallback for unsupported commands\"},\"typeVersion\":1},{\"id\":\"2aa961b8-f0af-4d5c-a6af-1be56ea4b2e6\",\"name\":\"Settings\",\"type\":\"n8n-n"}
{"prompt": "Create an n8n workflow for: Vector Database as a Big Data Analysis Tool for AI Agents [2/2 KNN] Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar \"Build production-ready AI Agents with Qdrant and n8n\". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection The first pipeline to upload an image dataset to Qdrant. The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification The first pipeline to upload an image dataset to Qdrant. This pipeline is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] KNN classification tool This tool takes any image URL, and as output, it returns a class of the object on the image based on the image uploaded to the Qdrant dataset (lands). An image URL is received via the Execute Workflow Trigger, which is then sent to the Voyage AI Multimodal Embeddings API to fetch its embedding. The image's embedding vector is then used to query Qdrant, returning a set of X similar images with pre-labeled classes. Majority voting is done for classes of neighbouring images. A loop is used to resolve scenarios where there is a tie in Majority Voting, and we increase the number of neighbours to retrieve. When the loop finally resolves, the identified class is returned to the calling workflow. Jenny | 2 months ago | Free", "completion": "{\"id\":\"itzURpN5wbUNOXOw\",\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"name\":\"[2/2] KNN classifier (lands dataset)\",\"tags\":[{\"id\":\"QN7etptCmdcGIpkS\",\"name\":\"classifier\",\"createdAt\":\"2024-12-08T22:08:15.968Z\",\"updatedAt\":\"2024-12-09T19:25:04.113Z\"}],\"nodes\":[{\"id\":\"33373ccb-164e-431c-8a9a-d68668fc70be\",\"name\":\"Embed image\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-140,-240],\"parameters\":{\"url\":\"https://api.voyageai.com/v1/multimodalembeddings\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={{\\n{\\n \\\"inputs\\\": [\\n {\\n \\\"content\\\": [\\n {\\n \\\"type\\\": \\\"image_url\\\",\\n \\\"image_url\\\": $json.imageURL\\n }\\n ]\\n }\\n ],\\n \\\"model\\\": \\\"voyage-multimodal-3\\\",\\n \\\"input_type\\\": \\\"document\\\"\\n}\\n}}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"Vb0RNVDnIHmgnZOP\",\"name\":\"Voyage API\"}},\"typeVersion\":4.2},{\"id\":\"58adecfa-45c7-4928-b850-053ea6f3b1c5\",\"name\":\"Query Qdrant\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[440,-240],\"parameters\":{\"url\":\"={{ $json.qdrantCloudURL }}/collections/{{ $json.collectionName }}/points/query\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={{\\n{\\n \\\"query\\\": $json.ImageEmbedding,\\n \\\"using\\\": \\\"voyage\\\",\\n \\\"limit"}
{"prompt": "Create an n8n workflow for: Vector Database as a Big Data Analysis Tool for AI Agents [2/3 - anomaly] Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar \"Build production-ready AI Agents with Qdrant and n8n\". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection The first pipeline to upload an image dataset to Qdrant. 2. This is the second pipeline to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification The first pipeline to upload an image dataset to Qdrant. The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] Setting Up Cluster (Class) Centres & Cluster (Class) Threshold Scores for Anomaly Detection Preparatory workflow to set cluster centres and cluster threshold scores so anomalies can be detected based on these thresholds. Here, we're using two approaches to set up these centres: the \"distance matrix approach\" and the \"multimodal embedding model approach\". Jenny | 2 months ago | Free", "completion": "{\"id\":\"m9aACcHqydEbH4nR\",\"meta\":{\"instanceId\":\"205b3bc06c96f2dc835b4f00e1cbf9a937a74eeb3b47c99d0c30b0586dbf85aa\"},\"name\":\"[2/3] Set up medoids (2 types) for anomaly detection (crops dataset)\",\"tags\":[{\"id\":\"spMntyrlE9ydvWFA\",\"name\":\"anomaly-detection\",\"createdAt\":\"2024-12-08T22:05:15.945Z\",\"updatedAt\":\"2024-12-09T12:50:19.287Z\"}],\"nodes\":[{\"id\":\"edaa871e-2b79-400e-8328-333d250bfdd2\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-660,-220],\"parameters\":{},\"typeVersion\":1},{\"id\":\"ebd964de-faa4-4dc0-9245-cc9154b9ce02\",\"name\":\"Total Points in Collection\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[180,-220],\"parameters\":{\"url\":\"={{ $('Qdrant cluster variables').item.json.qdrantCloudURL }}/collections/{{ $('Qdrant cluster variables').item.json.collectionName }}/points/count\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"exact\\\": true\\n}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"qdrantApi\"},\"credentials\":{\"qdrantApi\":{\"id\":\"it3j3hP9FICqhgX6\",\"name\":\"QdrantApi account\"}},\"typeVersion\":4.2},{\"id\":\"b51f6344-d090-4341-a908-581b78664b07\",\"name\":\"Cluster Distance Matrix\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1200,-360],\"parameters\":{\"url\":\"={{ $('Qdrant cluster variables').first().json.qdrantCloudURL }}/collections/{{"}
{"prompt": "Create an n8n workflow for: Extract Information from a Logo Sheet using forms, AI, Google Sheet and Airtable Instructions This automation enables you to just upload any Image (via Form) of a Logo Sheet, containing multiple Images of Product Logos (most likely) which brings them in some context to one another. After submitting an AI-Agent eats that Logo Sheet, turning it into an List of \"Productname\" and \"Attributes\", also checks if Tools are kind of similar to another, given the Context of the Image. We utilize AI Vision capabilities for that. NOTE: It might not be able to extract all informations. For a \"upload and forget it\" Workflow it works for me. You can even run it multiple times, to be sure. But if you need to make sure it extracts everything you might need to think about an Multi-Agent Setup with Validation-Agent Steps. Once the Agent finishes the extraction, it will traditionally and deterministicly add those Attributes to Airtable (Creates those, if not already existing.) and also Upserts the Tool Informations. It uses MD5 Hashes for turning Product Names into.. something fancy really, you could also use it without that, but I wanted to have something that looks atleast like an ID. Setup Set Up the Airtable like shown below. Update and set Credentials for all Airtable Nodes. Check or Adjust the Prompt of the Agent matching your use-case. Activate the Workflow. Open the Form (default: https://your-n8n.io/form/logo-sheet-feeder) Enjoy growing your Airtable. Enjoy the workflow! \u2764\ufe0f let the work flow \u2014 Workflow Automation & Development Marcel Claus-Ahrens | 2 months ago | Free +2", "completion": "{\"id\":\"dDAqkobn2pqgdl2N\",\"meta\":{\"instanceId\":\"9e331a89ae45a204c6dee51c77131d32a8c962ec20ccf002135ea60bd285dba9\"},\"name\":\"AI Logo Sheet Extractor to Airtable\",\"tags\":[],\"nodes\":[{\"id\":\"f7ecadb8-dc5d-4e8c-96b8-52c1dbad49b6\",\"name\":\"On form submission\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[-660,-220],\"webhookId\":\"43837a27-f752-40a8-852a-d5d63d647bfd\",\"parameters\":{\"options\":{\"path\":\"logo-sheet-feeder\"},\"formTitle\":\"AI Logo Sheet Feeder\",\"formFields\":{\"values\":[{\"fieldType\":\"file\",\"fieldLabel\":\"The Logo-Sheet as Image\",\"requiredField\":true},{\"fieldLabel\":\"Addional Prompt (e.g.: What the meaning of the graphic?) *optional but helps from time to time.\",\"placeholder\":\"It's a graph chart comparing AI Tools\"}]},\"formDescription\":\"Provide a Image with multiple Logos comparing or bringing multiple Tools into Context with one another.\"},\"typeVersion\":2.2},{\"id\":\"b1530578-bde9-4ee3-9cdb-545a621cdb84\",\"name\":\"Retrieve and Parser Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[-180,-220],\"parameters\":{\"options\":{\"systemMessage\":\"Your task is to retrieve Information from the given Input. Extract Categories and Attributes of all given and shown Tools, Softwares or Products you've got by the user.\\n\\nProvide the Output Array of Tools with the following Structure as JSON:\\n\\n[{\\n\\\"name\\\": \\\"Name of the Tool, Software, etc.\\\",\\n\\\"attributes\\\": [\\\"Some category or attribute\\\", \\\"something else you can see from the context or image\\\"],\\n\\\"similar\\\": [\\\"similar tool, product, etc. from shown context\\\", \\\"another similar software, product, tool from context\\\"]\\n},{\\n\\\"name\\\": \\\"Name of anotherTool, Software, etc.\\\",\\n\\\"attributes\\\": [\\\""}
{"prompt": "Create an n8n workflow for: Get Airtable data via AI and Obsidian Notes I am submitting this workflow for the Obsidian community to showcase the potential of integrating Obsidian with n8n. While straightforward, it serves as a compelling demonstration of the potential unlocked by integrating Obsidian with n8n. How it works This workflow lets you retrieve specific Airtable data you need in seconds, directly within your Obsidian note, using n8n. By highlighting a question in Obsidian and sending it to a webhook via the Post Webhook Plugin, you can fetch specific data from your Airtable base and instantly insert the response back into your note. The workflow leverages OpenAI\u2019s GPT model to interpret your query, extract relevant data from Airtable, and format the result for seamless integration into your note. Set up steps Install the Post Webhook Plugin: Add this plugin to your Obsidian vault from the plugin store or GitHub. Set up the n8n Webhook: Copy the webhook URL generated in this workflow and insert it into the Post Webhook Plugin's settings in Obsidian. Configure Airtable Access: Link your Airtable account and specify the desired base and table to pull data from. Test the Workflow: Highlight a question in your Obsidian note, use the \u201cSend Selection to Webhook\u201d command, and verify that data is returned as expected. Obsidi8n | 2 months ago | Free", "completion": "{\"id\":\"aZSJ2BZQhNduZZ8w\",\"meta\":{\"instanceId\":\"d47f3738b860eed937a1b18d7345fa2c65cf4b4957554e29477cb064a7039870\",\"templateCredsSetupCompleted\":true},\"name\":\"Get Airtable data in Obsidian Notes\",\"tags\":[{\"id\":\"zalLN3OHeRqcq4di\",\"name\":\"Obsidian\",\"createdAt\":\"2024-12-01T19:07:59.925Z\",\"updatedAt\":\"2024-12-01T19:07:59.925Z\"}],\"nodes\":[{\"id\":\"584cfe61-7f1b-4deb-ab4b-45a5ffd20daf\",\"name\":\"Airtable\",\"type\":\"n8n-nodes-base.airtableTool\",\"position\":[540,340],\"parameters\":{\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appP3ocJy1rXIo6ko\",\"cachedResultUrl\":\"https://airtable.com/appP3ocJy1rXIo6ko\",\"cachedResultName\":\"table\"},\"table\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"tblywtlpPtGQMTJRm\",\"cachedResultUrl\":\"https://airtable.com/appP3ocJy1rXIo6ko/tblywtlpPtGQMTJRm\",\"cachedResultName\":\"Dummy\"},\"options\":{},\"operation\":\"search\"},\"credentials\":{\"airtableTokenApi\":{\"id\":\"yiZ7ZC1md4geZovu\",\"name\":\"Airtable Personal Access Token account\"}},\"typeVersion\":2.1},{\"id\":\"8a100c92-7971-464b-b3c0-18272f0a0bef\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[220,340],\"parameters\":{\"model\":\"gpt-4o-mini\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"q8L9oWVM7QyzYEE5\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\""}
{"prompt": "Create an n8n workflow for: Telegram AI Bot: NeurochainAI Text & Image - NeurochainAI Basic API Integration This template provides a workflow to integrate a Telegram bot with NeurochainAI's inference capabilities, supporting both text processing and image generation. Follow these steps to get started: &gt; Purpose: Enables seamless integration between your Telegram bot and NeurochainAI for advanced AI-driven text and image tasks. Requirements Telegram Bot Token. NeurochainAI API Key. Sufficient credits to utilize NeurochainAI services. Features Text processing through NeurochainAI's inference engine. AI-powered image generation (Flux). Easy customization and scalability for your use case. Setup Import the template into N8N. Add your Telegram Bot Token and NeurochainAI API Key where prompted. Follow the step-by-step instructions embedded in the template for configuration. [NeurochainAI Website](https://www.neurochain.ai/ ) NeurochainAI Guides NeurochainAI | 3 months ago | Free", "completion": "{\"id\":\"RLWjEhY8L4TORAIj\",\"meta\":{\"instanceId\":\"36399efc72267ed21ee0d3747f5abdd0ee139cb67749ff919ff09fcd65230079\",\"templateCredsSetupCompleted\":true},\"name\":\"NeurochainAI Basic API Integration\",\"tags\":[],\"nodes\":[{\"id\":\"da34bd1a-4e4e-4133-acad-939d0cc96596\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[-1740,880],\"webhookId\":\"05885608-5344-4dcf-81ad-4550b9a01241\",\"parameters\":{\"updates\":[\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"VPtf3hBnwGucAQtu\",\"name\":\"TEMPLATE\"}},\"typeVersion\":1.1},{\"id\":\"3b3f4b00-6b3b-4346-8fcc-7ab75bcfe838\",\"name\":\"Code\",\"type\":\"n8n-nodes-base.code\",\"notes\":\"Extract the URL from the previous node\",\"position\":[80,260],\"parameters\":{\"jsCode\":\"// O valor vem como um array com uma string, ent\u00e3o precisamos pegar o primeiro item do array\\nconst rawUrl = $json.choices[0].text;\\n\\n// Remover colchetes e aspas (se existirem) e pegar o primeiro elemento do array\\nconst imageUrl = JSON.parse(rawUrl)[0];\\n\\nreturn {\\n json: {\\n imageUrl: imageUrl\\n }\\n};\"},\"notesInFlow\":true,\"typeVersion\":2},{\"id\":\"ccb91a15-96b5-42aa-a6ae-ff7ae79d1e8f\",\"name\":\"HTTP Request3\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[240,260],\"parameters\":{\"url\":\"={{ $json.imageUrl }}\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"588899b6-a68e-407e-b12f-f"}
{"prompt": "Create an n8n workflow for: Optimize & Update Printify Title and Description Workflow Printify Automation - Update Title and Description Workflow This n8n workflow automates the process of retrieving products from Printify, generating optimized product titles and descriptions, and updating them back to the platform. It leverages OpenAI for content generation and integrates with Google Sheets for tracking and managing updates. Features Integration with Printify**: Fetch shops and products through Printify's API. AI-Powered Optimization**: Generate engaging product titles and descriptions using OpenAI's GPT model. Google Sheets Tracking**: Log and manage updates in Google Sheets. Custom Brand Guidelines**: Ensure consistent tone by incorporating brand-specific instructions. Loop Processing**: Iteratively process each product in batches. Workflow Structure Nodes Overview Manual Trigger: Manually start the workflow for testing purposes. Printify - Get Shops: Retrieves the list of shops from Printify. Printify - Get Products: Fetches product details for each shop. Split Out: Breaks down the product list into individual items for processing. Loop Over Items: Iteratively processes products in manageable batches. Generate Title and Desc: Uses OpenAI GPT to create optimized product titles and descriptions. Google Sheets Integration: Trigger: Monitors Google Sheets for changes. Log Updates: Records product updates, including old and new titles/descriptions. Conditional Logic: If Nodes: Ensure products are ready for updates and stop processing once completed. Printify - Update Product: Sends updated titles and descriptions back to Printify. Brand Guidelines + Custom Instructions: Sets brand tone and seasonal instructions. Setup Instructions Prerequisites n8n Instance: Ensure n8n is installed and configured. Printify API Key: Obtain an API key from your Printify account. Add it to n8n under HTTP Header Auth. OpenAI API Key: Obtain an API key from OpenAI. Add it to n8n under OpenAI API. Google Sheets Integration: Share your Google Sheets with the Google API service account. Configure Google Sheets credentials in n8n. Workflow Configuration Set Brand Guidelines: Update the Brand Guidelines + Custom Instructions node with your brand name, tone, and seasonal instructions. Batch Size: Configure the Loop Over Items node for optimal batch sizes. Google Sheets Configuration: Set the correct Google Sheets document and sheet names in the integration nodes. Run the Workflow: Start manually or configure the workflow to trigger automatically. Key Notes Customization**: Modify API calls to support other platforms like Printful or Vistaprint. Scalability**: Use batch processing for efficient handling of large product catalogs. Error Handling**: Configure retries or logging for any failed nodes. Output Examples Optimized Content Example Input Title**: \"Classic White T-Shirt\" Generated Title**: \"Stylish Classic White Tee for Everyday Wear\" Input Description**: \"Plain white T-shirt made of cotton.\" Generated Description**: \"Discover comfort and style with our classic white tee, crafted from premium cotton for all-day wear. Perfect for casual outings or layering.\" Next Steps Monitor Updates: Use Google Sheets to review logs of updated products. Expand Integration: Add support for more Printify shops or integrate with other platforms. Enhance AI Prompts: Customize prompts for different product categories or seasonal needs. Feel free to reach out for additional guidance or troubleshooting! Alex Kim | 3 months ago | Free", "completion": "{\"id\":\"1V1gcK6vyczRqdZC\",\"meta\":{\"instanceId\":\"d868e3d040e7bda892c81b17cf446053ea25d2556fcef89cbe19dd61a3e876e9\",\"templateCredsSetupCompleted\":true},\"name\":\"Printify Automation - Update Title and Description - AlexK1919\",\"tags\":[{\"id\":\"NBHymnfw5EIluMXO\",\"name\":\"Printify\",\"createdAt\":\"2024-11-27T18:26:34.584Z\",\"updatedAt\":\"2024-11-27T18:26:34.584Z\"},{\"id\":\"QsH2EXuw2e7YCv0K\",\"name\":\"OpenAI\",\"createdAt\":\"2024-11-15T04:05:20.872Z\",\"updatedAt\":\"2024-11-15T04:05:20.872Z\"}],\"nodes\":[{\"id\":\"313b16dc-2583-42f3-a0f7-487e75d7a7ec\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-700,-100],\"parameters\":{},\"typeVersion\":1},{\"id\":\"fd59c09f-64cd-4e8a-80b1-d1abd9a52a5c\",\"name\":\"Printify - Get Shops\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-60,-100],\"parameters\":{\"url\":\"https://api.printify.com/v1/shops.json\",\"options\":{},\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"vBaDp4RbmXnEx2rj\",\"name\":\"AlexK1919 Printify Header Auth\"}},\"typeVersion\":4.2},{\"id\":\"8fa6a094-02f5-46c4-90d4-c17de302b004\",\"name\":\"Printify - Get Products\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[140,-100],\"parameters\":{\"url\":\"=https://api.printify.com/v1/shops/{{ $json"}
{"prompt": "Create an n8n workflow for: Send daily translated Calvin and Hobbes Comics to Discord How it works Automates the retrieval of Calvin and Hobbes daily comics. Extracts the comic image URL from the website. Translates comic dialogues to English and Korean. Posts the comic and translations to Discord daily. Set up steps Estimated setup time: ~10-15 minutes. Use a Schedule Trigger to automate the workflow at 9 AM daily. Add nodes for parameter setup, HTTP request, data extraction, and integration with Discord. Add detailed notes to each node in the workflow for easy understanding. Nathan Lee | 3 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"4bf26356-9c59-4cee-8eb8-8553b23a172f\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[560,-120],\"parameters\":{\"width\":660,\"height\":460,\"content\":\"![](https://raw.githubusercontent.com/2innnnn0/30-Days-of-ChatGPT/refs/heads/main/datapopcorn_logo_50px.png)\\n# Daily Cartoon (w/ AI Translate)\\n\\n### How it works\\n- Automates the retrieval of Calvin and Hobbes daily comics.\\n- Extracts the comic image URL from the website.\\n- Translates comic dialogues to English and Korean(Other Language)\\n- Posts the comic and translations to Discord daily.\\n\\n### Set up steps\\n- Estimated setup time: ~10-15 minutes.\\n- Use a **Schedule Trigger** to automate the workflow at 9 AM daily.\\n- Add nodes for parameter setup, HTTP request, data extraction, and integration with Discord.\\n- Add detailed notes to each node in the workflow for easy understanding.\"},\"typeVersion\":1},{\"id\":\"52d19472-41b4-4d71-874e-064ef9d6f248\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[620,380],\"parameters\":{\"rule\":{\"interval\":[{\"triggerAtHour\":9}]}},\"typeVersion\":1.2},{\"id\":\"bcc15f37-c048-4d9a-83cd-367856470095\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[1620,380],\"parameters\":{\"text\":\"Please write the original language and Korean together. \\n\\nEXAMPLE)\\nCalvin: \\\"YOU'VE NEVER HAD AN OBLIGATION, AN ASSIGNMENT, OR A DEADLINE IN ALL YOUR LIFE! YOU HAVE NO RESPONSIBILITIES AT ALL! IT MUST BE NICE!\\\" (\ub108\ub294 \ud3c9\uc0dd \ud55c \ubc88\ub3c4 \uc758\ubb34, "}
{"prompt": "Create an n8n workflow for: MongoDB AI Agent - Intelligent Movie Recommendations Who is this for? This workflow is designed for: Database administrators and developers working with MongoDB Content managers handling movie databases Organizations looking to implement AI-powered search and recommendation systems Developers interested in combining LangChain, OpenAI, and MongoDB capabilities What problem does this workflow solve? Traditional database queries can be complex and require specific MongoDB syntax knowledge. This workflow addresses: The complexity of writing MongoDB aggregation pipelines The need for natural language interaction with movie databases The challenge of maintaining user preferences and favorites The gap between AI language models and database operations What this workflow does This workflow creates an intelligent agent that: Accepts natural language queries about movies Translates user requests into MongoDB aggregation pipelines Queries a movie database containing detailed information including: Plot summaries Genre classifications Cast and director information Runtime and release dates Ratings and awards Provides contextual responses using OpenAI's language model Allows users to save favorite movies to the database Maintains conversation context using a window buffer memory Setup Required Credentials: OpenAI API credentials MongoDB connection details Node Configuration: Configure the MongoDB connection in the MongoDBAggregate node Set up the OpenAI Chat Model with your API key Ensure the webhook trigger is properly configured for receiving chat messages Database Requirements: A MongoDB collection named \"movies\" with the specified document structure Proper indexes for efficient querying Appropriate user permissions for read/write operations How to customize this workflow Modify the Document Structure: Update the tool description in the MongoDBAggregate node to match your collection schema Adjust the aggregation pipeline templates for your specific use case Enhance the AI Agent: Customize the prompt in the \"AI Agent - Movie Recommendation\" node Modify the window buffer memory size based on your context needs Add additional tools for more functionality Extend Functionality: Add more MongoDB operations beyond aggregation Implement additional workflows for different types of queries Create custom error handling and validation Add user authentication and rate limiting Integration Options: Connect to external APIs for additional movie data Add webhook endpoints for different platforms Implement caching mechanisms for frequent queries Add data transformation nodes for specific output formats This workflow serves as a foundation that can be adapted to various use cases beyond movie recommendations, such as e-commerce product search, content management systems, or any scenario requiring intelligent database interaction. Pavel Duchovny | 3 months ago | Free", "completion": "{\"id\":\"22PddLUgcjSJbT1w\",\"meta\":{\"instanceId\":\"fa7d5e2425ec76075df7100dbafffed91cc6f71f12fe92614bf78af63c54a61d\",\"templateCredsSetupCompleted\":true},\"name\":\"MongoDB Agent\",\"tags\":[],\"nodes\":[{\"id\":\"d8c07efe-eca0-48cb-80e6-ea8117073c5f\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1300,560],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"TreGPMKr9hrtCvVp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"636de178-7b68-429a-9371-41cf2a950076\",\"name\":\"MongoDBAggregate\",\"type\":\"n8n-nodes-base.mongoDbTool\",\"position\":[1640,540],\"parameters\":{\"query\":\"={{ $fromAI(\\\"pipeline\\\", \\\"The MongoDB pipeline to execute\\\" , \\\"string\\\" , [{\\\"$match\\\" : { \\\"rating\\\" : 5 } }])}}\",\"operation\":\"aggregate\",\"collection\":\"movies\",\"descriptionType\":\"manual\",\"toolDescription\":\"Get from AI the MongoDB Aggregation pipeline to get context based on the provided pipeline, the document structure of the documents is : {\\n \\\"plot\\\": \\\"A group of bandits stage a brazen train hold-up, only to find a determined posse hot on their heels.\\\",\\n \\\"genres\\\": [\\n \\\"Short\\\",\\n \\\"Western\\\"\\n ],\\n \\\"runtime\\\": 11,\\n \\\"cast\\\": [\\n \\\"A.C. Abadie\\\",\\n \\\"Gilbert M. 'Broncho Billy' Anderson\\\",\\n ...\\n ],\\n \\\"poster\\\": \\\"...jpg\\\",\\n \\\"title\\\": \\\"The Great Train Robbery\\\",\\n \\\"fullplot\\\": \\\"Among the earliest existing films in American cinema - notable as the ...\\\",\\n \\\"languages\\\": [\\n \\\"English\\\"\\n ],\\n \\\"released\\\": \\\"date\\\"\\n },\\n \\\"directors\\\": [\\"}
{"prompt": "Create an n8n workflow for: Telegram to Spotify with OpenAI Search music and play to Spotify from Telegram This workflow is a simple demonstration on accessing a message model from Telegram and it makes searching for songs an easy task even if you can't remember the artist or song name. An OpenAI message model tries to figure out the song and sends it to an active Spotify device**. Use case Imagine an office where you play music in the background and the employees can control the music without having to login to the playing account. How it works You describe the song in Telegram. Telegram bot sends the text to n8n. An OpenAI message model tries to find the song. Spotify gets the search query string. First match is then added to queue. -- If there is no match a message is sent to Telegram and the process ends. We change to the next track in the list. We make sure the song starts playing by trying to resume. We fetch the currently playing track. We return \"now playing\" information to Telegram: Song Name - Artist Name - Album Name. Error handling Every Spotify step has it's on error handler under settings where we output the error. Message parser receives the error and sends it to Telegram. Requirements Active workflow* OpenAI API key Telegram bot Spotify account and Oauth2 API Spotify active on a device** .* The Telegram trigger is activated only if this workflow is active. You can however TEST the workflow in the editor by clicking \"Test step\" and then it waits for the Telegram event. When event is received, just step through all steps or just clicking \"Test step\" on the \"Fetch Now Playing\" node. .** You must have a Spotify device active when trying to communicate with a device. Open Spotify and play something - not it is active. Alexander Bentlund | 3 months ago | Free", "completion": "{\"id\":\"F7CfIF10XjXhqbGb\",\"meta\":{\"instanceId\":\"ba8f1362d8ed4c2ce84171d2f481098de4ee775241bdc1660d1dce80434ec7d4\",\"templateCredsSetupCompleted\":true},\"name\":\"Play with Spotify from Telegram\",\"tags\":[],\"nodes\":[{\"id\":\"0395b3e4-94ef-49ea-9b4c-8f908e62f8c6\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[-60,20],\"webhookId\":\"e7aa284b-5eef-4ac1-94bf-8e4d307a3b14\",\"parameters\":{\"updates\":[\"message\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"gblW5oACGEPuccja\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"263edf45-58a0-45e8-91f8-601bc62c7d6f\",\"name\":\"OpenAI - Ask about a track\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[120,-120],\"parameters\":{\"modelId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-4o-mini\",\"cachedResultName\":\"GPT-4O-MINI\"},\"options\":{},\"messages\":{\"values\":[{\"content\":\"=get artist and song name from '{{ $json.message.text }}'. Reply only eg. 'track:song name artist:artist name'\"}]}},\"credentials\":{\"openAiApi\":{\"id\":\"vDcge3EgslxfX3EC\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.6},{\"id\":\"086aef8b-533a-4c33-9952-29d5adb152c8\",\"name\":\"Search track\",\"type\":\"n8n-nodes-base.spotify\",\"onError\":\"continueErrorOutput\",\"position\":[540,-200],\"parameters\":{\"limit\":1,\"query\":\"={{ $json.message.content }}\",\"filters\":{},\"resource\":\"track\",\"operation\":\"search\"},\"credentials\":{\""}
{"prompt": "Create an n8n workflow for: Convert URL HTML to Markdown Format and Get Page Links Use Case Transform web pages into AI-friendly markdown format: You need to process webpage content for LLM analysis You want to extract both content and links from web pages You need clean, formatted text without HTML markup You want to respect API rate limits while crawling pages What this Workflow Does The workflow uses Firecrawl.dev API to process webpages: Converts HTML content to markdown format Extracts all links from each webpage Handles API rate limiting automatically Processes URLs in batches from your database Setup Create a Firecrawl.dev account and get your API key Add your Firecrawl API key to the HTTP Request node's Authorization header Connect your URL database to the input node (column name must be \"Page\") or edit the array in Example fields from data source Configure your preferred output database connection How to Adjust it to Your Needs Modify input source to pull URLs from different databases Adjust rate limiting parameters if needed Customize output format for your specific use case More templates and n8n workflows &gt;&gt;&gt; @simonscrapes simonscrapes | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"6b6a2db47bdf8371d21090c511052883cc9a3f6af5d0d9d567c702d74a18820e\"},\"nodes\":[{\"id\":\"f4570aad-db25-4dcd-8589-b1c8335935de\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-180,3800],\"parameters\":{},\"typeVersion\":1},{\"id\":\"bd481559-85f2-4865-8d85-e50e72369f26\",\"name\":\"Wait\",\"type\":\"n8n-nodes-base.wait\",\"position\":[940,3620],\"webhookId\":\"f10708f0-38c6-4c75-b635-37222d5b183a\",\"parameters\":{\"amount\":45},\"typeVersion\":1.1},{\"id\":\"cc9e9947-19e4-47c5-95b0-a631d688a8b6\",\"name\":\"Sticky Note36\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[549.7858793743054,3709.534654112671],\"parameters\":{\"color\":7,\"width\":327.8244990224782,\"height\":268.48353140372035,\"content\":\"**40 at a time seems to be the memory limit on my server - run until complete with batches of 40 or increase based on your server memory**\\n\"},\"typeVersion\":1},{\"id\":\"9ebbd993-9194-40b1-a98e-352eb3a3f9eb\",\"name\":\"Sticky Note28\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-50.797941767307435,3729.028866440868],\"parameters\":{\"color\":7,\"width\":574.7594700148138,\"height\":248.90718753310907,\"content\":\"**Firecrawl.dev retrieves markdown inc. title, description, links & content. First define the URLs you'd like to scrape**\\n\"},\"typeVersion\":1},{\"id\":\"71c0f975-c0f9-47"}
{"prompt": "Create an n8n workflow for: Text automations using Apple Shortcuts Overview This workflow answers user requests sent via Mac Shortcuts Several Shortcuts call the same webhook, with a query and a type of query Types of query are: translate to english translate to spanish correct grammar (without changing the actual content) make content shorter make content longer How it works Select a text you are writing Launch the shortcut The text is sent to the webhook Depending on the type of request, a different prompt is used Each request is sent to an OpenAI node The workflow responds to the request with the response from GPT Shortcut replace the selected text with the new one For a demo and setup instructions: How to use it Activate the workflow Download this Shortcut template Install the shortcut In step 2 of the shortcut, change the url of the Webhook In Shortcut details, \"add Keyboard Shortcut\" with the key you want to use to launch the shortcut Go to settings, advanced, check \"Allow running scripts\" You are ready to use the shortcut. Select a text and hit the keyboard shortcut you just defined Agent Studio | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"f4f5d195bb2162a0972f737368404b18be694648d365d6c6771d7b4909d28167\"},\"nodes\":[{\"id\":\"b165115d-5505-4e03-bf41-c21320cb8b09\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[80,40],\"parameters\":{\"color\":7,\"width\":681.8337349708484,\"height\":843.1482165886073,\"content\":\"## Workflow: Text automations using Apple Shortcuts\\n\\n**Overview**\\n- This workflow answers user requests sent via Apple Shortcuts\\n- Several Shortcuts call the same webhook, with a query and a type of query\\n- Types of query are:\\n - translate to english\\n - translate to spanish\\n - correct grammar (without changing the actual content)\\n - make content shorter\\n - make content longer\\n\\n\\n**How it works**\\n- Select a text you are writing\\n- Launch the shortcut\\n- The text is sent to the webhook\\n- Depending on the type of request, a different prompt is used\\n- Each request is sent to an OpenAI node\\n- The workflow responds to the request with the response from GPT\\n- Shortcut replace the selected text with the new one\\n\\n**How to use it**\\n- Activate the workflow\\n- Download [this Shortcut template](https://drive.usercontent.google.com/u/0/uc?id=16zs5iJX7KeX_4e0SoV49_KfbU7-EF0NE&export=download)\\n- Install the shortcut\\n- In step 2 of the shortcut, change the url of the Webhook\\n- In Shortcut details, \\\"add Keyboard Shortcut\\\" with the key you want to use to launch the shortcut\\n- Go to settings, advanced, check \\\"Allow running scripts\\\"\\n- You are ready to use the shortcut. Select a text and hit the keyboard shortcut you just defined\\n\\n\\n**Notes**\\n- If you use rich formatting, you'll have to test multiple ways to replace characters in the output. For example, you"}
{"prompt": "Create an n8n workflow for: \ud83d\ude80 Local Multi-LLM Testing & Performance Tracker \ud83d\ude80 Local Multi-LLM Testing & Performance Tracker This workflow is perfect for developers, researchers, and data scientists benchmarking multiple LLMs with LM Studio. It dynamically fetches active models, tests prompts, and tracks metrics like word count, readability, and response time, logging results into Google Sheets. Easily adjust temperature \ud83d\udd25 and top P \ud83c\udfaf for flexible model testing. Level of Effort: \ud83d\udfe2 Easy \u2013 Minimal setup with customizable options. Setup Steps: Install LM Studio and configure models. Update IP to connect to LM Studio. Create a Google Sheet for result tracking. Key Outcomes: Benchmark LLM performance. Automate results in Google Sheets for easy comparison. Version 1.0 Wildkick | 5 months ago | Free", "completion": "{\"id\":\"WulUYgcXvako9hBy\",\"meta\":{\"instanceId\":\"d6b86682c7e02b79169c1a61ad0484dcda5bc8b0ea70f1a95dac239c2abfd057\",\"templateCredsSetupCompleted\":true},\"name\":\"Testing Mulitple Local LLM with LM Studio\",\"tags\":[{\"id\":\"RkTiZTdbLvr6uzSg\",\"name\":\"Training\",\"createdAt\":\"2024-06-18T16:09:35.806Z\",\"updatedAt\":\"2024-06-18T16:09:35.806Z\"},{\"id\":\"W3xdiSeIujD7XgBA\",\"name\":\"Template\",\"createdAt\":\"2024-06-18T22:15:34.874Z\",\"updatedAt\":\"2024-06-18T22:15:34.874Z\"}],\"nodes\":[{\"id\":\"08c457ef-5c1f-46d8-a53e-f492b11c83f9\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1600,420],\"parameters\":{\"color\":6,\"width\":478.38709677419376,\"height\":347.82258064516134,\"content\":\"## \ud83e\udde0Text Analysis\\n### Readability Score Ranges:\\nWhen testing model responses, readability scores can range across different levels. Here\u2019s a breakdown:\\n\\n- **90\u2013100**: Very easy to read (5th grade or below)\\n- **80\u201389**: Easy to read (6th grade)\\n- **70\u201379**: Fairly easy to read (7th grade)\\n- **60\u201369**: Standard (8th to 9th grade)\\n- **50\u201359**: Fairly difficult (10th to 12th grade)\\n- **30\u201349**: Difficult (College)\\n- **0\u201329**: Very difficult (College graduate)\\n- **Below 0**: Extremely difficult (Post-graduate level)\\n\"},\"typeVersion\":1},{\"id\":\"7801734c-5eb9-4abd-b234-e406462931f7\",\"name\":\"Get Models\",\"type\":\"n8n-nodes-base.http"}
{"prompt": "Create an n8n workflow for: create e-mail responses with fastmail and OpenAI Workflow Description: This n8n workflow automates the drafting of email replies for Fastmail using OpenAI's GPT-4 model. Here\u2019s the overall process: Email Monitoring: The workflow continuously monitors a specified IMAP inbox for new, unread emails. Email Data Extraction: When a new email is detected, it extracts relevant details such as the sender, subject, email body, and metadata. AI Response Generation: The extracted email content is sent to OpenAI's GPT-4, which generates a personalized draft response. Get Fastmail Session and Mailbox IDs: Connects to the Fastmail API to retrieve necessary session details and mailbox IDs. Draft Identification: Identifies the \"Drafts\" folder in the mailbox. Draft Preparation: Compiles all the necessary information to create the draft, including the generated response, original email details, and specified recipient. Draft Uploading: Uploads the prepared draft email to the \"Drafts\" folder in the Fastmail mailbox. Prerequisites: IMAP Email Account: You need to configure an IMAP email account in n8n to monitor incoming emails. Fastmail API Credentials: A Fastmail account with JMAP API enabled. You should set up HTTP Header authentication in n8n with your Fastmail API credentials. OpenAI API Key: An API key from OpenAI to access GPT-4. Make sure to configure the OpenAI credentials in n8n. Configuration Steps: Email Trigger (IMAP) Node: Provide your email server settings and credentials to monitor emails. HTTP Request Nodes for Fastmail: Set up HTTP Header authentication in n8n using your Fastmail API credentials. Replace the httpHeaderAuth credential IDs with your configured credential IDs. OpenAI Node: Configure the OpenAI API key in n8n. Replace the openAiApi credential ID with your configured credential ID. By following these steps and setting up the necessary credentials, you can seamlessly automate the creation of email drafts in response to new emails using AI-generated content. This workflow helps improve productivity and ensures timely, personalized communication. Vitali | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"04ab549d8bbb435ec33b81e4e29965c46cf6f0f9e7afe631018b5e34c8eead58\"},\"nodes\":[{\"id\":\"082d1828-72b1-48c0-8426-c8051c29f0db\",\"name\":\"Session\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-20,-20],\"parameters\":{\"url\":\"https://api.fastmail.com/jmap/session\",\"options\":{},\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"3IRsYkeB2ofrwQjv\",\"name\":\"Fastmail\"}},\"typeVersion\":4.2},{\"id\":\"d7dc4c50-c8fc-4999-918d-5d357567ed14\",\"name\":\"Get Mailbox IDs\",\"type\":\"n8n-nodes-base.httpRequest\",\"notes\":\"https://api.fastmail.com/.well-known/jmap\\n\\nhttps://api.fastmail.com/jmap/session\",\"position\":[200,-20],\"parameters\":{\"url\":\"https://api.fastmail.com/jmap/api/\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"using\\\": [\\\"urn:ietf:params:jmap:core\\\", \\\"urn:ietf:params:jmap:mail\\\"],\\n \\\"methodCalls\\\": [\\n [\\n \\\"Mailbox/get\\\",\\n {\\n \\\"accountId\\\": \\\"{{ $('Session').item.json.primaryAccounts['urn:ietf:params:jmap:mail'] }}\\\"\\n },\\n \\\"c0\\\"\\n ]\\n ]\\n }\",\"sendBody\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"headerParameters\":{\"parameters\":[{\"name\":\"Content-Type\",\"value\":\"application/json\"},{\"name\":\"Accept\",\"value\":\"application/json\"}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"3IRsYkeB2ofrwQjv\",\"name\":\"Fastmail\"}},\"typeVersion\":4.2},{\"id\":\"31be"}
{"prompt": "Create an n8n workflow for: Manipulate PDF with Adobe developer API Adobe developer API Did you know that Adobe provides an API to perform all sort of manipulation on PDF files : Split PDF, Combine PDF OCR Insert page, delete page, replace page, reorder page Content extraction (text content, tables, pictures) ... The free tier allows up to 500 PDF operation / month. As it comes directly from Adobe, it works often better than other alternatives. Adobe documentation: https://developer.adobe.com/document-services/docs/overview/pdf-services-api/howtos/ https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/gettingstarted/ What does this workflow do The API is a bit painful to use. To perform a transformation on a PDF it requires to Authenticate and get a temporal token Register a new asset (file) Upload you PDF to the registered asset Perform a query according to the transformation requested Wait for the query to be proccessed by Adobe backend Download the result This workflow is a generic wrapper to perform all these steps for any transformation endpoint. I usually use it from other workflow with an Execute Workflow node. Examples are given in the workflow. Example use case This service is useful for example to clean PDF data for an AI / RAG system. My favorite use-case is to extract table as images and forward images to an AI for image recognition / description which is often more accuarate than feedind raw tabular data to a LLM. digi-stud.io | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cd478e616d2616186f4f92b70cfe0c2ed95b5b209f749f2b873b38bdc56c47c9\"},\"nodes\":[{\"id\":\"f4b1bdd8-654d-4643-a004-ff1b2f32b5ae\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[580,1100],\"parameters\":{},\"typeVersion\":1},{\"id\":\"d6b1c410-81c3-486d-bdcb-86a4c6f7bf9e\",\"name\":\"Create Asset\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1940,580],\"parameters\":{\"url\":\"https://pdf-services.adobe.io/assets\",\"method\":\"POST\",\"options\":{\"redirect\":{\"redirect\":{}}},\"sendBody\":true,\"sendHeaders\":true,\"authentication\":\"genericCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"mediaType\",\"value\":\"application/pdf\"}]},\"genericAuthType\":\"httpHeaderAuth\",\"headerParameters\":{\"parameters\":[{\"name\":\"Authorization\",\"value\":\"=Bearer {{ $json.access_token }}\"}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"PU8GmSwXswwM1Fzq\",\"name\":\"Adobe API calls\"}},\"typeVersion\":4.1},{\"id\":\"9e900a45-d792-4dc5-938c-0d5cdfd2e647\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[1140,440],\"parameters\":{},\"typeVersion\":1},{\"id\":\"859f369d-f36f-4c3f-a50d-a17214fef2a3\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[20,140],\"parameters\":{\"color\":5,\"width\":667.6107231291055,\"height\":715.2927406867177,\"content\":\"# Adobe API Wrapper\\n\\nSee Adobe documentation:\\n- https://developer.adobe.com/document-services/docs"}
{"prompt": "Create an n8n workflow for: Visual Regression Testing with Apify and AI Vision Model This n8n workflow is a proof-of-concept template exploring how we might work with multimodal LLMs and their multi-image analysis capabilities. In this demo, we compare 2 screenshots of a webpage taken at different timestamps and pass both to our multimodal LLM for a visual comparison of differences. Handling multiple binary inputs (ie. images) in an AI request is supported by n8n's basic LLM node. How it works This template is intended to run as 2 parts: first to generate the base screenshots and next to run the visual regression test which captures fresh screenshots. Starting with a list of webpages captured in a Google sheet, base screenshots are captured for each using a external web scraping service called Apify.com (I prefer Apify but feel free to use whichever web scraping service available to you) These base screenshots are uploaded to Google Drive and will be referenced later when we run our testing. Phase 2 of the workflow, we'll use a scheduled trigger to fire sometime in the future which will reuse our web scraping service to generate fresh screenshots of our desired webpages. Next, re-download our base screenshots in parallel and with both old and new captures, we'll pass these to our LLM node. In the LLM node's options, we'll define 2 \"user message\" inputs with the type of binary (data) for our images. Finally, we'll prompt our LLM with our testing criteria and capture the regressions detected. Note, results will vary depending on which LLM you use. A final report can be generated using the LLM's output and is uploaded to Linear. Requirements Apify.com API key for web screenshotting service Google Drive and Sheets access to store list of webpages and captures Customising this workflow Have your own preferred web screenshotting service? Feel free to swap out Apify with your service of choice. If the web screenshot is too large, it may prove difficult for the LLM to spot differences with precision. Try splitting up captures into smaller images instead. Jimleuk | 5 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"cb62c9a5-2f43-4328-af94-84c2cb731d9c\",\"name\":\"Base Image\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[260,660],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"={{ $json.base }}\"},\"options\":{\"binaryPropertyName\":\"data_1\"},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"yOwz41gMQclOadgu\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"b1c304cc-9949-441a-ac2a-275c8d4c51fc\",\"name\":\"Google Gemini Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatGoogleGemini\",\"position\":[1120,900],\"parameters\":{\"options\":{},\"modelName\":\"models/gemini-1.5-pro-latest\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"dSxo6ns5wn658r8N\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":1},{\"id\":\"964d94bf-be2a-424e-ab0e-c1c1fe260ebd\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1320,900],\"parameters\":{\"schemaType\":\"manual\",\"inputSchema\":\"{\\n \\\"type\\\": \\\"array\\\",\\n \\\"items\\\": {\\n \\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n\\t\\t\\\"type\\\": {\\n \\t\\t\\\"type\\\": \\\"string\\\",\\n \\\"description\\\": \\\"type of regression. One of text, number, image, color or position.\\\"\\n \\t\\t},\\n\\t\\t\\\"description\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"previous_state\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"current_state\\\": { \\\"type\\\":"}
{"prompt": "Create an n8n workflow for: Extract spending history from gmail to google sheet How it works Fetch transaction notification emails (including attachments) Clean up data Let AI (Basic LLM Chain node) generate bookkeeping item Send to Google sheet Details The example fetch email from Gmail lables, suggested using filters to automatically orgianize email into the labels Data will send to \"raw data\" sheet Example google sheet: https://docs.google.com/spreadsheets/d/1_IhdHj8bxtsfH2MRqKuU2LzJuzm4DaeKSw46eFcyYts/edit?gid=1617968863#gid=1617968863 hana | 5 months ago | Free", "completion": "{\"id\":\"nkMjcOC4hpte1a0t\",\"meta\":{\"instanceId\":\"3986dc65ca3ddc4ee46e71fc194b0a9d4ef46d960a5e71624f9f7eaa198213cb\",\"templateCredsSetupCompleted\":true},\"name\":\"Extract spend details (template)\",\"tags\":[{\"id\":\"9mCuuNEpnYNvVzb8\",\"name\":\"Finance\",\"createdAt\":\"2024-09-15T07:22:30.749Z\",\"updatedAt\":\"2024-09-15T07:22:30.749Z\"}],\"nodes\":[{\"id\":\"8e1e0861-9f06-4fe2-a9c1-423bab246959\",\"name\":\"Get invoice\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[600,380],\"parameters\":{\"simple\":false,\"filters\":{\"labelIds\":[\"Label_7885838942566773656\"]},\"options\":{\"downloadAttachments\":true},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"fegneFqi8XJX3NJH\",\"name\":\"Gmail account (hana@hanamizuki.tw)\"}},\"typeVersion\":1.1},{\"id\":\"364fe355-672a-4074-800a-a7496c4fb1b2\",\"name\":\"Get payment\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[600,580],\"parameters\":{\"simple\":false,\"filters\":{\"labelIds\":[\"Label_371722915607774622\"]},\"options\":{\"downloadAttachments\":true},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"fegneFqi8XJX3NJH\",\"name\":\"Gmail account (hana@hanamizuki.tw)\"}},\"typeVersion\":1.1},{\"id\":\"e3218faf-2486-46e0-bf43-3bc52927e2bd\",\"name\":\"Extract invoice\",\"type\":\"n8n-nodes-base.extractFromFile\",\"notes\":\"No attachements\",\"onError\":\"continueRegularOutput\",\"position\":[820,380],\""}
{"prompt": "Create an n8n workflow for: KB Tool - Confluence Knowledge Base Enhance Query Resolution with the Knowledge Base Tool! Our KB Tool - Confluence KB is crafted to seamlessly integrate into the IT Ops AI SlackBot Workflow, enhancing the IT support process by enabling sophisticated search and response capabilities via Slack. Workflow Functionality: Receive Queries**: Directly accepts user queries from the main workflow, initiating a dynamic search process. AI-Powered Query Transformation**: Utilizes OpenAI's models or local ai to refine user queries into searchable keywords that are most likely to retrieve relevant information from the Knowledge Base. Confluence Integration**: Executes searches within Confluence using the refined keywords to find the most applicable articles and information. Deliver Accurate Responses**: Gathers essential details from the Confluence results, including article titles, links, and summaries, preparing them to be sent back to the parent workflow for final user response. To view a demo video of this workflow in action, click here. Quick Setup Guide: Ensure correct configurations are set for OpenAI and Confluence API integrations. Customize query transformation logic as per your specific Knowledge Base structure to improve search accuracy. Need Help? Dive into our Documentation or get support from the Community Forum! Deploy this tool to provide precise and informative responses, significantly boosting the efficiency and reliability of your IT support workflow. Angel Menendez | 6 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"f1142274-898d-43da-a7ff-2b2e03f2dc73\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[1220,840],\"parameters\":{},\"typeVersion\":1},{\"id\":\"1f407421-2dd6-4e0c-bc74-cfb291e475ed\",\"name\":\"Query Confluence\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1640,840],\"parameters\":{\"url\":\"https://n8n-labs.atlassian.net/wiki/rest/api/search\",\"options\":{},\"sendQuery\":true,\"sendHeaders\":true,\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpBasicAuth\",\"queryParameters\":{\"parameters\":[{\"name\":\"cql\",\"value\":\"=text ~ \\\"{{ $json.query }}\\\"\"}]},\"headerParameters\":{\"parameters\":[{\"name\":\"accept\",\"value\":\"application/json\"}]}},\"credentials\":{\"httpBasicAuth\":{\"id\":\"B1Cj4Uh9d9WKWxBO\",\"name\":\"Confluence API Key\"}},\"typeVersion\":4.2},{\"id\":\"f1ab7e79-6bd8-4b87-b6dc-96f9d46cdd16\",\"name\":\"Return Tool Response\",\"type\":\"n8n-nodes-base.set\",\"position\":[2040,840],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"c1d46e59-9340-43f3-bc2a-fbd4e0def74f\",\"name\":\"response\",\"type\":\"string\",\"value\":\"=\\\"Title\\\": \\\"{{ $json.results[0].content.title }}\\\"\\n\\\"Link\\\": \\\"{{ $json._links.base }}{{ $json.results[0].content._links.webui }}\\\"\\n\\\"Content\\\": {{ $json[\\\"results\\\"][0][\\\"excerpt\\\"] }}\\nWhen users request the password, make sure to send them the link above to reset it in markdown"}
{"prompt": "Create an n8n workflow for: Use AI to organize your Todoist Inbox How it works This workflow adds a priority to each Todoist item in your inbox, based on a list of projects that you add in the workflow. Setup Add your Todoist credentials Add your OpenAI credentials Set your project names and add priority Mutasem | 6 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"d45cf237-dbbc-48ed-a7f0-fa9506ae1d67\",\"name\":\"Update priority in todoist\",\"type\":\"n8n-nodes-base.todoist\",\"position\":[2060,520],\"parameters\":{\"taskId\":\"={{ $('Get inbox tasks').item.json.id }}\",\"operation\":\"update\",\"updateFields\":{\"priority\":\"={{ $('Your Projects').first().json.projects[$json.message.content] }}\"}},\"credentials\":{\"todoistApi\":{\"id\":\"1\",\"name\":\"Todoist account\"}},\"retryOnFail\":true,\"typeVersion\":2,\"waitBetweenTries\":5000},{\"id\":\"4d0ebf98-5a1d-4dfd-85df-da182b3c5099\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[600,520],\"parameters\":{\"rule\":{\"interval\":[{}]}},\"typeVersion\":1.2},{\"id\":\"a950e470-6885-42f4-9b17-7b2c2525d3e4\",\"name\":\"Get inbox tasks\",\"type\":\"n8n-nodes-base.todoist\",\"position\":[1020,520],\"parameters\":{\"filters\":{\"projectId\":\"938017196\"},\"operation\":\"getAll\",\"returnAll\":true},\"credentials\":{\"todoistApi\":{\"id\":\"1\",\"name\":\"Todoist account\"}},\"retryOnFail\":true,\"typeVersion\":2,\"waitBetweenTries\":5000},{\"id\":\"093bcb2e-79b7-427e-b13d-540a5b28f427\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[540,200],\"parameters\":{\"color\":3,\"width\":358.6620209059232,\"height\":256.5853658536585,\"content\":\"## \ud83d\udcab To setup this template\\n\\n1. Add your Todoist credentials\\n2. Add your OpenAI credentials\\n3. Set your project names and add priority\"},\"typeVersion\":1},{\"id\":\"430290e7-1732-46fe-a38d-fa6dc7f78a26\",\"name\":\"St"}
{"prompt": "Create an n8n workflow for: Customer Insights with Qdrant, Python and Information Extractor This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings: Survey Insights Customer Insights Community Insights This template demonstrates the Customer Insights scenario where Trustpilot reviews can be quickly grouped by similarity and an AI agent can generate insights on those groupings. With this workflow, marketers can save days and even weeks of work breaking down their own or competitor reviews and identify frequently mentioned positives and negatives. Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vQ6ipJnXWXgr5wlUJnhioNpeYrxaIpsRYZCwN3C-fFXumkbh9TAsA_JzE0kbv7DcGAVIP7az0L46_2P/pubhtml How it works Trustpilot reviews are scraped for a particular company using the HTTP request node. Reviews are then inserted into a Qdrant collection carefully tagged with the question and Trustpilot metadata. Reviews are fetched and put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups. Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for. The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the marketer. Requirements Qdrant Vectorstore for storing embeddings. OpenAI account for embeddings and LLM. Customising the Template Adjust clustering parameters which make sense for your data. Consider expanding date range of reviews for insights over common intervals: 3mth, 6mth and YTD. Jimleuk | 7 months ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"63501cc8-77c9-4037-9f70-da23b6d20b03\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[280,440],\"parameters\":{},\"typeVersion\":1},{\"id\":\"00de989c-d9e9-4b42-b5db-7097800a6017\",\"name\":\"Zip Entries\",\"type\":\"n8n-nodes-base.set\",\"position\":[1380,360],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"833a554d-2b39-4160-9348-18b17b28ce30\",\"name\":\"data\",\"type\":\"array\",\"value\":\"={{ \\n $json.review_author.map((review_author, idx) => ({\\n review_author,\\n review_author_reviews_count: $json.review_author_reviews_count[idx].replace(' reviews', '').toInt(),\\n review_country: $json.review_country[idx],\\n review_date: $json.review_date[idx].toDate(),\\n review_date_of_experience: $json.review_date_of_experience[idx].replace('Date of experience: ', '').toDate(),\\n review_rating: $json.review_rating[idx].toInt(),\\n review_text: $json.review_text[idx],\\n review_title: $json.review_title[idx],\\n review_url: $('Get TrustPilot Page').params.url.match(/https:\\\\/\\\\/[^/]+/) + $json.review_url[idx],\\n }))\\n}}\"}]}},\"typeVersion\":3.4},{\"id\":\"9290e116-c001-49d5-ae4c-d91cd246f2c2\",\"name\":\"Extract Reviews\",\"type\":\"n8n-nodes-base.html"}
{"prompt": "Create an n8n workflow for: Survey Insights with Qdrant, Python and Information Extractor This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings: Survey Insights Customer Insights Community Insights This template demonstrates the Survey Insights scenario where survey participant responses can be quickly grouped by similarity and an AI agent can generate insights on those groupings. With this workflow, researchers can save days and even weeks of work breaking down cohorts of participants and identify frequently mentioned positives and negatives. Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vT6m8XH8JWJTUAfwojc68NAUGC7q0lO7iV738J7aO5fuVjiVzdTRRPkMmT1C4N8TwejaiT0XrmF1Q48/pubhtml# How it works All survey questions and responses are imported from a Google Sheet. Responses are then inserted into a Qdrant collection carefully tagged with the question and survey metadata. For each question, all relevant response are put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups. Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for. The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the researcher. Requirements Survey data and format as shown in the attached google sheet. Qdrant Vectorstore for storing embeddings. OpenAI account for embeddings and LLM. Customising the Template Adjust clustering parameters which make sense for your data. Add more clusters for open-ended questions and less clusters when responses are multiple choice. Jimleuk | 7 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"0404384b-10b6-4666-84a4-8870db30c607\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[1220,280],\"parameters\":{\"model\":\"text-embedding-3-small\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"a6741f04-5a5b-47a9-ac08-eb562f9f6052\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[1340,280],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"question\",\"value\":\"={{ $json.question }}\"},{\"name\":\"participant\",\"value\":\"={{ $json.participant }}\"},{\"name\":\"survey\",\"value\":\"={{ $('Get Survey Results').params.documentId.cachedResultName }}\"}]}},\"jsonData\":\"={{ $json.answer }}\",\"jsonMode\":\"expressionData\"},\"typeVersion\":1},{\"id\":\"7663c3dd-f713-4034-bef6-0c000285f54f\",\"name\":\"Convert to Question Answer Pairs\",\"type\":\"n8n-nodes-base.set\",\"position\":[720,160],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"6b593ffb-ffbd-4cf5-a508-cd4f2a6d1004\",\"name\":\"data\",\"type\":\"array\",\"value\":\"={{\\n Object.keys($json)\\n .filter(key => !['row_number', 'Participant'].includes(key))\\n .map(key => ({ question: key, answer: $json[key], participant: $json.Participant }))\\n}}\"}]}},\"typeVersion\":3.4},{\"id\":\"84873f0c-81"}
{"prompt": "Create an n8n workflow for: Query n8n Credentials with AI SQL Agent This n8n workflow is a fun way to query and search over your credentials on your n8n instance. Good to know Your credentials should remain safe as this workflow does not decrypt or use any decrypted data. Example Usage \"Which workflows are using Slack and Google Calendar?\" \"Which workflows have AI in their name but are not using openAI?\" How it works Using the n8n API, it fetches all workflow data on the instance. Workflow data contains references to credentials used so this will be extracted. With some necessary reformatting, the workflows and their credentials metadata are stored to a SQLite database. Next, an AI agent is used with a custom SQL tool that reads the SQLite database created in the previous step. The AI agent is instructed to perform SQL queries against our workflow credential table when asked about credentials by the user. Requirements You'll need an n8n API key. Please note that only workflows will be scoped to your API key. Customising the workflow Add extra table fields to the SQLite database to answer even more complex queries such as: workflow status to differentiate between active and inactive workflows. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"382dddd4-da50-49fa-90a2-f7d6d160afdf\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[920,280],\"parameters\":{},\"typeVersion\":1},{\"id\":\"efa8f415-62f7-43b3-a76a-a2eabf779cb8\",\"name\":\"Map Workflows & Credentials\",\"type\":\"n8n-nodes-base.set\",\"position\":[1360,280],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"0fd19a68-c561-4cc2-94d6-39848977e6d2\",\"name\":\"workflow_id\",\"type\":\"string\",\"value\":\"={{ $json.id }}\"},{\"id\":\"a81f9e6f-9c78-4c3d-9b79-e820f8c5ba29\",\"name\":\"workflow_name\",\"type\":\"string\",\"value\":\"={{ $json.name }}\"},{\"id\":\"58ab0f2f-7598-48de-bea1-f3373c5731fe\",\"name\":\"credentials\",\"type\":\"array\",\"value\":\"={{ $json.nodes.map(node => node.credentials).compact().reduce((acc,cred) => { const keys = Object.keys(cred); const items = keys.map(key => ({ type: key, ...cred[key] })); acc.push(...items); return acc; }, []) }}\"}]}},\"typeVersion\":3.3},{\"id\":\"9e9b4f9c-12b7-47ba-8cf4-a9818902a538\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1084,252],\"parameters\":{\"width\":216,\"height\":299.56273929030715,\"content\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\"}
{"prompt": "Create an n8n workflow for: Enhance Customer Chat by Buffering Messages with Twilio and Redis This n8n workflow demonstrates a simple approach to improve chat UX by staggering an AI Agent's reply for users who send in a sequence of partial messages and in short bursts. How it works Twilio webhook receives user's messages which are recorded in a message stack powered by Redis. The execution is immediately paused for 5 seconds and then another check is done against the message stack for the latest message. The purpose of this check lets use know if the user is sending more messages or if they are waiting for a reply. The execution is aborted if the latest message on the stack differs from the incoming message and continues if they are the same. For the latter, the agent receives the buffered messages up to that point and is able to respond to them in a single reply. Requirements A Twilio account and SMS-enabled phone number to receive messages. Redis instance for the messages stack. OpenAI account for the language model. Customising the workflow This workflow should work for other common messaging platforms such as Whatsapp and Telegram. 5 seconds too long or too short? Adjust the wait threshold to suit your customers. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"d61d8ff3-532a-4b0d-a5a7-e02d2e79ddce\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2660,480],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"b6d5c1cf-b4a1-4901-b001-0c375747ee63\",\"name\":\"No Operation, do nothing\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[1660,520],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f4e08e32-bb96-4b5d-852e-26ad6fec3c8c\",\"name\":\"Add to Messages Stack\",\"type\":\"n8n-nodes-base.redis\",\"position\":[1340,200],\"parameters\":{\"list\":\"=chat-buffer:{{ $json.From }}\",\"tail\":true,\"operation\":\"push\",\"messageData\":\"={{ $json.Body }}\"},\"credentials\":{\"redis\":{\"id\":\"zU4DA70qSDrZM1El\",\"name\":\"Redis account\"}},\"typeVersion\":1},{\"id\":\"181ae99e-ebe7-4e99-b5a5-999acc249621\",\"name\":\"Should Continue?\",\"type\":\"n8n-nodes-base.if\",\"position\":[1660,360],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"ec39573f-f92a-4fe4-a832-0a137de8e7d0\",\"operator\":{\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Get Latest Message Stack').item.json.messages.last() }}\",\""}
{"prompt": "Create an n8n workflow for: Generating Image Embeddings via Textual Summarisation This n8n template demonstrates an approach to image embeddings for purpose of building a quick image contextual search. Use-cases could for a personal photo library, product recommendations or searching through video footage. How it works A photo is imported into the workflow via Google Drive. The photo is processed by the edit image node to extract colour information. This information forms part of our semantic metadata used to identify the image. The photo is also processed by a vision-capable model which analyses the image and returns a short description with semantic keywords. Both pieces of information about the image are combined with the metadata of the image to form a document describing the image. This document is then inserted into our vector store as a text embedding which is associated with our image. From here, the user can query the vector store as they would any document and the relevant image references and/or links should be returned. Requirements Google account to download image files from Google Drive. OpenAI account for the Vision-capable AI and Embedding models. Customise this workflow Text summarisation is just one of many techniques to generate image embeddings. If the results are unsatisfactory, there are dedicated image embedding models such as Google's vertex AI multimodal embeddings. Jimleuk | 7 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"141638a4-b340-473f-a800-be7dbdcff131\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[695,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"6ccdaca5-f620-4afa-bed6-92f3a450687d\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[875,380],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"0B43u2YYOTJR2cC1BRkptZ3N4QTk4NEtxRko5cjhKUUFyemw0\",\"cachedResultUrl\":\"https://drive.google.com/file/d/0B43u2YYOTJR2cC1BRkptZ3N4QTk4NEtxRko5cjhKUUFyemw0/view?usp=drivesdk&resourcekey=0-UJ8EfTMMBRNVyBb6KhN2Tg\",\"cachedResultName\":\"0B0A0255.jpeg\"},\"options\":{},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"yOwz41gMQclOadgu\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"b0c2f7a4-a336-4705-aeda-411f2518aaef\",\"name\":\"Get Color Information\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[1200,200],\"parameters\":{\"operation\":\"information\"},\"typeVersion\":1},{\"id\":\"3e42b3f1-6900-4622-8c0d-2d9a27a7e1c9\",\"name\":\"Resize Image\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[1200,580],\"parameters\":{\"width\":512,\"height\":512,\"options\":{},\"operation\":\"resize\",\"res"}
{"prompt": "Create an n8n workflow for: Introduction to the HTTP Tool This n8n template showcases the new HTTP tool released in version 1.47.0. Overall, the tool helps simplify AI Agent workflows where custom sub-workflows were performing the same simple http requests. Comparisons 1. AI agent that can scrape webpages Remake of https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/ Changes: Replaces Execute Workflow Tool and Subworkflow Replaces Response Formatting 2. Allow your AI to call an API to fetch data Remake of https://n8n.io/workflows/2094-allow-your-ai-to-call-an-api-to-fetch-data/ Changes: Replaces Execute Workflow Tool and Subworkflow Replaces Manual Query Params Definitions Replaces Response Formatting Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"abccacce-bbdc-428e-94e0-19996c5bfe02\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1720,160],\"parameters\":{\"color\":7,\"width\":319.5392879244982,\"height\":218.88813194060202,\"content\":\"### AI agent that can scrape webpages\\nRemake of https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/\\n\\n**Changes**:\\n* Replaces Execute Workflow Tool and Subworkflow\\n* Replaces Response Formatting\"},\"typeVersion\":1},{\"id\":\"9fc05c79-5a2d-4ac4-a4f5-32b9c1b385e1\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1340,340],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"45c9bdaf-d51e-4026-8911-4b04c5473b06\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1720,560],\"parameters\":{\"color\":7,\"width\":365.9021913627245,\"height\":245.35379866205295,\"content\":\"### Allow your AI to call an API to fetch data\\nRemake of https://n8n.io/workflows/2094-allow-your-ai-to-call-an-api-to-fetch-data/\\n\\n**Changes**:\\n* Replaces Execute Workflow Tool and Subworkflow\\n* Replaces Manual Query Params Definitions\\n* Replaces Response Formatting\"},\"typeVersion\":1},{\"id\":\"bc1754"}
{"prompt": "Create an n8n workflow for: Build a Tax Code Assistant with Qdrant, Mistral.ai and OpenAI This n8n workflows builds another example of creating a knowledgebase assistant but demonstrates how a more deliberate and targeted approach to ingesting the data can produce much better results for your chatbot. In this example, a government tax code policy document is used. Whilst we could split the document into chunks by content length, we often lose the context of chapters and sections which may be required by the user. Our approach then is to first split the document into chapters and sections before importing into our vector store. Additionally, using metadata correctly is key to allow filtering and scoped queries. Example Human: \"Tell me about what the tax code says about cargo for intentional commerce?\" AI: \"Section 11.25 of the Texas Property Tax Code pertains to \"MARINE CARGO CONTAINERS USED EXCLUSIVELY IN INTERNATIONAL COMMERCE.\" In this section, a person who is a citizen of a foreign country or an en...\" How it works The tax code policy document is downloaded as a zip file from the government website and its pages are extracted as separate chapters. Each chapter is then parsed and split into its sections using data manipulation expressions. Each section is then inserted into our Qdrant vector store tagged with its source, chapter and section numbers as metadata. When our AI Agent needs to retrieve data from our vector store, we use a custom workflow tool to perform the query to Qdrant. Because we're relying on Qdrant's advanced filtering capabilities, we perform the search using the Qdrant API rather than the Qdrant node. When the AI Agent, needs to pull full wording or extracts, we can use Qdrant's scroll API and metadata filtering to do so. This makes Qdrant behave like a key-value store for our document. Requirements A Qdrant instance is required for the vector store and specifically for it's filtering functionality. Mistral.ai account for Embeddings and AI models. Customising this workflow Depending on your use-case, consider returning actual PDF pages (or links) to the user for the extra confirmation and to build trust. Not using Mistral? You are able to replace but note to match the distance and dimension size of Qdrant collection to your chosen embedding model. Jimleuk | 7 months ago | Free +5", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"1bb3c94e-326e-41ca-82e4-102a598dba39\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-320,300],\"parameters\":{},\"typeVersion\":1},{\"id\":\"751b283b-ea88-4fcd-ace3-3c86631f8876\",\"name\":\"Embeddings Mistral Cloud\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsMistralCloud\",\"position\":[1760,560],\"parameters\":{\"options\":{}},\"credentials\":{\"mistralCloudApi\":{\"id\":\"EIl2QxhXAS9Hkg37\",\"name\":\"Mistral Cloud account\"}},\"typeVersion\":1},{\"id\":\"f0851949-1036-4040-84df-61295cc5db74\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[1900,560],\"parameters\":{\"options\":{\"metadata\":{\"metadataValues\":[{\"name\":\"chapter\",\"value\":\"={{ $('For Each Section...').item.json.chapter }}\"},{\"name\":\"section\",\"value\":\"={{ $('For Each Section...').item.json.label }}\"},{\"name\":\"=title\",\"value\":\"={{ $('For Each Section...').item.json.title }}\"},{\"name\":\"content_order\",\"value\":\"={{ $itemIndex }}\"}]}},\"jsonData\":\"={{ $json.content }}\",\"jsonMode\":\"expressionData\"},\"typeVersion\":1},{\"id\":\"41d10b61-9fbe-446e-a65a-0db6e0116e5b\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[1920,680],\"parameters\":{\"options\":{},\"chunkSize\":2000},\"typeVersion\":1},{\"id\":\"a1ecb096-4d31-4993-b801-ca"}
{"prompt": "Create an n8n workflow for: Reconcile Rent Payments with Local Excel Spreadsheet and OpenAI This n8n workflow is designed to work on the local network and assists with reconciling downloaded bank statements with internal tenant records to quickly highlight any issues with payments such as missed or late payments or those of incorrect amounts. This assistant can then generate a report to quick flag attention to ensure remedial action is taken. How it works The workflow monitors a local network drive to watch for new bank statements that are added. This bank statement is then imported into the n8n workflow, its contents extracted and sent to the AI Agent. The AI Agent analyses the line items to identify the dates and any incoming payments from tenants. The AI agent then uses an locally-hosted Excel (\"XLSX\") spreadsheet to get both tenant records and property records. From this data, it can determine for each active tenant when payment is due, the amount and the tenancy duration. Comparing to the bank statement, the AI Agent can now report on where tenants have missed their payments, made late payments or are paying the incorrect amounts. The final report is generated and logged in the same XLSX for a human to check and action. Requirements A self-hosted version of n8n is required. OpenAI account for the AI model Customising this workflow If you organisation has a Slack or Teams account, consider sending reports to a channel for increased productivity. Email may be a good choice too. Want to go fully local? A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/1YRKjfakpInm23F_g8AHupKPBN-fphWgK/view?usp=sharing Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"bebbf9cf-8103-4694-a3be-ae3ee1e9ebaf\",\"name\":\"Watch For Bank Statements\",\"type\":\"n8n-nodes-base.localFileTrigger\",\"position\":[780,400],\"parameters\":{\"path\":\"/home/node/host_mount/reconciliation_project\",\"events\":[\"add\"],\"options\":{\"ignored\":\"!**/*.csv\"},\"triggerOn\":\"folder\"},\"typeVersion\":1},{\"id\":\"eca26bed-ba44-4507-97d4-9154e26908a5\",\"name\":\"Get Tenant Details\",\"type\":\"@n8n/n8n-nodes-langchain.toolCode\",\"position\":[1660,540],\"parameters\":{\"name\":\"get_tenant_details\",\"jsCode\":\"const xlsx = require('xlsx');\\n\\nconst { spreadsheet_location } = $('Set Variables').item.json;\\nconst sheetName = 'tenants';\\n\\nconst wb = xlsx.readFile(spreadsheet_location, { sheets: [sheetName] });\\nconst rows = xlsx.utils.sheet_to_json(wb.Sheets[sheetName], { raw: false });\\n\\nconst queryToList = [].concat(typeof query === 'string' ? query.split(',') : query);\\n\\nconst result = queryToList.map(q => (\\n rows.find(row =>\\n row['Tenant Name'].toLowerCase() === q.toLowerCase()\\n || row['Tenant ID'].toLowerCase() === q.toString().toLowerCase()\\n )\\n));\\n\\nreturn result ? JSON.stringify(result) : `No results were found for ${query}`;\",\"description\":\"Call this tool to get a tenant's details which includes their tenancy terms, rent amount and any notes attached to their account. Pass in one or an array of either the tenant ID or the name of the tenant.\"},\"typeVersion\":1.1},{\"id\":\"76b68"}
{"prompt": "Create an n8n workflow for: Organise Your Local File Directories With AI If you have a shared or personal drive location with a high frequency of files created by humans, it can become difficult to organise. This may not matter... until you need to search for something! This n8n workflow works with the local filesystem to target the messy folder and categorise as well as organise its files into sub directories automatically. Disclaimer Unfortunately due to the intended use-case, this workflow will not work on n8n Cloud and a self-hosted version of n8n is required. How it works Uses the local file trigger to activate once a new file is introduced to the directory The new file's filename and filetype are analysed using AI to determine the best location to move this file. The AI assess the current subdirectories as to not create duplicates. If a relevant subdirectory is not found, a new subdirectory is suggested. Finally, an Execute Command node uses the AI's suggestions to move the new file into the correct location. Requirements Self-hosted version of n8n. The nodes used in this workflow only work in the self-hosted version. If you are using docker, you must create a bind mount to a host directory. Mistral.ai account for LLM model Customise this workflow If the frequency of files created is high enough, you may not want the trigger to active on every new file created event. Switch to a timer to avoid concurrency issues. Want to go fully local? A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/1iqJ_zCGussXpfaUBYGrN5opziEFAEQMu/view?usp=sharing Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"c92e3d01-4385-4e99-a9a7-77279b3d9cb3\",\"name\":\"Local File Trigger\",\"type\":\"n8n-nodes-base.localFileTrigger\",\"position\":[720,120],\"parameters\":{\"path\":\"/home/node/host_mount/shared_drive\",\"events\":[\"add\"],\"options\":{\"awaitWriteFinish\":true},\"triggerOn\":\"folder\"},\"typeVersion\":1},{\"id\":\"a08f5acc-ee46-49e7-be4d-99edc95ab41f\",\"name\":\"Get Files and Folders\",\"type\":\"n8n-nodes-base.executeCommand\",\"position\":[1200,120],\"parameters\":{\"command\":\"=ls -p {{ $json.directory }} | grep -v / || true; \\\\\\necho \\\"===\\\"; \\\\\\nls -p {{ $json.directory }} | grep / || true;\"},\"typeVersion\":1},{\"id\":\"f3ab100a-986d-49bc-aeb5-979f16b2fd46\",\"name\":\"Files and Folders to Array\",\"type\":\"n8n-nodes-base.set\",\"position\":[1380,120],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"ad893795-cae8-4418-99e0-2c68126337d3\",\"name\":\"files\",\"type\":\"array\",\"value\":\"={{ $json.stdout.split('===')[0].split('\\\\n').filter(item => !item.endsWith('Zone.Identifier')).compact() }}\"},{\"id\":\"0e7e8571-6b86-481d-a20c-3a7c621c562f\",\"name\":\"folders\",\"type\":\"array\",\"value\":\"={{ $json.stdout.split('===')[1].split('\\\\n').compact() }}\"}]}},\"typeVersion\":3.3},{\"id\":\"56c4a8b4-c5b0-4e2f-806b-fef5fb5260b5\",\"name"}
{"prompt": "Create an n8n workflow for: Recipe Recommendations with Qdrant and Mistral This n8n workflow demonstrates creating a recipe recommendation chatbot using the Qdrant vector store recommendation API. Use this example to build recommendation features in your AI Agents for your users. How it works For our recipes, we'll use HelloFresh's weekly course and recipes for data. We'll scrape the website for this data. Each recipe is split, vectorised and inserted into a Qdrant Collection using Mistral Embeddings Additionally the whole recipe is stored in a SQLite database for later retrieval. Our AI Agent is setup to recommend recipes from our Qdrant vector store. However, instead of the default similarity search, we'll use the Recommendation API instead. Qdrant's Recommendation API allows you to provide a negative prompt; in our case, the user can specify recipes or ingredients to avoid. The AI Agent is now able to suggest a recipe recommendation better suited for the user and increase customer satisfaction. Requirements Qdrant vector store instance to save the recipes Mistral.ai account for embeddings and LLM agent Customising the workflow This workflow can work for a variety of different audiences. Try different sets of data such as clothes, sports shoes, vehicles or even holidays. Jimleuk | 7 months ago | Free +5", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"1eb82902-a1d6-4eff-82a2-26908a82cea2\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[720,320],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e0031fc3-27f1-45d9-910b-4c07dd322115\",\"name\":\"Get This Week's Menu\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[992,370],\"parameters\":{\"url\":\"=https://www.hellofresh.co.uk/menus/{{ $now.year }}-W{{ $now.weekNumber }}\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"2c556cc7-7d4e-4d80-902f-9686e756ed8c\",\"name\":\"Extract Available Courses\",\"type\":\"n8n-nodes-base.code\",\"position\":[992,650],\"parameters\":{\"jsCode\":\"const pageData = JSON.parse($input.first().json.data)\\nreturn pageData.props.pageProps.ssrPayload.courses.slice(0, 10);\"},\"typeVersion\":2},{\"id\":\"90c39db6-6116-4c37-8d48-a6d5e8f8c777\",\"name\":\"Extract Server Data\",\"type\":\"n8n-nodes-base.html\",\"position\":[992,510],\"parameters\":{\"options\":{\"trimValues\":false,\"cleanUpText\":true},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"data\",\"cssSelector\":\"script#__NEXT_DATA__\"}]}},\"typeVersion\":1.2},{\"id\":\"fbd4ed97-0154-4991-bf16-d9c4cb3f4776\",\"name\":\"Get Course Metadata\",\"type\":\"n8n-nodes-base.set\",\"position\":[1172,370],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\""}
{"prompt": "Create an n8n workflow for: Build Your Own Image Search Using AI Object Detection, CDN and ElasticSearch This n8n workflow demonstrates how to automate indexing of images to build a object-based image search. By utilising a Detr-Resnet-50 Object Classification model, we can identify objects within an image and store these associations in Elasticsearch along with a reference to the image. How it works An image is imported into the workflow via HTTP request node. The image is then sent to Cloudflare's Worker AI API where the service runs the image through the Detr-Resnet-50 object classification model. The API returns the object associations with their positions in the image, labels and confidence score of the classification. Confidence scores of less the 0.9 are discarded for brevity. The image's URL and its associations are then index in an ElasticSearch server ready for searching. Requirements A Cloudflare account with Workers AI enabled to access the object classification model. An ElasticSearch instance to store the image url and related associations. Extending this workflow Further enrich your indexed data with additional attributes or metrics relevant to your users. Use a vectorstore to provide similarity search over the images. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"6359f725-1ede-4b05-bc19-05a7e85c0865\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[680,292],\"parameters\":{},\"typeVersion\":1},{\"id\":\"9e1e61c7-f5fd-4e8a-99a6-ccc5a24f5528\",\"name\":\"Fetch Source Image\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1000,292],\"parameters\":{\"url\":\"={{ $json.source_image }}\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"9b1b94cf-3a7d-4c43-ab6c-8df9824b5667\",\"name\":\"Split Out Results Only\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[1428,323],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"result\"},\"typeVersion\":1},{\"id\":\"fcbaf6c3-2aee-4ea1-9c5e-2833dd7a9f50\",\"name\":\"Filter Score >= 0.9\",\"type\":\"n8n-nodes-base.filter\",\"position\":[1608,323],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"367d83ef-8ecf-41fe-858c-9bfd78b0ae9f\",\"operator\":{\"type\":\"number\",\"operation\":\"gte\"},\"leftValue\":\"={{ $json.score }}\",\"rightValue\":0.9}]}},\"typeVersion\":2},{\"id\":\"954ce7b0-ef82-4203-8706-17cfa5e5e3ff\",\"name\":\"Crop Object From Image\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[2080,432],\"parameters\":{\"width\":\"={{ $json.box.xmax - $json"}
{"prompt": "Create an n8n workflow for: Enrich Property Inventory Survey with Image Recognition and AI Agent This n8n workflow assists property managers and surveyors by reducing the time and effort it takes to complete property inventory surveys. In such surveys, articles and goods within a property may need to be captured and reported as a matter of record. This can take a sizable amount of time if the property or number of items is big enough. Our solution is to delegate this task to a capable AI Agent who can identify and fill out the details of each item automatically. How it works An AirTable Base is used to capture just the image of an item within the property Our workflow monitoring this AirTable Base sends the photo to an AI image recognition model to describe the item for purpose of identification. Our AI agent uses this description and the help of Google's reverse image search in an attempt to find an online product page for the item. If found, the product page is scraped for the item's specifications which are then used to fill out the rest of the details of the item in our Airtable. Requirements Airtable for capturing photos and product information OpenAI account to for image recognition service and AI for agent SerpAPI account for google reverse image search. Firecrawl.dev account for webspacing. Customising this workflow Try building an internal inventory database to query and integrate into the workflow. This could save on costs by avoiding fetching new each time for common items. Jimleuk | 7 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"192d3e4f-6bb0-4b87-a1fa-e32c9efb49cc\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[336,34],\"parameters\":{},\"typeVersion\":1},{\"id\":\"32a7a772-76a6-4614-a6ab-d2b152a5811f\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1220,180],\"parameters\":{\"model\":\"gpt-4o\",\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"8c444314-ed7d-4ca0-b0fa-b6d1e964c698\",\"name\":\"Get Applicable Rows\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[516,34],\"parameters\":{\"base\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"appbgxPBurOmQK3E7\",\"cachedResultUrl\":\"https://airtable.com/appbgxPBurOmQK3E7\",\"cachedResultName\":\"Building Inventory Survey Example\"},\"table\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"tblEHkoTvKpa4Aa0Q\"},\"options\":{},\"operation\":\"search\",\"returnAll\":false,\"filterByFormula\":\"AND(Image!=\\\"\\\", AI_status=FALSE())\"},\"credentials\":{\"airtableTokenApi\":{\"id\":\"Und0frCQ6SNVX3VV\",\"name\":\"Airtable Personal Access Token account\"}},\"typeVersion\":2},{\"id\":\"f90578fa-b886-4653-8ff7-0c91884dc517\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger"}
{"prompt": "Create an n8n workflow for: Customer Support Channel and Ticketing System with Slack and Linear This n8n workflow demonstrates how to create a really simple yet effective customer support channel and pipeline by combining Slack, Linear and AI tools. Built on n8n's ability to integrate anything, this workflow is intended for small support teams who want to maximise re-use of the tools they already have with an interface which is doesn't require any onboarding. Read the blog post here: https://blog.n8n.io/automated-customer-support-tickets-with-n8n-slack-linear-and-ai/ How it works The workflow is connected to a slack channel setup with the customer to capture support issues. Only messages which are tagged with a \"\u2705\" reaction are captured by the workflow. Messages are tagged by the support team in the channel. Each captured support issue is sent to the AI model to classify, prioritise and rewrite into a support ticket. The generated support ticket is uploaded to Linear for the support team to investigate and track. Support team is able to report back to the user via the channel when issue is fixed. Requirements Slack channel to be monitored Linear account and project Customising this workflow Don't have Linear? This workflow can work just as well with traditional ticketing systems like JIRA. Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"2b3112a9-046e-4aae-8fcc-95bddf3bb02e\",\"name\":\"Slack\",\"type\":\"n8n-nodes-base.slack\",\"position\":[828,327],\"parameters\":{\"limit\":10,\"query\":\"in:#n8n-tickets has::ticket:\",\"options\":{},\"operation\":\"search\"},\"credentials\":{\"slackApi\":{\"id\":\"VfK3js0YdqBdQLGP\",\"name\":\"Slack account\"}},\"typeVersion\":2.2},{\"id\":\"65fd6821-4d19-436c-81d9-9bdb0f5efddd\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1920,480],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"85125704-7363-40de-af84-f267f8c7e919\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[2100,480],\"parameters\":{\"jsonSchema\":\"{\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"title\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"summary\\\": { \\\"type\\\": \\\"string\\\" },\\n \\\"ideas\\\": {\\n \\\"type\\\": \\\"array\\\",\\n \\\"items\\\": { \\\"type\\\": \\\"string\\\" }\\n },\\n \\\"priority\\\": { \\\"type\\\": \\\"string\\\" }\\n }\\n}\"},\"typeVersion\":1.1},{\"id\":\"eda8851a-1929-4f2f-9149-627c0fe62fbc\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[628,327],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"minutes"}
{"prompt": "Create an n8n workflow for: Speed Up Social Media Banners With BannerBear.com This n8n workflow shows an easy way to automate the creation of social media assets using AI and a service like BannerBear. Designed for the busy marketer, leveraging AI image generation capabilities can help cut down production times and allow reinvesting into higher quality content. How it works This workflow generates social media banners for online events. Using a form trigger, a user can define the banner text and suggest an image to be generated. This request is passed to OpenAI's Dalle-3 image generation service to produce a relevant graphic for the event banner. This generated image is uploaded and sent to BannerBear where a template will use it and the rest of the form data to produce the banner. BannerBear returns the final banner which can now be used in an assortment of posts and publications. Requirements A BannerBear.com account and template is required An OpenAI account to use the Dalle-3 service. Customising the workflow We've only shown a small section of what BannerBear has to offer. With experimentation and other asset generating services such as AI audio and video, you should be able to generate more than just static banners! Jimleuk | 7 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"81ea4c6a-d603-4688-8b72-d9c79faf7adf\",\"name\":\"n8n Form Trigger\",\"type\":\"n8n-nodes-base.formTrigger\",\"position\":[1272,455],\"webhookId\":\"d280e773-3bd8-44ce-a147-8b404251fce9\",\"parameters\":{\"path\":\"d280e773-3bd8-44ce-a147-8b404251fce9\",\"options\":{},\"formTitle\":\"BannerBear Clone\",\"formFields\":{\"values\":[{\"fieldType\":\"dropdown\",\"fieldLabel\":\"Template\",\"fieldOptions\":{\"values\":[{\"option\":\"n8n Meetup Template\"},{\"option\":\"AI Meetup Template\"}]}},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Title of Event\",\"requiredField\":true},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Location of Event\",\"requiredField\":true},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Date of Event\",\"requiredField\":true},{\"fieldType\":\"textarea\",\"fieldLabel\":\"Image Prompt\",\"requiredField\":true}]},\"formDescription\":\"Generate an image and apply text\"},\"typeVersion\":2},{\"id\":\"dea26687-4060-488b-a09f-e21900fec2fc\",\"name\":\"Upload to Cloudinary\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1920,480],\"parameters\":{\"url\":\"https://api.cloudinary.com/v1_1/daglih2g8/image/upload\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"sendQuery\":true,\"contentType\":\"multipart-form-data\",\"authentication\":\"genericCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"file\",\"parameterType\":\"formBinaryData\",\"inputDataFieldName\":\"data\"}]},\"genericAuthType\":\"httpQueryAuth\",\"queryParameters\":{\"parameters\":[{\"name\":\"upload_preset\",\"value\":\"n8n-workflows-preset\"}]}},\"credentials\":{\"httpQueryAuth\":{\"id\":\"sT9jeKzZiLJ3bVPz\",\""}
{"prompt": "Create an n8n workflow for: Automate Your RFP Process with OpenAI Assistants This n8n workflow demonstrates how to automate oftern time-consuming form filling tasks in the early stages of the tendering process; the Request for Proposal document or \"RFP\". It does this by utilising a company's knowledgebase to generating question-and-answer pairs using Large Language Models. How it works A buyer's RFP is submitted to the workflow as a digital document that can be parsed. Our first AI agent scans and extracts all questions from the document into list form. The supplier sets up an OpenAI assistant prior loaded with company brand, marketing and technical documents. The workflow loops through each of the buyer's questions and poses these to the OpenAI assistant. The assistant's answers are captured until all questions are satisified and are then exported into a new document for review. A sales team member is then able to use this document to respond quickly to the RFP before their competitors. Example Webhook Request curl --location 'https://&lt;n8n_webhook_url&gt;' \\ --form 'id=\"RFP001\"' \\ --form 'title=\"BlueChip Travel and StarBus Web Services\"' \\ --form 'reply_to=\"jim@example.com\"' \\ --form 'data=@\"k9pnbALxX/RFP Questionnaire.pdf\"' Requirements An OpenAI account to use AI services. Customising the workflow OpenAI assistants is only one approach to hosting a company knowledgebase for AI to use. Exploring different solutions such as building your own RAG-powered database can sometimes yield better results in terms of control of how the data is managed and cost. Jimleuk | 7 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"26ba763460b97c249b82942b23b6384876dfeb9327513332e743c5f6219c2b8e\"},\"nodes\":[{\"id\":\"51dbe3b4-42f6-43c9-85dc-42ae49be6ba9\",\"name\":\"Get RFP Data\",\"type\":\"n8n-nodes-base.extractFromFile\",\"position\":[1003,278],\"parameters\":{\"options\":{},\"operation\":\"pdf\"},\"typeVersion\":1},{\"id\":\"c42e6bfc-a426-4d12-bf95-f3fe6e944631\",\"name\":\"Item List Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserItemList\",\"position\":[2140,540],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"1703e9c3-f49e-4272-ad11-0b9d4e9a76c6\",\"name\":\"For Each Question...\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[2460,340],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"a54fa4ee-6f67-41a9-89fe-fd9f2bf094de\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[760,60],\"parameters\":{\"color\":7,\"width\":532.597092515486,\"height\":508.1316876142587,\"content\":\"## 1. API to Trigger Workflow\\n[Read more about using Webhooks](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.webhook/)\\n\\nThis workflow requires the user to submit the RFP document via an API request. It's a common pattern to use the webhook node for this purpose. Be sure to secure this webhook endpoint in production!\"},\"typeVersion\":1},{\"id\":\"fdef005f-7838-4b8c-8af4-4b7c6f947ee2\",\"name\":\"Set Variables\",\"type\":\"n8n-nodes-base."}
{"prompt": "Create an n8n workflow for: Store Notion's Pages as Vector Documents into Supabase with OpenAI Workflow updated on 17/06/2024:** Added 'Summarize' node to avoid creating a row for each Notion content block in the Supabase table.* Store Notion's Pages as Vector Documents into Supabase This workflow assumes you have a Supabase project with a table that has a vector column. If you don't have it, follow the instructions here: Supabase Langchain Guide Workflow Description This workflow automates the process of storing Notion pages as vector documents in a Supabase database with a vector column. The steps are as follows: Notion Page Added Trigger: Monitors a specified Notion database for newly added pages. You can create a specific Notion database where you copy the pages you want to store in Supabase. Node: Page Added in Notion Database Retrieve Page Content: Fetches all block content from the newly added Notion page. Node: Get Blocks Content Filter Non-Text Content: Excludes blocks of type \"image\" and \"video\" to focus on textual content. Node: Filter - Exclude Media Content Summarize Content: Concatenates the Notion blocks content to create a single text for embedding. Node: Summarize - Concatenate Notion's blocks content Store in Supabase: Stores the processed documents and their embeddings into a Supabase table with a vector column. Node: Store Documents in Supabase Generate Embeddings: Utilizes OpenAI's API to generate embeddings for the textual content. Node: Generate Text Embeddings Create Metadata and Load Content: Loads the block content and creates associated metadata, such as page ID and block ID. Node: Load Block Content & Create Metadata Split Content into Chunks: Divides the text into smaller chunks for easier processing and embedding generation. Node: Token Splitter Dataki | 8 months ago | Free", "completion": "{\"id\":\"DvP6IHWymTIVg8Up\",\"meta\":{\"instanceId\":\"b9faf72fe0d7c3be94b3ebff0778790b50b135c336412d28fd4fca2cbbf8d1f5\",\"templateCredsSetupCompleted\":true},\"name\":\"Store Notion's Pages as Vector Documents into Supabase with OpenAI\",\"tags\":[],\"nodes\":[{\"id\":\"495609cd-4ca0-426d-8413-69e771398188\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[480,400],\"parameters\":{\"width\":637.1327972412109,\"height\":1113.7434387207031,\"content\":\"## Store Notion's Pages as Vector Documents into Supabase\\n\\n**This workflow assumes you have a Supabase project with a table that has a vector column. If you don't have it, follow the instructions here:** [Supabase Vector Columns Guide](https://supabase.com/docs/guides/ai/vector-columns)\\n\\n## Workflow Description\\n\\nThis workflow automates the process of storing Notion pages as vector documents in a Supabase database with a vector column. The steps are as follows:\\n\\n1. **Notion Page Added Trigger**:\\n - Monitors a specified Notion database for newly added pages. You can create a specific Notion database where you copy the pages you want to store in Supabase.\\n - Node: `Page Added in Notion Database`\\n\\n2. **Retrieve Page Content**:\\n - Fetches all block content from the newly added Notion page.\\n - Node: `Get Blocks Content`\\n\\n3. **Filter Non-Text Content**:\\n - Excludes blocks of type \\\"image\\\" and \\\"video\\\" to focus on textual content.\\n - Node: `Filter - Exclude Media Content`\\n\\n4. **Summarize Content**:\\n - Concatenates the Notion blocks content to create a single text for embedding.\\n - Node: `Summarize - Concatenate Notion's blocks content`\\n\\n5. **Store in Supabase**:\\n - Stores the processed documents and their"}
{"prompt": "Create an n8n workflow for: Share YouTube Videos with AI Summaries on Discord Boost engagement on your Discord server by automatically sharing new YouTube videos along with AI generated summaries of their content. This workflow is ideal for content creators and community managers looking to provide value and spark interest through summarized content, making it easier for community members to decide if a video is of interest to them. Watch this video tutorial to learn more about the template. How it works RSS Feed Trigger**: Monitors your YouTube channel for new uploads using the RSS feed. Video Captions Retrieval**: Fetches video captions using the YouTube API to get detailed content data. AI Summary Generation**: Uses an AI model to generate concise summaries from the video captions, highlighting key points. Discord Notification**: Posts video announcements along with their AI generated summaries to a specified Discord channel using a webhook. Set up steps Configure YouTube RSS Feed: Set up the RSS feed node to detect new video uploads. Add your YouTube channel ID to the URL in the first node: https://www.youtube.com/feeds/videos.xml?channel_id=YOUR_CHANNEL_ID. Connect OpenAI Account: To enable AI summary generation, connect your OpenAI account in n8n. Set Up Discord Webhook: Create a webhook in your Discord server and configure it in the Discord node. Design the Message: Format the Discord message as you like to include the video title, link, and the AI generated summary. Example This template empowers you to maintain a highly engaging Discord community, ensuring members receive not only regular updates but also valuable insights into each video's content without needing to watch immediately. Mike Russell | 10 months ago | Free", "completion": "{\"id\":\"LF8gz3iz74u45a5i\",\"meta\":{\"instanceId\":\"889f0d7d968f3b02a88433e2529a399907d2ca89e329934b608193beaa2301f8\"},\"name\":\"YouTube Videos with AI Summaries on Discord\",\"tags\":[],\"nodes\":[{\"id\":\"48c87027-7eea-40b9-a73c-4e002b748783\",\"name\":\"YouTube Video Trigger\",\"type\":\"n8n-nodes-base.rssFeedReadTrigger\",\"position\":[560,220],\"parameters\":{\"feedUrl\":\"https://www.youtube.com/feeds/videos.xml?channel_id=UC08Fah8EIryeOZRkjBRohcQ\",\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"typeVersion\":1},{\"id\":\"56166228-b365-4043-b48c-098b4de71f6f\",\"name\":\"Retrieve Caption Data\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[780,220],\"parameters\":{\"url\":\"https://www.googleapis.com/youtube/v3/captions\",\"options\":{},\"sendQuery\":true,\"authentication\":\"predefinedCredentialType\",\"queryParameters\":{\"parameters\":[{\"name\":\"videoId\",\"value\":\"={{ $json.id.match(/(?:[^:]*:){2}\\\\s*(.*)/)[1] }}\"},{\"name\":\"part\",\"value\":\"snippet\"}]},\"nodeCredentialType\":\"youTubeOAuth2Api\"},\"credentials\":{\"youTubeOAuth2Api\":{\"id\":\"uy3xy1Ks2ATwRGr4\",\"name\":\"Creator Magic - YouTube account\"}},\"typeVersion\":4.2},{\"id\":\"c029ac6f-3071-4045-83f6-2dede0c1f358\",\"name\":\"Download Captions\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1220,220],\"parameters\":{\"url\":\"=https://www.googleapis.com/youtube/v3/captions/{{ $json.caption.id }}\",\"options\":{},\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"youTubeOAuth"}
{"prompt": "Create an n8n workflow for: Post New YouTube Videos to X Automated YouTube Video Promotion Workflow Automate the promotion of new YouTube videos on X (formerly Twitter) with minimal effort. This workflow is perfect for content creators, marketers, and social media managers who want to keep their audience updated with fresh content consistently. How it works This workflow triggers every 30 minutes to check for new YouTube videos from a specified channel. If a new video is found, it utilizes OpenAI's ChatGPT to craft an engaging, promotional message for X. Finally, the workflow posts the generated message to Twitter, ensuring your latest content is shared with your audience promptly. Set up steps Schedule the workflow to run at your desired frequency. Connect to your YouTube account and set up the node to fetch new videos based on your Channel ID. Integrate with OpenAI to generate promotional messages using GPT-3.5 turbo. Link to your X account and set up the node to post the generated content. Please note, you'll need API keys and credentials for YouTube, OpenAI, and X. Check out this quick video tutorial to make the setup process a breeze. Additional Tips Customize the workflow to match your branding and messaging tone. Test each step to ensure your workflow runs smoothly before going live. Mike Russell | 10 months ago | Free", "completion": "{\"id\":\"O9FXr8iXzhSgYKaL\",\"meta\":{\"instanceId\":\"d8bbc8c5a59875a8be9f3c7142d858bc46c4b8e36a11781a25e945fcf9a5767a\"},\"name\":\"Post New YouTube Videos to X\",\"tags\":[],\"nodes\":[{\"id\":\"576be5c4-1ed0-4d01-a980-cb2fc31e2223\",\"name\":\"Post to X\",\"type\":\"n8n-nodes-base.twitter\",\"position\":[1280,380],\"parameters\":{\"text\":\"={{ $json.message.content }}\",\"additionalFields\":{}},\"credentials\":{\"twitterOAuth2Api\":{\"id\":\"FjHOuF0APzoMqIjG\",\"name\":\"X account\"}},\"typeVersion\":2},{\"id\":\"3b87cf2a-51d5-4589-9729-bb1fe3cfceca\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[620,254.76543209876536],\"parameters\":{\"color\":3,\"width\":221.82716049382665,\"height\":308.7901234567902,\"content\":\"\ud83c\udd94 Ensure you enter your YouTube Channel ID in the \\\"Channel ID\\\" field of this node. You can find your [Channel ID here](https://youtube.com/account_advanced).\"},\"typeVersion\":1},{\"id\":\"912e631c-aa43-4e02-9816-b35fe6e62dd8\",\"name\":\"Generate Post for X with ChatGPT\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[900,380],\"parameters\":{\"modelId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"gpt-3.5-turbo\",\"cachedResultName\":\"GPT-3.5-TURBO\"},\"options\":{},\"messages\":{\"values\":[{\"content\":\"=Write an engaging post about my latest YouTube video for X (Twitter) of no more than 140 characters in length. Link to the video at https://youtu.be/{{ $json.id.videoId }} use this title and description: {{ $json."}
{"prompt": "Create an n8n workflow for: Automated AI image analysis and response via Telegram Example: @SubAlertMe_Bot Summary: The automated image analysis and response workflow using n8n is a sophisticated solution designed to streamline the process of analyzing images sent via Telegram and delivering insightful responses based on the analysis outcomes. This cutting-edge workflow employs a series of meticulously orchestrated nodes to ensure seamless automation and efficiency in image processing tasks. Use Cases: This advanced workflow caters to a myriad of scenarios where real-time image analysis and response mechanisms are paramount. The use cases include: Providing immediate feedback on images shared within Telegram groups. Enabling automated content moderation based on the analysis of image content. Facilitating rapid categorization and tagging of images based on the results of the analysis. Detailed Workflow Setup: To effectively implement this workflow, users must adhere to a meticulous setup process, which includes: Access to the versatile n8n platform, ensuring seamless workflow orchestration. Integration of a Telegram account to facilitate image reception and communication. Utilization of an OpenAI account for sophisticated image analysis capabilities. Configuration of Telegram and OpenAI credentials within the n8n environment for seamless integration. Proficiency in creating and interconnecting nodes within the n8n workflow for optimal functionality. Detailed Node Description: Get the Image (Telegram Trigger): Actively triggers upon receipt of an image via Telegram, ensuring prompt processing. Extracts essential information from the received image message to initiate further actions. Merge all fields To get data from trigger: Seamlessly amalgamates all relevant data fields extracted from the trigger node for comprehensive data consolidation. Analyze Image (OpenAI): Harnesses the powerful capabilities of OpenAI services to conduct in-depth analysis of the received image. Processes the image data in base64 format to derive meaningful insights from the visual content. Aggregate all fields: Compiles and consolidates all data items for subsequent processing and analysis, ensuring comprehensive data aggregation. Send Content for the Analyzed Image (Telegram): Transmits the analyzed content back to the Telegram chat interface for seamless communication. Delivers the analyzed information in textual format, enhancing user understanding and interaction. Switch Node: The Switch node is pivotal for decision-making based on predefined conditions within the workflow. It evaluates incoming data to determine the existence or absence of specific elements, such as images in this context. Utilizes a set of rules to assess the presence of image data in the message payload and distinguishes between cases where images are detected and when they are not. This crucial node plays a pivotal role in directing the flow of the workflow based on the outcomes of its evaluations. Conclusion: The automation of image analysis processes through this sophisticated workflow not only enhances operational efficiency but also revolutionizes communication dynamics within Telegram interactions. By incorporating this advanced workflow solution, users can optimize their image analysis workflows, bolster communication efficacy, and unlock new levels of automation in image processing tasks. AlQaisi | 10 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\"},\"nodes\":[{\"id\":\"ecb4bbc8-939a-4c6c-80b6-6f053d1d7745\",\"name\":\"Get the Image\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[1640,880],\"webhookId\":\"8404b32c-14bd-428e-88a6-560755f0f7ba\",\"parameters\":{\"updates\":[\"message\"],\"additionalFields\":{\"download\":true}},\"credentials\":{\"telegramApi\":{\"id\":\"k3RE6o9brmFRFE9p\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"2fd523b7-5f89-4e53-9445-4336b51cad51\",\"name\":\"Send Content for the Analyzed image\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[2380,760],\"parameters\":{\"text\":\"={{ $json.content }}\",\"chatId\":\"={{ $('Get the Image').item.json.message.chat.id }}\",\"additionalFields\":{\"appendAttribution\":false}},\"credentials\":{\"telegramApi\":{\"id\":\"k3RE6o9brmFRFE9p\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"b77fe84f-7651-42aa-aa40-f903b10c8fb1\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,360],\"parameters\":{\"width\":1235.4238259410247,\"height\":1361.9843517631348,\"content\":\"# Automated Image Analysis and Response via Telegram\\n\\n## Example: @SubAlertMe_Bot\\n\\n## Summary:\\nThe automated image analysis and response workflow using n8n is a sophisticated solution designed to streamline the process of analyzing images sent via Telegram and delivering insightful responses based on the analysis outcomes. This cutting-edge workflow employs a series of meticulously orchestrated nodes to ensure seamless automation and efficiency in image processing tasks.\\"}
{"prompt": "Create an n8n workflow for: AI-Powered Children's English Storytelling on Telegram with OpenAI Unleashing Creativity: Transforming Children's English Storytelling with Automation and AI Check this example: https://t.me/st0ries95 Summary In the realm of children's storytelling, automation is revolutionizing the way captivating tales are created and shared. This article highlights the transformative power of setting up a workflow for AI-powered children's English storytelling on Telegram. By delving into the use cases and steps involved, we uncover how this innovative approach is inspiring young minds and fostering a love for storytelling in children. Usecase The workflow for children's stories is a game-changer for content creators, educators, and parents seeking to engage children through imaginative and educational storytelling. Here's how this workflow is making a difference: Streamlined Content Creation: By providing a structured framework and automation for story generation, audio creation, and image production, the workflow simplifies the process of crafting captivating children's stories. Enhanced Educational Resources: Teachers can leverage this workflow to develop interactive educational materials that incorporate storytelling, making learning more engaging for students. Personalized Parental Engagement: Parents can share personalized stories with their children, nurturing a passion for reading and creativity while strengthening family bonds through shared storytelling experiences. Community Connection: Organizations and community groups can use the workflow to connect with their audience and promote literacy and creativity by creating and sharing children's stories. Inspiring Imagination: Through automated creation and sharing of enchanting stories, the workflow aims to spark imagination, inspire young minds, and instill a love for storytelling in children. Node Explanation OpenAI Chat Model: Utilizes the OpenAI Chat Model to generate text for the children's stories. Schedule Trigger: Triggers the workflow at set intervals (every 12 hours) to generate new stories. Recursive Character Text Splitter: Splits text into smaller chunks for processing. OpenAI Chat Model2: Another OpenAI Chat Model node for generating prompts for image creation. Send Story Text: Sends the generated story text to a specified Telegram chat. Send Audio for the Story: Sends audio files of the stories to the Telegram chat. Send Story Picture: Shares images related to the stories on Telegram. Create a Kids Stories: Generates captivating short tales for kids using prompts provided. Generate Audio for the Story: Converts generated text into audio files for storytelling. Create a Prompt for DALL-E: Creates prompts for generating images related to the stories. Generate a Picture for the Story: Generates pictures based on the prompts for visual storytelling. By embracing automation in children's storytelling, we unleash creativity, inspire young minds, and create magical experiences that resonate with both storytellers and listeners alike. AlQaisi | 10 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\",\"templateId\":\"2233\"},\"nodes\":[{\"id\":\"757a7e67-073a-4fa1-b571-2ddd147b35f6\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1000,1240],\"parameters\":{\"model\":\"gpt-3.5-turbo-16k-0613\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"kDo5LhPmHS2WQE0b\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"761ed83a-2cfb-474a-b596-922e5a7e2717\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[660,1060],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"hours\",\"hoursInterval\":12}]}},\"typeVersion\":1.1},{\"id\":\"41faf334-30d6-4cc0-9a94-9c486ec3fa6c\",\"name\":\"OpenAI Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1520,1420],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"kDo5LhPmHS2WQE0b\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"d9ad0a3a-2ce6-4071-8262-8176b3eecf36\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1780,220],\"parameters\":{\"width\":1004.4263690337257,\"height\":811.7188223885136,\"content\":\"### Setting Up a Workflow for \\\"AI-Powered Children's English Storytelling on Telegram\\\"\\n\\nIn this guide, we will walk you through the process of setting up a"}
{"prompt": "Create an n8n workflow for: Image Creation with OpenAI and Telegram Image Creation with OpenAI and Telegram Check this channel: AutoTechAi_bot Description: In the realm of automation and artificial intelligence, n8n offers a sophisticated platform for seamlessly integrating AI algorithms to enhance image creation and communication processes. This innovative workflow leverages the capabilities of OpenAI and Telegram to facilitate creative image generation and streamline communication channels, ultimately enhancing user engagement and interaction. How to Use: Set Up Credentials: Configure credentials for the Telegram account and OpenAI API to enable seamless integration. Configure Nodes: Telegram Trigger Node: Set up the node to initiate the workflow based on incoming messages from users on Telegram. OpenAI Node: Utilize advanced AI algorithms to analyze text content from messages and generate intelligent responses. Telegram Node: Send processed data, including images and responses, back to users on Telegram for seamless communication. Merge Node: Organize and combine processed data for efficient handling and integration within the workflow. Aggregate Node: Aggregate all item data, including binaries if specified, for comprehensive reporting and analysis purposes. Run Workflow: Initiate the workflow to leverage AI-enhanced image processing and communication capabilities for enhanced user interactions. Monitor Execution: Keep an eye on the workflow execution for any errors or issues that may occur during processing. Customize Workflow: Tailor the workflow nodes, parameters, or AI models to align with specific business objectives and user engagement strategies. Experience Benefits: Embrace the power of AI-driven image processing and interactive communication on Telegram to elevate user engagement and satisfaction levels. By following these steps, businesses can unlock the transformative potential of AI integration in image creation and communication workflows using n8n. Elevate your user engagement strategies and deliver exceptional experiences to your audience through innovative AI-driven solutions. Embark on a journey of innovation and efficiency with AI integration in image creation and communication workflows using n8n! AlQaisi | 11 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"f691e434c527bcfc50a22f01094756f14427f055aa0b6917a75441617ecd7fb2\"},\"nodes\":[{\"id\":\"a998289c-65da-49ea-ba8a-4b277d9e16f3\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[1060,640],\"webhookId\":\"2901cde3-b35a-4b0b-a1ba-17a7d9f80125\",\"parameters\":{\"updates\":[\"message\",\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"pbbCqv0hRu9TDmWm\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"7f50072a-5312-4a47-823e-0513cd9d383a\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[1380,640],\"parameters\":{\"prompt\":\"={{ $json.message.text }}\",\"options\":{},\"resource\":\"image\"},\"credentials\":{\"openAiApi\":{\"id\":\"p4Qrsjiuev2epBzW\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.3},{\"id\":\"a59264d6-c199-4d7b-ade4-1e31f10eb632\",\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1580,1000],\"parameters\":{\"chatId\":\"={{ $json.data[1].message.from.id }}\",\"operation\":\"sendPhoto\",\"binaryData\":true,\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"pbbCqv0hRu9TDmWm\",\"name\":\"Telegram account\"}},\"typeVersion\":1.1},{\"id\":\"e0719c38-75ae-4082-91ba-d68c7cd28339\",\"name\":\"Merge\",\"type\":\"n8n-nodes-base.merge\",\"position\":[1060,1000],\"parameters\":{},\"typeVersion\":2.1},{\"id\":\"bee"}
{"prompt": "Create an n8n workflow for: Translate Telegram audio messages with AI (55 supported languages) Use case This workflow enables a Telegram bot that can: Accept speech input in one of 55 supported languages Automatically detect the language spoken and translate the speech to another language Responds back with the translated speech output. This allows users to communicate across language barriers by simply speaking to the bot, which will handle the translation seamlessly. How does it work? Translation In the translation step the workflow converts the user's speech input to text and detects the language of the input text. If it's English, it will translate to French. If it's French, it will translate to English. To change the default translation languages, you can update the prompt in the AI node. Output In the output step, we provide the translated text output back to the user and speech output is generated in the translated language. Setup steps Obtain Telegram API Token Start a chat with the BotFather. Enter /newbot and reply with your new bot's display name and username. Copy the bot token and use it in the Telegram node credentials in n8n. Update the Settings node to customize the desired languages Activate the flow Full list of supported languages All supported languages: Derek Cheung | 11 months ago | Free", "completion": "{\"id\":\"IvgAFAUOSI3biT4L\",\"meta\":{\"instanceId\":\"2723a3a635131edfcb16103f3d4dbaadf3658e386b4762989cbf49528dccbdbd\"},\"name\":\"Translate Telegram audio messages with AI (55 supported languages) v1\",\"tags\":[],\"nodes\":[{\"id\":\"f91fa0cf-ea01-4fc0-9ef2-754da399b7fb\",\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[440,220],\"webhookId\":\"c537cfcc-6c4a-436a-8871-d32f8ce016cb\",\"parameters\":{\"updates\":[\"*\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"Ov00cT0t4h4AFtZ0\",\"name\":\"Telegram account\"}},\"typeVersion\":1},{\"id\":\"057ae05f-2c7d-48c5-a057-a6917a88971c\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1240,0],\"parameters\":{\"width\":556.5162909529794,\"height\":586.6978417266175,\"content\":\"## Translation\\n\\n- Converts from speech to text.\\n\\n- Translates the language from the native language to translated language (as specified in settings node)\\n\\n\"},\"typeVersion\":1},{\"id\":\"c6947668-118e-4e23-bc55-1cdbce554a20\",\"name\":\"Text reply\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[2240,220],\"parameters\":{\"text\":\"={{ $json.text }}\",\"chatId\":\"={{ $('Telegram Trigger').item.json.message.chat.id }}\",\"additionalFields\":{\"parse_mode\":\"Markdown\"}},\"credentials\":{\"telegramApi\":{\"id\":\"Ov00cT0t4h4AFtZ0\",\"name\":\"Telegram account\"}},\"typeVersion\":1},{\"id\":\"93551aea-0213-420d-bf82-7669ab291dae\",\"name\":\"Telegram1\",\"type"}
{"prompt": "Create an n8n workflow for: OpenAI Assistant workflow: upload file, create an Assistant, chat with it! This is an end-to-end workflow for creating a simple OpenAI Assistant. The whole process is done with n8n nodes and do not require any programming experience. The workflow is divided into three main steps: Step 1: Get a Google Drive File and Upload to OpenAI The workflow starts by retrieving a file from Google Drive using the \"Get File\" node. The example file used is a Music Festival document. The retrieved file is then uploaded to OpenAI using the \"Upload File to OpenAI\" node. Run this section only once. The file is stored persistently on the OpenAI side. Step 2: Set Up a New Assistant In this step, a new assistant is created using the \"Create new Assistant\" node. The assistant is given a name, description, and system prompt. The uploaded file from Step 1 is attached as a knowledge source for the assistant. Same as for Step 1, run this section only once. Step 3: Chat with the Assistant The \"Chat Trigger\" node initiates the conversation with the assistant. The \"OpenAI Assistant\" node handles the conversation, using the assistant created in Step 2. Step 4: Expand the Assistant This step provides resources for ideas on how to expand the Assistant's capabilities: Create a WhatsApp bot Create a simple Telegram bot Create a Telegram AI bot (YouTube video) By following this workflow, users can create their own AI-powered assistants using OpenAI's API and integrate them with various platforms like WhatsApp and Telegram. Yulia | 11 months ago | Free", "completion": "{\"id\":\"InzSAe2cnTJImvLm\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"OpenAI Assistant workflow: uploa file, create an Assistant, chat with it!\",\"tags\":[],\"nodes\":[{\"id\":\"fc64b8c8-3457-4a96-8321-094accb71c56\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"disabled\":true,\"position\":[980,280],\"parameters\":{},\"typeVersion\":1},{\"id\":\"356299ae-155b-40cf-a3a4-2ae38819f998\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1140,0],\"parameters\":{\"color\":7,\"width\":513,\"height\":350.4434384638342,\"content\":\"## STEP 1. Get a Google Drive file and upload to OpenAI \\n\\n[Music Festival example document](https://docs.google.com/document/d/1_miLvjUQJ-E9bWgEBK87nHZre26-4Fz0RpfSfO548H0/edit?usp=sharing\\n)\\n\\n[OpenAI API doc for the file upload](https://platform.openai.com/docs/api-reference/files)\\n\"},\"typeVersion\":1},{\"id\":\"48b39a32-e0b0-4c04-b99f-07ed040d743d\",\"name\":\"Get File\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[1200,180],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"1_miLvjUQJ-E9bWgEBK87nHZre26-4Fz0RpfSfO548H0\",\"cachedResultUrl\":\"https://docs.google.com/document/d/1_miLvjUQJ-E9bWgEBK87nHZre26-4Fz0RpfSfO"}
{"prompt": "Create an n8n workflow for: Compose reply draft in Gmail with OpenAI Assistant This workflow uses OpenAI Assistant to compose draft replies for labeled email messages. It automatically connects the drafts to Gmail threads. \ud83d\udca1 You can add knowledge base to your OpenAI Assistant and make your reply drafts very customized (e.g. compose response with product information in response to inquiry from customer). \ud83c\udfac See this workflow in action in my YouTube video about automating Gmail. How it works? The workflow is triggered at regular intervals (default: every 1 minute \u2013 you can change this value) to check for messages with a specific label (e.g., \"AI\"). The content of the retrieved email message is then forwarded to the OpenAI Assistant node, and a reply draft is generated. Next, the response from the Assistant is converted to HTML, and a raw message in RFC standard is composed. \ud83d\udca1 You can learn more about composing drafts with the Gmail API in the official Google documentation. The raw email message (reply draft) is encoded and attached to the original thread ID. Finally, the trigger label (in this case: \"AI\") is removed to prevent the workflow from looping. Set up steps Set credentials for Gmail and OpenAI. Add new label in Gmail account for messages that should be handled by the workflow (e.g. name it \"AI\"). Select this label in the first and last Gmail nodes in workflow. Create and configure your OpenAI Assistant. Select your assistant in \"OpenAI Assistant\" node. Optionally: change trigger interval (by default interval is 1 minute). If you like this workflow, please subscribe to my YouTube channel and/or my newsletter. Oskar | 11 months ago | Free", "completion": "{\"nodes\":[{\"id\":\"a99b3164-fe36-4dde-9525-110c1ae08afb\",\"name\":\"Convert raw to base64\",\"type\":\"n8n-nodes-base.code\",\"position\":[3320,580],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"jsCode\":\"const encoded = Buffer.from($json.raw).toString('base64');\\n\\nreturn { encoded };\"},\"typeVersion\":2},{\"id\":\"f0f731bd-7b2f-4c39-bc06-42fd57bc4ae8\",\"name\":\"Add email draft to thread\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[3580,580],\"parameters\":{\"url\":\"https://www.googleapis.com/gmail/v1/users/me/drafts\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\\"message\\\":{\\\"raw\\\":\\\"{{ $json.encoded }}\\\", \\\"threadId\\\": \\\"{{ $('Map fields for further processing').item.json[\\\"threadId\\\"] }}\\\"}}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"gmailOAuth2\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"uBcIMfsTtKjexw7I\",\"name\":\"Gmail (workfloowstutorial@gmail.com)\"}},\"typeVersion\":4.1},{\"id\":\"c1ce3400-4582-46c7-a85d-8fa9c325ff7b\",\"name\":\"Remove AI label from email\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[3820,580],\"parameters\":{\"resource\":\"thread\",\"threadId\":\"={{ $('Map fields for further processing').item.json[\\\"threadId\\\"] }}\",\"operation\":\"removeLabels\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"uBcIMfsTtKjexw7I\",\"name\":\"Gmail (workfloowstutorial@gmail.com)\"}},\"typeVersion\":2.1},{\"id\":\"65f0508a-ca2e-49ce-b02f-ef6505b5e983\",\"name\":\"Schedule trigger (1 min)\",\"type\":\"n8n-"}
{"prompt": "Create an n8n workflow for: Scrape and summarize posts of a news site without RSS feed using AI and save them to a NocoDB The News Site from Colt, a telecom company, does not offer an RSS feed, therefore web scraping is the choice to extract and process the news. The goal is to get only the newest posts, a summary of each post and their respective (technical) keywords. Note that the news site offers the links to each news post, but not the individual news. We collect first the links and dates of each post before extracting the newest ones. The result is sent to a SQL database, in this case a NocoDB database. This process happens each week thru a cron job. Requirements: Basic understanding of CSS selectors and how to get them via browser (usually: right click &rarr; inspect) ChatGPT API account - normal account is not sufficient A NocoDB database - of course you may choose any type of output target Assumptions: CSS selectors work on the news site The post has a date with own CSS selector - meaning date is not part of the news content \"Warnings\" Not every site likes to be scraped, especially not in high frequency Each website is structured in different ways, the workflow may then need several adaptations. Askan | 11 months ago | Free", "completion": "{\"id\":\"xM8Z5vZVNTNjCySL\",\"meta\":{\"instanceId\":\"b8ef33547995f2a520f12118ac1f7819ea58faa7a1096148cac519fa08be8e99\"},\"name\":\"News Extraction\",\"tags\":[],\"nodes\":[{\"id\":\"97711d12-20de-40aa-b994-d2b10f20a5e5\",\"name\":\"Extract the HTML with the right css class\",\"type\":\"n8n-nodes-base.html\",\"position\":[-500,0],\"parameters\":{\"options\":{\"trimValues\":true},\"operation\":\"extractHtmlContent\",\"extractionValues\":{\"values\":[{\"key\":\"data\",\"attribute\":\"href\",\"cssSelector\":\"=div:nth-child(9) > div:nth-child(3) > a:nth-child(2)\",\"returnArray\":true,\"returnValue\":\"attribute\"}]}},\"typeVersion\":1},{\"id\":\"b874b570-daae-4878-b525-07ac30756eb1\",\"name\":\"Summary\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[-880,440],\"parameters\":{\"model\":\"gpt-4-1106-preview\",\"prompt\":{\"messages\":[{\"content\":\"=Create a summary in less than 70 words {{ $json[\\\"content\\\"] }}\"}]},\"options\":{},\"resource\":\"chat\"},\"credentials\":{\"openAiApi\":{\"id\":\"0Vdk5RlVe7AoUdAM\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"72696278-2d44-4073-936a-6fe9df1bc7d8\",\"name\":\"Keywords\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[-880,620],\"parameters\":{\"model\":\"gpt-4-1106-preview\",\"prompt\":{\"messages\":[{\"content\":\"=name the 3 most important technical keywords in {{ $json[\\\"content\\\"] }} ? just name them without any explanations or other sentences\"}]},\"options\":{},\"resource\":\"chat\"},\"credentials\":{\"openAiApi\":{\"id\":\"0Vdk5RlVe7AoUdAM\",\"name\":\"OpenAi account\"}},\"type"}
{"prompt": "Create an n8n workflow for: Transcribe Audio Files, Summarize with GPT-4, and Store in Notion Who is this for? This workflow template is perfect for content creators, researchers, students, or anyone who regularly works with audio files and needs to transcribe and summarize them for easy reference and organization. What problem does this workflow solve? Transcribing audio files and summarizing their content can be time-consuming and tedious when done manually. This workflow automates the process, saving users valuable time and effort while ensuring accurate transcriptions and concise summaries. What this workflow does This template automates the following steps: Monitors a specified Google Drive folder for new audio files Sends the audio file to OpenAI's Whisper API for transcription Passes the transcribed text to GPT-4 for summarization Creates a new page in Notion with the summary Setup To set up this workflow: Connect your Google Drive, OpenAI, and Notion accounts to n8n Configure the Google Drive node with the folder you want to monitor for new audio files Set up the OpenAI node with your API key and desired parameters for Whisper and GPT-4 Specify the Notion database where you want the summaries to be stored How to customize this workflow Adjust the Google Drive folder being monitored Modify the OpenAI node parameters to fine-tune the transcription and summarization process Change the Notion database or page properties to match your preferred structure With this AI-powered workflow, you can effortlessly transcribe audio files, generate concise summaries, and store them in a structured manner within Notion. Streamline your audio content processing and organization with this automated template. Pat | 11 months ago | Free", "completion": "{\"id\":\"TWcBOEMLFs7e6KjP\",\"meta\":{\"instanceId\":\"c95a2bbed4422e86c4fa3e73b42c7571c9c1b1107f8abf6b7e8c8144a55fa53c\"},\"name\":\"Whisper Transkription copy\",\"tags\":[],\"nodes\":[{\"id\":\"4bb98287-b0fc-4b34-8cf0-f0870cf313e6\",\"name\":\"Google Drive Trigger\",\"type\":\"n8n-nodes-base.googleDriveTrigger\",\"position\":[1340,560],\"parameters\":{\"event\":\"fileCreated\",\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]},\"triggerOn\":\"specificFolder\",\"folderToWatch\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"182i8n7kpsac79jf04WLYC4BV8W7E_w4E\",\"cachedResultUrl\":\"\",\"cachedResultName\":\"Recordings\"}},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"LtLwYGZCoaOB8E9U\",\"name\":\"Google Drive account\"}},\"typeVersion\":1},{\"id\":\"29cb5298-7ac5-420d-8c03-a6881c94a6a5\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[1580,560],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"={{ $json.id }}\"},\"options\":{\"fileName\":\"={{ $json.originalFilename }}\",\"binaryPropertyName\":\"data\"},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"LtLwYGZCoaOB8E9U\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"45dbc4b3-ca47-4d88-8a32-030f2c3ce135\",\"name\":\"Notion\",\"type\":\"n8n-nodes-base.notion\",\"position\":[2420,560],\"parameters\":{\"title\":\"={{ JSON.parse($json.message.content).audioContentSummary.title }} \",\"pageId\":{\"__rl\":true,\"mode\":\"url\",\"value\":\"\"},\"blockUi\":{\"blockValues\":[{\"type\":\"heading"}
{"prompt": "Create an n8n workflow for: Extract data from resume and create PDF with Gotenberg With this workflow you can extract data from resume documents uploaded via a Telegram bot. Workflow transform readable content of PDF resume into structured data, using AI nodes and returns PDF with formatted, plain HTML. You can modify this workflow to perform other actions with structured data (e.g. insert it into database or create other, well-formatted documents). Functionality of this workflow was presented during the n8n community call on March 7, 2024 - recording of presentation available here. \u26a0\ufe0f Workflow made for demo purposes. If you want to use it in real life, please make sure necessary measures for personal data protection are set. How it works? User uploads readable PDF resume document into Telegram bot. After authentication based on chat ID parameter, workflow extracts text from the PDF and transfers it into AI chain with connected sub-nodes: OpenAI Chat Model and Structured Output (JSON) Parser. Then, each extracted section (employment history, projects etc.) is formatted into desired HTML structure. Finally, the document is converted into new, structured PDF using Gotenberg. \ud83d\udca1 This workflow requires installed Gotenberg. If you are not familiar with this software, please have a look on my YouTube tutorial. You can also replace call to Gotenberg with other PDF generation service (such as PDFMonkey or ApiTemplate). Set up steps Create Telegram bot and add its credentials in n8n. Set your chat ID parameter in Auth node. Adjust JSON schema in Structured Output Parser according to your needs. Optionally: replace HTTP call to Gotenberg with PDF generation service of your choice. If you like this workflow, please subscribe to my YouTube channel and/or my newsletter. Oskar | 11 months ago | Free +2", "completion": "{\"nodes\":[{\"id\":\"79849bb5-00a4-42e6-92c4-b06c7a20eb3e\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1580,340],\"parameters\":{\"model\":\"gpt-4-turbo-preview\",\"options\":{\"temperature\":0,\"responseFormat\":\"json_object\"}},\"credentials\":{\"openAiApi\":{\"id\":\"jazew1WAaSRrjcHp\",\"name\":\"OpenAI (workfloows@gmail.com)\"}},\"typeVersion\":1},{\"id\":\"85df0106-1f78-4412-8751-b84d417c8bf9\",\"name\":\"Convert education to HTML\",\"type\":\"n8n-nodes-base.code\",\"position\":[2420,180],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"jsCode\":\"function convertToHTML(list) {\\n let html = '';\\n\\n list.forEach((education, index) => {\\n if (index > 0) {\\n html += '<br /><br />'; // Add a new line if it's not the first item\\n }\\n html += `<b>Institution:</b> ${education.institution}<br />\\n<b>Start year:</b> ${education.start_year}<br />\\n<b>Degree:</b> ${education.degree}`;\\n });\\n\\n return html;\\n}\\n\\n// Assuming payload is already defined\\nconst payload = $input.item.json.education;\\n\\nconst htmlOutput = convertToHTML(payload);\\nreturn {\\n htmlOutput\\n};\"},\"typeVersion\":2},{\"id\":\"da4fc45d-712f-4171-b72a-66b74b4d8e05\",\"name\":\"Auto-fixing Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserAutofixing\",\"position\":[1820,340],\"parameters\":{},\"typeVersion\":1},{\"id\":\"225a7513-6fd4-4672-9b40-b10b00f121a7"}
{"prompt": "Create an n8n workflow for: ChatGPT Automatic Code Review in Gitlab MR Who this template is for This template is for every engineer who wants to automate their code reviews or just get a 2nd opinion on their PR. How it works This workflow will automatically review your changes in a Gitlab PR using the power of AI. It will trigger whenever you comment with +0 to a Gitlab PR, get the code changes, analyze them with GPT, and reply to the PR discussion. Set up Steps Set up webhook of note_events in Gitlab repository (see here on how to do it) Configure ChatGPT credentials Note \"+0\" in MergeRequest to trigger automatic review by ChatGPT assert | 12 months ago | Free", "completion": "{\"nodes\":[{\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[880,540],\"parameters\":{\"content\":\"## Edit your own prompt \u2b07\ufe0f\\n\"},\"typeVersion\":1},{\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-380,580],\"parameters\":{\"content\":\"## Filter comments and customize your trigger words \u2b07\ufe0f\"},\"typeVersion\":1},{\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-120,560],\"parameters\":{\"content\":\"## Replace your gitlab URL and token \u2b07\ufe0f\"},\"typeVersion\":1},{\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[-540,760],\"webhookId\":\"6cfd2f23-6f45-47d4-9fe0-8f6f1c05829a\",\"parameters\":{\"path\":\"e21095c0-1876-4cd9-9e92-a2eac737f03e\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":1.1},{\"name\":\"Code\",\"type\":\"n8n-nodes-base.code\",\"position\":[720,540],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"jsCode\":\"// Loop over input items and add a new field called 'myNewField' to the JSON of each one\\nvar diff = $input.item.json.gitDiff\\n\\nlet lines = diff.trimEnd().split('\\\\n');\\n\\nlet originalCode = '';\\nlet newCode = '';\\n\\nlines.forEach(line => {\\n console.log(line)\\n if (line.startsWith('-')) {\\n originalCode += line + \\\"\\\\n\\\";\\n } else if (line.startsWith('+')) {\\n newCode += line + \\\"\\\\n\\\";\\n } else {\\n originalCode += line + \\\"\\\\n\\\";\\n newCode += line + \\\"\\\\n\\\";\\n }\\n});\\n\\nreturn {\\n originalCode:originalCode,\\n newCode:newCode\\n};\\n\\n\"},\"typeVersion\":2},{\"name\":\""}
{"prompt": "Create an n8n workflow for: Qualify new leads in Google Sheets via OpenAI's GPT-4 This n8n workflow was developed to evaluate and categorize incoming leads based on certain criteria. The workflow is triggered by adding a new row in a Google Sheets document. The workflow uses the OpenAI node to process the lead information. The system query contains detailed qualification rules and the response format. The user message contains the data for the individual lead. The JSON response from the OpenAI node is then processed by the Edit Fields node to extract the response. This response is merged together with the original lead data by the Merge node. Finally, the Google Sheets node updates the original lead entry in the Google Sheets document with the qualification result (\"qualified\" or \"not qualified\") in a separate column. This allows for easy tracking and sorting of the qualified leads. Yulia | 12 months ago | Free", "completion": "{\"id\":\"8FLJK1NsduFL0Y5P\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"Qualify new leads in Google Sheets via OpenAI's GPT-4\",\"tags\":[{\"id\":\"y9tvM3hISJKT2jeo\",\"name\":\"Ted's Tech Talks\",\"createdAt\":\"2023-08-15T22:12:34.260Z\",\"updatedAt\":\"2023-08-15T22:12:34.260Z\"}],\"nodes\":[{\"id\":\"1f179325-0bec-4e5c-8ebd-0a2bb3ebefaa\",\"name\":\"Merge\",\"type\":\"n8n-nodes-base.merge\",\"position\":[1440,340],\"parameters\":{\"mode\":\"combine\",\"options\":{},\"combinationMode\":\"mergeByPosition\"},\"typeVersion\":2.1},{\"id\":\"7b548661-2b32-451f-ba52-91ca86728f1e\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[358,136.3642172523962],\"parameters\":{\"width\":442,\"height\":360.6357827476038,\"content\":\"### 1. Create a Google Sheet document\\n* This template uses Google Sheet document connected to Google Forms, but a standalone Sheet document will work too\\n* Adapt initial trigger to your needs: check for new entries periodically or add a manual trigger\\n\\n[Link to the Google Sheet template](https://docs.google.com/spreadsheets/d/1jk8ZbfOMObvHGGImc0sBJTZB_hracO4jRqfbryMgzEs)\"},\"typeVersion\":1},{\"id\":\"308b4dce-4656-47bd-b217-69565b1c34f6\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[820,420],\"parameters\":{\"width\":471,\"height\":322,\"content\":\"### 2. Provide lead qualification instructions\\n* Create a __system message__ with overall instructions\\"}
{"prompt": "Create an n8n workflow for: AI-powered WooCommerce Support-Agent With this workflow you get a fully automated AI powered Support-Agent for your WooCommerce webshop. It allows customers to request information about things like: the status of their order the ordered products shipping and billing address current DHL shipping status How it works The workflow receives chat messages from an in a website integrated chat. For security and data-privacy reasons, does the website transmit the email address of the user encrypted with the requests. That ensures that user can just request the information about their own orders. An AI agent with a custom tool supplies the needed information. The tool calls a sub-workflow (in this case, in the same workflow for convenience) to retrieve the required information. This includes the full information of past orders plus the shipping information from DHL. If otherr shipping providers are used it should be simple to adjust the workflow to query information from other APIs like UPS, Fedex or others. Jan Oberhauser | 12 months ago | Free +3", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"be49d136-6392-481d-8544-d4f4d4fd0357\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[90,260],\"parameters\":{\"color\":7,\"width\":1000.427054367056,\"height\":572.2734520891689,\"content\":\"## Find WooCommerce User-ID\\nUser-ID is required to query past orders\"},\"typeVersion\":1},{\"id\":\"5932f77b-63e2-4991-aa16-2b8587b2b560\",\"name\":\"No email provided\",\"type\":\"n8n-nodes-base.set\",\"position\":[400,640],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"9a06428b-4115-4eee-97f4-8e828c5a7e5a\",\"name\":\"response\",\"type\":\"string\",\"value\":\"No email address got provided.\"}]}},\"typeVersion\":3.3},{\"id\":\"909a9a47-8683-4a1f-a359-8f6a878f8cd7\",\"name\":\"If email provided\",\"type\":\"n8n-nodes-base.if\",\"position\":[140,460],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"13f7bd32-5760-4ac3-8292-c8beb131a4a5\",\"operator\":{\"type\":\"string\",\"operation\":\"notEmpty\",\"singleValue\":true},\"leftValue\":\"={{ $json.email }}\",\"rightValue\":\"\"}]}},\"typeVersion\":2},{\"id\":\"f5bd2098-090b-4537-8e61-90afb4c85ad8\",\"name\":\"If user found\",\"type\":\"n8n-nodes-base.if\",\"position\":[620,440],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"st"}
{"prompt": "Create an n8n workflow for: Twitter Virtual AI Influencer Twitter Virtual AI Influencer Workflow Template This n8n workflow template empowers creators to launch a virtual AI influencer that tweets regularly, engaging audiences with a unique niche, writing style, and inspiration. By automating content creation and posting, it ensures a consistent and natural online presence, tailored to your specific influencer profile. Features Scheduled Posting**: Automates tweet posting every 6 hours, with randomized posting minutes to mimic natural activity. On-Demand Posting**: Offers flexibility with manual trigger options for immediate content sharing. Influencer Profile Configuration**: Customize your virtual influencer by defining a target niche, personal writing style, and sources of inspiration. Content Generation**: Leverages advanced AI to craft tweets that resonate with your audience, aiming for viral engagement. Tweet Validation**: Ensures all generated content adheres to Twitter's character limit, maintaining quality and relevance. Workflow Steps Schedule Posting: Configured to post every 6 hours, this step introduces randomness in posting time to simulate human behavior. Trigger Posting Manually: Provides an option to manually initiate a tweet, offering control over the timing of your content. Configure Influencer Profile: Set up your influencer's niche, style, and inspiration to guide the AI in generating targeted content. Generate Tweet Content: Utilizes a sophisticated AI model to produce engaging tweets based on the configured profile. Validate Tweet: Checks if the generated tweet meets Twitter's length constraints, ensuring all content is ready for posting. Post Tweet: Finalizes the process by sharing the AI-generated tweet to your designated Twitter account. Configuration Notes Niche**: Define a specific area of interest, such as \"Modern Stoicism,\" to focus your influencer's content. Writing Style**: Customize the tone and style of the tweets to reflect a personal touch, enhancing relatability. Inspiration**: Input sources of inspiration, including books and philosophies, to steer the content generation process. Getting Started To deploy this template: Import the workflow into your n8n workspace. Customize the influencer profile settings to match your desired niche, style, and inspiration. Connect your Twitter account through the provided OAuth2 credentials setup. Activate the workflow to start building your virtual influencer's presence on Twitter. Embrace the power of AI to create a distinctive and engaging virtual influencer, captivating your audience with minimal effort. Alex Grozav | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"ea9ddb4c-af49-480c-8b73-221b3741069d\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[920,400],\"parameters\":{\"width\":389,\"height\":265,\"content\":\"## Scheduled posting \\nWrite a tweet every 6 hours and randomize the minutes that it's posted at to make it seem natural.\\n\"},\"typeVersion\":1},{\"id\":\"9650b047-7d5e-4ed2-948c-d5be77a94b5d\",\"name\":\"Post tweet\",\"type\":\"n8n-nodes-base.twitter\",\"position\":[2940,520],\"parameters\":{\"text\":\"={{ $json.message.content.tweet }}\",\"additionalFields\":{}},\"credentials\":{\"twitterOAuth2Api\":{\"id\":\"b3qa9dBp2PxbufK3\",\"name\":\"X account\"}},\"typeVersion\":2},{\"id\":\"fd7fc941-37de-4f88-87c0-f62ad1ebe2d6\",\"name\":\"Schedule posting every 6 hours\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[1140,500],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"hours\",\"hoursInterval\":6,\"triggerAtMinute\":\"={{ Math.floor(Math.random() * 60) }}\"}]}},\"typeVersion\":1.1},{\"id\":\"107fd741-5c17-4cd6-98aa-088bf8df523d\",\"name\":\"Trigger posting manually\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[1140,820],\"parameters\":{},\"typeVersion\":1},{\"id\":\"831cd431-56e5-482e-a8a5-e5c5ac078ba4\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1360,400],\"parameters\":{\"width\":389,\"height\":265,\"content\":\"## Configure influencer profile \\"}
{"prompt": "Create an n8n workflow for: Allow your AI to call an API to fetch data Use n8n to bring data from any API to your AI. This workflow uses the Chat Trigger to provide the chat interface, and the Custom n8n Workflow Tool to call a second workflow that calls the API. The second workflow uses AI functionality to refine the API request based on the user's query. It then makes an API call, and returns the response to the main workflow. This workflow is used in Advanced AI examples | Call an API to fetch data in the documentation. To use this workflow: Load it into your n8n instance. Add your credentials as prompted by the notes. Requires n8n 1.28.0 or above Deborah | 1 year ago | Free +3", "completion": "{\"name\":\"Let your AI call an API\",\"nodes\":[{\"id\":\"35ce49a9-eda1-4da5-a6cb-39a39680111b\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[520,1140],\"parameters\":{},\"typeVersion\":1},{\"id\":\"ec6e261b-922f-401c-9392-12ff73488a5f\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[460,960],\"parameters\":{\"color\":7,\"width\":1180.0825159534493,\"height\":775.3931210698682,\"content\":\"### Sub-workflow: Custom tool\\nThe agent above can call this workflow. It calls an example API called \\\"Bored API\\\" and returns a string with an activity idea.\"},\"typeVersion\":1},{\"id\":\"25780e87-0ae0-4bee-9582-75f98bf66635\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[460,460],\"parameters\":{\"color\":7,\"width\":927.5,\"height\":486.5625,\"content\":\"### Main workflow: AI agent using custom tool\"},\"typeVersion\":1},{\"id\":\"f262f04b-964c-47c0-b7dc-a86b82b0184b\",\"name\":\"Chat Trigger\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[600,560],\"webhookId\":\"e3f23177-c7c1-417b-a513-1c1090dda0a2\",\"parameters\":{},\"typeVersion\":1},{\"id\":\"29678c05-9a48-41ec-9c5c-af36ee0d7fc6\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[860,780],\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"e078f88b-489c-42ea-9544-6bdaeae213a0\",\"name\":\"Sticky Note\",\"type\":\"n8n"}
{"prompt": "Create an n8n workflow for: Chat with a database using AI This workflow allows you to ask questions about data stored in a database using AI. To use it, you'll need an OpenAI API key (although you could also swap in a model from another service). Supported databases: Postgres MySQL SQLite The workflow uses n8n's embedded chat, but you could also modify it to work with a chat service such as Slack, MS Teams or WhatsApp. Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"d277b81b-05fd-4733-807a-f63eb6e87742\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[960,500],\"parameters\":{\"model\":\"gpt-4\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"VQtv7frm7eLiEDnd\",\"name\":\"OpenAi account 7\"}},\"typeVersion\":1},{\"id\":\"c56b3269-32b0-4782-ad50-74af8ee2d8a7\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[960,320],\"parameters\":{\"agent\":\"sqlAgent\",\"input\":\"={{ $json.chatInput }}\",\"options\":{},\"dataSource\":\"postgres\"},\"credentials\":{\"postgres\":{\"id\":\"ON7WH990zsLlqqlT\",\"name\":\"Creators site staging\"}},\"typeVersion\":1.2},{\"id\":\"8781c9fd-f6de-4a85-b05d-8f55647304f7\",\"name\":\"Chat Trigger\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[740,320],\"webhookId\":\"792d3290-b52a-486c-85a8-d736e7807c0e\",\"parameters\":{},\"typeVersion\":1},{\"id\":\"3c485755-0594-4a5a-b38c-440694668181\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[620,480],\"parameters\":{\"content\":\"## Try me out\\nClick the 'chat' button at the bottom of the canvas and paste in:\\n\\n_Which tables are available?_\"},\"typeVersion\":1},{\"id\":\"8f296643-7375-4974-be5d-9a4223d11e5f\",\"name\":\"Sticky Note1\",\"type\":\"n8"}
{"prompt": "Create an n8n workflow for: Use any LangChain module in n8n (with the LangChain code node) LangChain is a framework for building AI functionality that users large language models. By leveraging the functionality of LangChain, you can write even more powerful workflows. This workflow shows how you can write LangChain code within n8n, including importing LangChain modules. The workflow itself produces a summary of a YouTube video, when given the video's ID. Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"bd15bdc0-a08a-4a03-a95f-530d9e883243\",\"name\":\"Sticky Note7\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[746,570],\"parameters\":{\"width\":328.41313484548044,\"height\":211.30313955500145,\"content\":\"Before executing, replace `YOUR_API_KEY` with an API key for searchapi.io\"},\"typeVersion\":1},{\"id\":\"b844ae02-f859-4eff-90e4-562b1a810ef5\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[340,640],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f0b52a82-9d0c-4f1e-b392-07bbabec3261\",\"name\":\"Set YouTube video ID\",\"type\":\"n8n-nodes-base.set\",\"position\":[560,640],\"parameters\":{\"fields\":{\"values\":[{\"name\":\"videoId\",\"stringValue\":\"OsMVtuuwOXc\"}]},\"options\":{}},\"typeVersion\":3.2},{\"id\":\"1eff9f6c-9bcf-41dc-83f4-c3a828188cd3\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[800,820],\"parameters\":{\"model\":\"gpt-3.5-turbo-16k\",\"options\":{\"temperature\":0.3}},\"credentials\":{\"openAiApi\":{\"id\":\"VQtv7frm7eLiEDnd\",\"name\":\"OpenAi account 7\"}},\"typeVersion\":1},{\"id\":\"4fa8b4b5-3397-4fc7-a4a4-4876c2a63b70\",\"name\":\"LangChain Code\",\"type\":\"@n8n/n8n-nodes-langchain.code\",\"position\":[780,640],\"parameters"}
{"prompt": "Create an n8n workflow for: Use an open-source LLM (via HuggingFace) This workflow demonstrates how to connect an open-source model to a Basic LLM node. The workflow is triggered when a new manual chat message appears. The message is then run through a Language Model Chain that is set up to process text with a specific prompt to guide the model's responses. Note that open-source LLMs with a small number of parameters require slightly different prompting with more guidance to the model. You can change the default Mistral-7B-Instruct-v0.1 model to any other LLM supported by HuggingFace. You can also connect other nodes, such as Ollama. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"4Nk0fnan0JC9YFS2\",\"meta\":{\"instanceId\":\"03524270bab2c2dfd5b82778cd1355e56cdda3cf098bf2dfd865e18164c00485\"},\"name\":\"Mistral open-source LLM example\",\"tags\":[],\"nodes\":[{\"id\":\"87ca4479-7433-44d1-95ff-3cf8c13d011e\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[760,360],\"parameters\":{},\"typeVersion\":1},{\"id\":\"4a60cb10-c5d8-4850-941b-ecdd07cd32f1\",\"name\":\"Basic LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[980,360],\"parameters\":{\"prompt\":\"=You are a helpful assistant. Please reply politely to the users.\\nUse emojis and a text.\\nQ: {{ $json.input }}\\nA: \"},\"typeVersion\":1},{\"id\":\"a32c19bf-0f0e-4c9f-99ba-79836f10e899\",\"name\":\"Hugging Face Inference Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmOpenHuggingFaceInference\",\"position\":[1000,560],\"parameters\":{\"model\":\"mistralai/Mistral-7B-Instruct-v0.1\",\"options\":{\"maxTokens\":512,\"temperature\":0.8,\"frequencyPenalty\":2}},\"credentials\":{\"huggingFaceApi\":{\"id\":\"FPEdrzelyhrERF11\",\"name\":\"HuggingFaceApi account\"}},\"typeVersion\":1},{\"id\":\"157d6fc5-1523-43fc-937e-92c67b1f3991\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[480,20],\"parameters\":{\"width\":1083,\"height\":317,\"content\":\"## This is an example of basic LLM Chain connected to an open-source model\\n### The Chain is connected to the Mistral-7"}
{"prompt": "Create an n8n workflow for: Personalize marketing emails using customer data and AI This workflow uses AI to analyze customer sentiment from product feedback. If the sentiment is negative, AI will determine whether offering a coupon could improve the customer experience. Upon completing the sentiment analysis, the workflow creates a personalized email templates. This solution streamlines the process of engaging with customers post-purchase, particularly when addressing dissatisfaction, and ensures that outreach is both personalized and automated. This workflow won the 1st place in our last AI contest. Note that to use this template, you need to be on n8n version 1.19.4 or later. | 1 year ago | Free", "completion": "{\"id\":\"Z6TY8mQOfbIvN86d\",\"meta\":{\"instanceId\":\"d7d7b1e232b4091cbd27b67d20aa4c7ff54b11591e8e3f22d5db8387cc91fe3c\"},\"name\":\"AI Competition \u2013 Small Business Problems \u2013 Rebound Email\",\"tags\":[],\"nodes\":[{\"id\":\"12ef13ef-1634-4baa-88c5-2794d10cca5d\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-240,220],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c4d1116c-70e9-4161-a28a-821e91dca5c1\",\"name\":\"Download Dummy Data\",\"type\":\"n8n-nodes-base.httpRequest\",\"notes\":\"uargh.. excel\",\"position\":[-20,220],\"parameters\":{\"url\":\"https://let-the-work-flow.com/dummy/n8n-contest-merch.xlsx\",\"options\":{}},\"notesInFlow\":true,\"typeVersion\":4.1},{\"id\":\"df8f6536-fa39-4d41-86b9-cafd57699089\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-300,120],\"parameters\":{\"width\":715.3278290432247,\"height\":315.32782904322477,\"content\":\"## Get and prepare Dummy Data\"},\"typeVersion\":1},{\"id\":\"4c5e8173-46b4-44b9-8ee8-bd6938594de8\",\"name\":\"Html Template for our Email\",\"type\":\"n8n-nodes-base.html\",\"position\":[2700,-120],\"parameters\":{\"html\":\"<!DOCTYPE html>\\n<html>\\n<head>\\n <meta charset=\\\"UTF-8\\\" />\\n <title>{{ $json['Headline'] }}</title>\\n</head>\\n<body>\\n <div class=\\\"container\\\">\\n <img class=\\\"logo\\\" src=\\\"https://img.logoipsum"}
{"prompt": "Create an n8n workflow for: AI: Conversational agent with custom tool written in JavaScript This workflow implements a custom tool via JavaScript code which returns a random color to users and excludes the given colors. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"HYG74hPY8xIGAzPF\",\"meta\":{\"instanceId\":\"021d3c82ba2d3bc090cbf4fc81c9312668bcc34297e022bb3438c5c88a43a5ff\"},\"name\":\"LangChain Example - Code Tool\",\"tags\":[{\"id\":\"snf16n0p2UrGP838\",\"name\":\"LangChain - Example\",\"createdAt\":\"2023-09-25T16:21:55.962Z\",\"updatedAt\":\"2023-09-25T16:21:55.962Z\"}],\"nodes\":[{\"id\":\"c22c0b8b-d085-4b9d-aa43-f02bfbfa3732\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[680,360],\"parameters\":{},\"typeVersion\":1},{\"id\":\"424805bb-c940-4957-abf0-7463745636ce\",\"name\":\"Code Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolCode\",\"position\":[1340,580],\"parameters\":{\"name\":\"my_color_selector\",\"jsCode\":\"const colors = [\\n 'red',\\n 'green',\\n 'blue',\\n 'yellow',\\n 'pink',\\n 'white',\\n 'black',\\n 'orange',\\n 'brown',\\n];\\n\\nconst ignoreColors = query.split(',').map((text) => text.trim());\\n\\n// remove all the colors that should be ignored\\nconst availableColors = colors.filter((color) => {\\n return !ignoreColors.includes(color);\\n});\\n\\n// Select a random color\\nreturn availableColors[Math.floor(Math.random() * availableColors.length)];\\n\",\"description\":\"Call this tool to get a random color. The input should be a string with comma-separated names of colors to exclude.\"},\"typeVersion\":1},{\"id\":\"6f661ce1-dd64-4bbb-a41c-0bf643c48961\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n"}
{"prompt": "Create an n8n workflow for: Load and summarize Google Drive files with AI This workflow includes advanced features like text summarization and tokenization, it's ideal for automating document processing tasks that require parsing and summarizing text data from Google Drive. To use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"0YsoBMNF71ej3huj\",\"meta\":{\"instanceId\":\"2171524c45c9a6e2ae08ae7c36e794ca5fe44a3e0e7998767887a279f08097cc\"},\"name\":\"Load & Summarize Google Drive files\",\"tags\":[],\"nodes\":[{\"id\":\"6766c882-a6d8-43b3-b5c7-d1e66c6eabd1\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[660,440],\"parameters\":{},\"typeVersion\":1},{\"id\":\"d9c55b59-6169-4fa1-a537-0798fab860c5\",\"name\":\"Summarization Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainSummarization\",\"position\":[1100,440],\"parameters\":{\"type\":\"refine\"},\"typeVersion\":1},{\"id\":\"8cb85bf6-7d3f-4774-b0a0-2abd13bdcfb7\",\"name\":\"Google Drive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[880,440],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"16j_spEguDAj26j5Vij1ToFeTyW_iwphR\",\"cachedResultUrl\":\"https://drive.google.com/file/d/16j_spEguDAj26j5Vij1ToFeTyW_iwphR/view?usp=drivesdk\",\"cachedResultName\":\"bitcoin.pdf\"},\"options\":{},\"operation\":\"download\"},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"yTPPOWwFNgw5MKrl\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"e19d5a5d-da7e-46b5-86fa-51c4d88517e0\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmOpenAi\",\"position\":[1120,620],\"parameters\":{\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"wJt"}
{"prompt": "Create an n8n workflow for: Slack chatbot powered by AI This workflow offers an effective way to handle a chatbot's functionality, making use of multiple tools for information retrieval, conversation context storage, and message sending. It's a setup tailored for a Slack environment, aiming to offer an interactive, AI-driven chatbot experience. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"3tTCy5OOvtcJXpZ5\",\"meta\":{\"instanceId\":\"ec7a5f4ffdb34436e59d23eaccb5015b5238de2a877e205b28572bf1ffecfe04\"},\"name\":\"Slack Chatbot\",\"tags\":[],\"nodes\":[{\"id\":\"50351a19-624e-4ff2-a31d-fb5ed064fae2\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[-500,475],\"webhookId\":\"28b84545-96aa-42f5-990b-aa8783a320ca\",\"parameters\":{\"path\":\"slack-gilfoyle\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":1},{\"id\":\"78d5af93-3f09-463d-8e57-a72f5766a12c\",\"name\":\"Slack\",\"type\":\"n8n-nodes-base.slack\",\"position\":[960,460],\"parameters\":{\"text\":\"={{ $json.output }}\",\"user\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"={{ $('Webhook').item.json.body.event.user }}\"},\"select\":\"user\",\"otherOptions\":{}},\"credentials\":{\"slackApi\":{\"id\":\"X92QgQpOCoF1VHtg\",\"name\":\"Slack account 2\"}},\"typeVersion\":2.1},{\"id\":\"98ec8934-f2a1-484b-ac8c-921ef7f3efe5\",\"name\":\"Chat OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[300,815],\"parameters\":{\"model\":\"gpt-4\",\"options\":{\"temperature\":0.7}},\"credentials\":{\"openAiApi\":{\"id\":\"wJtZwsVKW5v6R2Iy\",\"name\":\"OpenAi account 2\"}},\"typeVersion\":1},{\"id\":\"0cd7299c-20f4-4249-a652-782eaf01b0c2\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[600,815],\""}
{"prompt": "Create an n8n workflow for: OpenAI GPT-3: Company Enrichment from website content Enrich your company lists with OpenAI GPT-3 \u2193 You\u2019ll get valuable information such as: Market (B2B or B2C) Industry Target Audience Value Proposition This will help you to: add more personalization to your outreach make informed decisions about which accounts to target I've made the process easy with an n8n workflow. Here is what it does: Retrieve website URLs from Google Sheets Extract the content for each website Analyze it with GPT-3 Update Google Sheets with GPT-3 data Lucas Perret | 2 years ago | Free", "completion": "{\"meta\":{\"instanceId\":\"f0a68da631efd4ed052a324b63ff90f7a844426af0398a68338f44245d1dd9e5\"},\"nodes\":[{\"id\":\"04750e9b-6ce3-401b-89e7-f1f17f3a4a28\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-180,300],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7a8bb997-5a2d-4ee0-a1ca-bebe9fe32bc2\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[640,460],\"parameters\":{\"url\":\"=https://www.{{ $node[\\\"Split In Batches\\\"].json[\\\"Domain\\\"] }}\",\"options\":{\"redirect\":{\"redirect\":{\"followRedirects\":true}}}},\"typeVersion\":3,\"continueOnFail\":true},{\"id\":\"6409f0c4-bf93-4a1d-a74c-e294fb39895f\",\"name\":\"HTML Extract\",\"type\":\"n8n-nodes-base.htmlExtract\",\"position\":[820,460],\"parameters\":{\"options\":{\"trimValues\":false},\"extractionValues\":{\"values\":[{\"key\":\"body\",\"cssSelector\":\"html\"}]}},\"typeVersion\":1,\"continueOnFail\":true},{\"id\":\"f45fcc6a-9ccd-43c9-9eaf-1797768e1e62\",\"name\":\"OpenAI\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1140,460],\"parameters\":{\"prompt\":\"=This is the content of the website {{ $node[\\\"Split In Batches\\\"].json[\\\"Domain\\\"] }}:\\\"{{ $json[\\\"contentShort\\\"] }}\\\"\\n\\nIn a JSON format:\\n\\n- Give me the value proposition of the company. In less than 25 words. In English. Casual Tone. Format is: \\\"[Company Name] helps [target audience] [achieve desired outcome] and [additional benefit]\\\"\\n\\n- Give me the industry of the company. (Classify using this industry list: [Agriculture"}
{"prompt": "Create an n8n workflow for: Summarize SERPBear data with AI (via Openrouter) and save it to Baserow Who's this for? If you own a website and need to analyze your keyword rankings If you need to create a keyword report on your rankings If you want to grow your keyword positions SerpBear is an opensourced SEO tool specifically for keyword analytics. Click here to read details of how I use it Example output of A.I. Key Observations about Ranking Performance: The top-performing keyword is \u201cOpenrouter N8N\u201d with a current position of 7 and an improving trend. Two keywords, \u201cBest Docker Synology\u201d and \u201cBitwarden Synology\u201d, are not ranking in the top 100 and have a stable trend. Three keywords, \u201cObsidian Second Brain\u201d, \u201cAI Generated Reference Letter\u201d, and \u201cActual Budget Synology\u201d, and \u201cN8N Workflow Generator\u201d are not ranking well and have a declining trend. Keywords showing the most improvement: \u201cOpenrouter N8N\u201d has an improving trend and a relatively high ranking of 7. Keywords needing attention: \u201cObsidian Second Brain\u201d has a declining trend and a low ranking of 69. \u201cAI Generated Reference Letter\u201d has a declining trend and a low ranking of 84. \u201cActual Budget Synology\u201d, \u201cN8N Workflow Generator\u201d, \u201cBest Docker Synology\u201d, and \u201cBitwarden Synology\u201d are not ranking in the top 100. Use case Instead of hiring an SEO expert, I run this report weekly. It checks the keyword rankings of the past week and gives me recommendations on what to improve. How it works The workflow gathers SerpBear analytics for the past 7 days. It passes the data to openrouter.ai for A.I. analysis. Finally it saves to baserow. How to use this Input your SerpBearcredentials Enter your domain name Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Date, Note, Blog Created by Rumjahn Keith Rumjahn | 3 months ago | Free", "completion": "{\"id\":\"qmmXKcpJOCm9qaCk\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"SERPBear analytics template\",\"tags\":[],\"nodes\":[{\"id\":\"2ad0eb40-6628-4c6b-bc15-7081e7712f1a\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[260,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"5a3c9ad8-a562-4bb0-bb11-c325552d8101\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[260,160],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\"}]}},\"typeVersion\":1.2},{\"id\":\"bdfa7388-f9b3-4145-90de-2e58138e14bf\",\"name\":\"Get data from SerpBear\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[580,260],\"parameters\":{\"url\":\"https://myserpbearinstance.com/api/keyword?id=22\",\"options\":{},\"sendQuery\":true,\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"queryParameters\":{\"parameters\":[{\"name\":\"domain\",\"value\":\"rumjahn.com\"}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"3fshHb4fyI5XfLyq\",\"name\":\"Header Auth account 6\"}},\"executeOnce\":false,\"typeVersion\":4.2,\"alwaysOutputData\":false},{\"id\":\"c169f4e3-ab60-4b46-9f49-cf27a13dd7c6\",\"name\":\"Parse data from SerpBear\",\"type\":\"n8n-nodes-base.code\",\"position\":[820,260],\"parameters\":{\"jsCode\":\"const keywords = items[0].json.keywords;\\nconst today = new Date().toISOString().split('T')[0];\\n\\n// Create summary for each keyword\\n"}
{"prompt": "Create an n8n workflow for: Summarize Umami data with AI (via Openrouter) and save it to Baserow Who's this for? Anyone who wants to improve the SEO of their website Umami users who want insights on how to improve their site SEO managers who need to generate reports weekly Case study You can read more about how this works here. How it works This workflow calls the Umami API to get data Then it sends the data to A.I. for analysis It saves the data and analysis to Baserow How to use this Input your Umami credentials Input your website property ID Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Date, Summary, Top Pages, Blog (name of your blog). Future development Use this as a template. There's alot more Umami stats you can pull from the API. Change the A.I. prompt to give even more detailed analysis. Created by Rumjahn Keith Rumjahn | 4 months ago | Free", "completion": "{\"id\":\"eZT6SZ4Kvmq5TzyQ\",\"meta\":{\"instanceId\":\"558d88703fb65b2d0e44613bc35916258b0f0bf983c5d4730c00c424b77ca36a\",\"templateCredsSetupCompleted\":true},\"name\":\"Umami analytics template\",\"tags\":[],\"nodes\":[{\"id\":\"8a54ac1c-a072-42e6-a3ba-8cde33475eb5\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[460,220],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e81c9be0-f59d-467e-9bda-eeb2d66ed31e\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[460,380],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\",\"triggerAtDay\":[4]}]}},\"typeVersion\":1.2},{\"id\":\"01b04872-9aea-4834-8df5-f6c91914133d\",\"name\":\"Get view stats from Umami\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[760,260],\"parameters\":{\"url\":\"=https://umami.mydomain.com/api/websites/86d4095c-a2a8-4fc8-9521-103e858e2b41/event-data/stats?startAt={{ DateTime.now().minus({ days: 7 }).toMillis() }}&endAt={{ DateTime.now().toMillis() }}&unit=hour&timezone=Asia%2FHong_Kong\",\"options\":{},\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"FKsKXvQUlaX5qt9n\",\"name\":\"Header Auth account 3\"}},\"typeVersion\":4.2},{\"id\":\"38d342e3-10ad-4260-8f44-5a3233ec3166\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[660,-260"}
{"prompt": "Create an n8n workflow for: Enhance Security Operations with the Qualys Slack Shortcut Bot! Enhance Security Operations with the Qualys Slack Shortcut Bot! Our Qualys Slack Shortcut Bot is strategically designed to facilitate immediate security operations directly from Slack. This powerful tool allows users to initiate vulnerability scans and generate detailed reports through simple Slack interactions, streamlining the process of managing security assessments. Workflow Highlights: Interactive Modals**: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations. Dynamic Workflow Execution**: Integrates seamlessly with Qualys to execute vulnerability scans and create reports based on user-specified parameters. Real-Time Feedback**: Offers instant feedback within Slack, updating users about the status of their requests and delivering reports directly through Slack channels. Operational Flow: Parse Webhook Data**: Captures and parses incoming data from Slack to understand user commands accurately. Execute Actions**: Depending on the user's selection, the workflow triggers other sub-workflows like 'Qualys Start Vulnerability Scan' or 'Qualys Create Report' for detailed processing. Respond to Slack**: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses. Setup Instructions: Verify that Slack and Qualys API integrations are correctly configured for seamless interaction. Customize the modal interfaces to align with your organization's operational protocols and security policies. Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected. Need Assistance? Explore our Documentation or get help from the n8n Community for more detailed guidance on setup and customization. Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of vulnerabilities and streamlined reporting. To handle the actual processing of requests, you will also need to deploy these two subworkflows: Qualys Start Vulnerability Scan Qualys Create Report To simplify deployment, use this Slack App manifest to quickly create an app with the correct permissions: { \"display_information\": { \"name\": \"Qualys n8n Bot\", \"description\": \"n8n Integration for Qualys\", \"background_color\": \"#2a2b2e\" }, \"features\": { \"bot_user\": { \"display_name\": \"Qualys n8n Bot\", \"always_online\": false }, \"shortcuts\": [ { \"name\": \"Scan Report Generator\", \"type\": \"global\", \"callback_id\": \"qualys-scan-report\", \"description\": \"Generate a report from the latest scan to review vulnerabilities and compliance.\" }, { \"name\": \"Launch Qualsys VM Scan\", \"type\": \"global\", \"callback_id\": \"trigger-qualys-vmscan\", \"description\": \"Start a Qualys Vulnerability scan from the comfort of your Slack Workspace\" } ] }, \"oauth_config\": { \"scopes\": { \"bot\": [ \"commands\", \"channels:join\", \"channels:history\", \"channels:read\", \"chat:write\", \"chat:write.customize\", \"files:read\", \"files:write\" ] } }, \"settings\": { \"interactivity\": { \"is_enabled\": true, \"request_url\": \"Replace everything inside the double quotes with your workflow webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad\"\", \"message_menu_options_url\": \"Replace everything inside the double quotes with your workflow message options webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad\"\" }, \"org_deploy_enabled\": false, \"socket_mode_enabled\": false, \"token_rotation_enabled\": false } } Angel Menendez | 4 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"03e9d14e9196363fe7191ce21dc0bb17387a6e755dcc9acc4f5904752919dca8\"},\"nodes\":[{\"id\":\"adfda9cb-1d77-4c54-b3ea-e7bf438a48af\",\"name\":\"Parse Webhook\",\"type\":\"n8n-nodes-base.set\",\"position\":[760,640],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"e63f9299-a19d-4ba1-93b0-59f458769fb2\",\"name\":\"response\",\"type\":\"object\",\"value\":\"={{ $json.body.payload }}\"}]}},\"typeVersion\":3.3},{\"id\":\"b3e0e490-18e0-44b5-a960-0fdbf8422515\",\"name\":\"Qualys Create Report\",\"type\":\"n8n-nodes-base.executeWorkflow\",\"position\":[1720,1740],\"parameters\":{\"options\":{},\"workflowId\":\"icSLX102kSS9zNdK\"},\"typeVersion\":1},{\"id\":\"80ae074b-bda5-4638-b46f-246a1b9530ae\",\"name\":\"Required Report Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[1520,1740],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"47cd1502-3039-4661-a6b1-e20a74056550\",\"name\":\"report_title\",\"type\":\"string\",\"value\":\"={{ $json.response.view.state.values.report_title.report_title_input.value }}\"},{\"id\":\"6a8a0cbf-bf3e-4702-956e-a35966d8b9c5\",\"name\":\"base_url\",\"type\":\"string\",\"value\":\"https://qualysapi.qg3.apps.qualys.com\"},{\"id\":\"9a15f4db-f006-4ad8-a2c0-4002dd3e2655\",\"name\":\"output_format\",\"type\":\"string\",\"value\":\"={{ $json.response.view.state.values.output_format.output"}
{"prompt": "Create an n8n workflow for: Monthly Spotify Track Archiving and Playlist Classification Monthly Spotify Track Archiving and Playlist Classification This n8n workflow allows you to automatically archive your monthly Spotify liked tracks in a Google Sheet, along with playlist details and descriptions. Based on this data, Claude 3.5 is used to classify each track into multiple playlists and add them in bulk. Who is this template for? This workflow template is perfect for Spotify users who want to systematically archive their listening history and organize their tracks into custom playlists. What problem does this workflow solve? It automates the monthly process of tracking, storing, and categorizing Spotify tracks into relevant playlists, helping users maintain well-organized music collections and keep a historical record of their listening habits. Workflow Overview Trigger Options**: Can be initiated manually or on a set schedule. Spotify Playlists Retrieval**: Fetches the current playlists and filters them by owner. Track Details Collection**: Retrieves information such as track ID and popularity from the user\u2019s library. Audio Features Fetching**: Uses Spotify's API to get audio features for each track. Data Merging**: Combines track information with their audio features. Duplicate Checking**: Filters out tracks that have already been logged in Google Sheets. Data Logging**: Archives new tracks into a Google Sheet. AI Classification**: Uses an AI model to classify tracks into suitable playlists. Playlist Updates**: Adds classified tracks to the corresponding playlists. Setup Instructions Credentials Setup: Make sure you have valid Spotify OAuth2 and Google Sheets access credentials. Trigger Configuration: Choose between manual or scheduled triggers to start the workflow. Google Sheets Preparation: Set up a Google Sheet with the necessary structure for logging track details. Spotify Playlists Setup: Have a diverse range of playlists and exhaustive description (see example) ready to accommodate different music genres and moods. Customization Options Adjust Playlist Conditions**: Modify the AI model\u2019s classification criteria to align with your personal music preferences. Enhance Track Analysis**: Incorporate additional audio features or external data sources for more refined track categorization. Personalize Data Logging**: Customize which track attributes to log in Google Sheets based on your archival preferences. Configure Scheduling**: Set a preferred schedule for periodic track archiving, e.g., monthly or weekly. Cost Estimate For 300 tracks, the token usage amounts to approximately 60,000 tokens (58,000 for input and 2,000 for completion), costing around 20 cents with Claude 3.5 Sonnet (as of October 2024). Playlists' Description Examples | Playlist Name | Playlist Description | |-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Classique | Indulge in the timeless beauty of classical music with this refined playlist. From baroque to romantic periods, this collection showcases renowned compositions. | | Poi | Find your flow with this dynamic playlist tailored for poi, staff, and ball juggling. Featuring rhythmic tracks that complement your movements. | | Pro Sound | Boost your productivity and focus with this carefully selected mix of concentration-enhancing music. Ideal for work or study sessions. | | ChillySleep | Drift off to dreamland with this soothing playlist of sleep-inducing tracks. Gentle melodies and ambient sounds create a peaceful atmosphere for restful sleep. | | To Sing | Warm up your vocal cords and sing your heart out with karaoke-friendly tracks. Featuring popular songs, perfect for solo performances or group sing-alongs. | | 1990s | Relive the diverse musical landscape of the 90s with this eclectic mix. From grunge to pop, hip-hop to electronic, this playlist showcases defining genres. | | 1980s | Take a nostalgic trip back to the era of big hair and neon with this 80s playlist. Packed with iconic hits and forgotten gems, capturing the energy of the decade.| | Groove Up | Elevate your mood and energy with this upbeat playlist. Featuring a mix of feel-good tracks across various genres to lift your spirits and get you moving. | | Reggae & Dub | Relax and unwind with the laid-back vibes of reggae and dub. This playlist combines classic reggae tunes with deep, spacious dub tracks for a chilled-out vibe. | | Psytrance | Embark on a mind-bending journey with this collection of psychedelic trance tracks. Ideal for late-night dance sessions or intense focus. | | Cumbia | Sway to the infectious rhythms of Cumbia with this lively playlist. Blending traditional Latin American sounds with modern interpretations for a danceable mix. | | Funky Groove | Get your body moving with this collection of funk and disco tracks. Featuring irresistible basslines and catchy rhythms, perfect for dance parties. | | French Chanson | Experience the romance and charm of France with this mix of classic and modern French songs, capturing the essence of French musical culture. | | Workout Motivation | Push your limits and power through your exercise routine with this high-energy playlist. From warm-up to cool-down, these tracks will keep you motivated. | | Cinematic Instrumentals | Immerse yourself in a world of atmospheric sounds with this collection of cinematic instrumental tracks, perfect for focus, relaxation, or contemplation. | Arnaud MARIE | 4 months ago | Free +2", "completion": "{\"meta\":{\"instanceId\":\"8e95de061dd3893a50b8b4c150c8084a7848fb1df63f53533941b7c91a8ab996\"},\"nodes\":[{\"id\":\"6325369f-5881-4e4e-b71b-510a64b236ef\",\"name\":\"Retrieve relevant info\",\"type\":\"n8n-nodes-base.set\",\"position\":[1260,400],\"parameters\":{\"mode\":\"raw\",\"options\":{},\"jsonOutput\":\"={\\n\\\"track\\\" : \\\"{{ $json.track.name.replaceAll('\\\"',\\\"'\\\") }}\\\",\\n\\\"artist\\\": \\\"{{ $json.track.artists[0].name }}\\\",\\n\\\"album\\\" :\\\"{{ $json.track.album.name }}\\\",\\n\\\"track_spotify_uri\\\" : \\\"{{ $json.track.uri }}\\\",\\n\\\"track_spotify_id\\\" : \\\"{{ $json.track.id }}\\\",\\n\\\"external_urls\\\": \\\"{{ $json.track.external_urls.spotify }}\\\",\\n\\\"track_popularity\\\" : \\\"{{ $json.track.popularity }}\\\",\\n\\\"album_release_date\\\" : \\\"{{ $json.track.album.release_date.toDateTime().year }}\\\"\\n}\"},\"typeVersion\":3.4},{\"id\":\"2252fe16-6ee7-4fbe-b74e-d9bdcc7ad708\",\"name\":\"Batch preparation\",\"type\":\"n8n-nodes-base.code\",\"position\":[1560,280],\"parameters\":{\"jsCode\":\"const items = $input.all();\\nconst trackSpotifyIds = items.map((item) => item?.json?.track_spotify_id);\\n\\nconst aggregatedItems = [];\\nfor (let i = 0; i < trackSpotifyIds.length; i += 100) {\\n aggregatedItems.push({\\n json: {\\n trackSpotifyIds: trackSpotifyIds.slice(i, i + 100),\\n },\\n });\\n}\\n\\nreturn aggregatedItems;\\n\"},\"typeVersion\":2},{\"id\":\"83c181f8-ed"}
{"prompt": "Create an n8n workflow for: Venafi Cloud Slack Cert Bot Enhance Security Operations with the Venafi Slack CertBot! Venafi Presentation - Watch Video Our Venafi Slack CertBot is strategically designed to facilitate immediate security operations directly from Slack. This tool allows end users to request Certificate Signing Requests that are automatically approved or passed to the Secops team for manual approval depending on the Virustotal analysis of the requested domain. Not only does this help centralize requests, but it helps an organization maintain the security certifications by allowing automated processes to log and analyze requests in real time. Workflow Highlights: Interactive Modals**: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations. Dynamic Workflow Execution**: Integrates seamlessly with Venafi to execute CSR generation and if any issues are found, AI can generate a custom report that is then passed to a slack teams channel for manual approval with the press of a single button. Operational Flow: Parse Webhook Data**: Captures and parses incoming data from Slack to understand user commands accurately. Execute Actions**: Depending on the user's selection, the workflow triggers other actions within the flow like automatic Virustotal Scanning. Respond to Slack**: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses. Setup Instructions: Verify that Slack and Qualys API integrations are correctly configured for seamless interaction. Customize the modal interfaces to align with your organization's operational protocols and security policies. Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected. Need Assistance? Explore Venafi's Documentation or get help from the n8n Community for more detailed guidance on setup and customization. Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of CSR's. Angel Menendez | 5 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"1092ab50-67a0-4e50-8c10-f05f70b45f56\",\"name\":\"Venafi TLS Protect Cloud\",\"type\":\"n8n-nodes-base.venafiTlsProtectCloud\",\"position\":[2860,1700],\"parameters\":{\"options\":{},\"commonName\":\"={{ $('Parse Webhook').item.json.response.view.state.values.domain_name_block.domain_name_input.value.match(/^(\\\\*\\\\.)?([a-zA-Z0-9-]+\\\\.)+[a-zA-Z]{2,}$/g).toString() }}\",\"generateCsr\":true,\"applicationId\":\"f3c15c80-7151-11ef-9a22-abeac49f7094\",\"additionalFields\":{\"organizationalUnits\":[\"={{ $json.name }}\"]},\"certificateIssuingTemplateId\":\"d28d82b1-714b-11ef-9026-7bb80b32867a\"},\"credentials\":{\"venafiTlsProtectCloudApi\":{\"id\":\"WU38IpfutNNkJWuo\",\"name\":\"Venafi TLS Protect Cloud account\"}},\"typeVersion\":1},{\"id\":\"0c1f1b92-2da4-413f-a4cc-68c816e8511c\",\"name\":\"Parse Webhook\",\"type\":\"n8n-nodes-base.set\",\"position\":[440,1100],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"e63f9299-a19d-4ba1-93b0-59f458769fb2\",\"name\":\"response\",\"type\":\"object\",\"value\":\"={{ $json.body.payload }}\"}]}},\"typeVersion\":3.3},{\"id\":\"95fb1907-c9e0-4164-b0b0-c3691bb46b9a\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[108.346754831423"}
{"prompt": "Create an n8n workflow for: Community Insights using Qdrant, Python and Information Extractor This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings: Survey Insights Customer Insights Community Insights This template demonstrates the Community Insights scenario where HN commments can be quickly grouped by similarity and an AI agent can generate insights on those groupings. With this workflow, Researchers or HN users can quickly breakdown community consensus on a particular topic and identify frequently mentioned positives and negatives. Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vQXaQU9XxsxnUIIeqmmf1PuYRuYtwviVXTv6Mz9Vo6_a4ty-XaJHSeZsptjWXS3wGGDG8Z4u16rvE7l/pubhtml How it works HN comments are imported via the Hacknews API node. Comments are then inserted into a Qdrant collection carefully tagged with the Hackernews API metadata. Comments are then fetched and are put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups. Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for. The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the researcher or the HN user. Requirements Works best with lots of comments! Qdrant Vectorstore for storing embeddings. OpenAI account for embeddings and LLM. Customising the Template Adjust clustering parameters which make sense for your data. Adjust sentimentality setting if comments are overwhelmingly negative at times. Jimleuk | 7 months ago | Free +4", "completion": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"5961a808-a873-497e-bc42-5b760ded1571\",\"name\":\"When clicking \u2018Test workflow\u2019\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[380,360],\"parameters\":{},\"typeVersion\":1},{\"id\":\"7fa03eaa-7865-46ce-9f58-7e19fc0ec89b\",\"name\":\"Hacker News\",\"type\":\"n8n-nodes-base.hackerNews\",\"position\":[1200,400],\"parameters\":{\"articleId\":\"={{ $('Set Variables').item.json.story_id }}\",\"additionalFields\":{\"includeComments\":true}},\"typeVersion\":1},{\"id\":\"82675738-9df7-47a3-8363-264bb09255f4\",\"name\":\"Split Out\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[1560,400],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"data\"},\"typeVersion\":1},{\"id\":\"6800be57-40da-4d80-ac35-304403423263\",\"name\":\"Get Comments\",\"type\":\"n8n-nodes-base.set\",\"position\":[1380,400],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"91110cf7-1932-43ca-b24e-9d4ed40447e6\",\"name\":\"data\",\"type\":\"array\",\"value\":\"={{\\n$json.children.flatMap(item => {\\n return [\\n { id: item.id, story_id: item.story_id, story_title: $json.title, author: item.author, text: item.text },\\n ...item.children.flatMap(item1 => {\\n return [\\n { id: item1.id, story_id: item1.story_id, story_title: $json.title, author: item1.author, text: item1.text },\\n ...item1.children.flatMap(item2 => {\\n return"}
{"prompt": "Create an n8n workflow for: Enrich Pipedrive's Organization Data with OpenAI GPT-4o & Notify it in Slack This workflow enriches new Pipedrive organization's data by adding a note to the organization object in Pipedrive. It assumes there is a custom \"website\" field in your Pipedrive setup, as data will be scraped from this website to generate a note using OpenAI. Then, a notification is sent in Slack. \u26a0\ufe0f Disclaimer This workflow uses a scraping API. Before using it, ensure you comply with the regulations regarding web scraping in your country or state. Important Notes The OpenAI model used is GPT-4o, chosen for its large input token capacity. However, it is not the cheapest model if cost is very important to you. The system prompt in the OpenAI Node generates output with relevant information, but feel free to improve or modify it according to your needs. How It Works Node 1: Pipedrive Trigger - An Organization is Created This is the trigger of the workflow. When an organization object is created in Pipedrive, this node is triggered and retrieves the data. Make sure you have a \"website\" custom field in Pipedrive (the name of the field in the n8n node will appear as a random ID and not with the Pipedrive custom field name). Node 2: ScrapingBee - Get Organization's Website's Homepage Content This node scrapes the content from the URL of the website associated with the Pipedrive Organization created in Node 1. The workflow uses the ScrapingBee API, but you can use any preferred API or simply the HTTP request node in n8n. Node 3: OpenAI - Message GPT-4o with Scraped Data This node sends HTML-scraped data from the previous node to the OpenAI GPT-4o model. The system prompt instructs the model to extract company data, such as products or services offered and competitors (if known by the model), and format it as HTML for optimal use in a Pipedrive Note. Node 4: Pipedrive - Create a Note with OpenAI Output This node adds a Note to the Organization created in Pipedrive using the OpenAI node output. The Note will include the company description, target market, selling products, and competitors (if GPT-4o was able to determine them). Node 5 & 6: HTML To Markdown & Code - Markdown to Slack Markdown These two nodes format the HTML output to Slack Markdown. The Note created in Pipedrive is in HTML format, as specified by the System Prompt of the OpenAI Node. To send it to Slack, it needs to be converted to Markdown and then to Slack Markdown. Node 7: Slack - Notify This node sends a message in Slack containing the Pipedrive Organization Note created with this workflow. Dataki | 8 months ago | Free", "completion": "{\"id\":\"\",\"meta\":{\"instanceId\":\"\",\"templateCredsSetupCompleted\":true},\"name\":\"piepdrive-test\",\"tags\":[],\"nodes\":[{\"id\":\"b2838678-c796-4c99-a3da-a2cd1b42ea97\",\"name\":\"Pipedrive Trigger - An Organization is created\",\"type\":\"n8n-nodes-base.pipedriveTrigger\",\"position\":[820,380],\"webhookId\":\"f5de09a8-6601-4ad5-8bc8-9b3f4b83e997\",\"parameters\":{\"action\":\"added\",\"object\":\"organization\"},\"credentials\":{\"pipedriveApi\":{\"id\":\"\",\"name\":\"Pipedrive Connection\"}},\"typeVersion\":1},{\"id\":\"5aa05d79-b2fa-4040-b4ca-cad83adf2798\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-20,120],\"parameters\":{\"width\":656.3637637842876,\"height\":1455.9537026322007,\"content\":\"# Enrich Pipedrive's Organization Data with GPT-4o When an Organization is Created in Pipedrive\\n\\nThis workflow **enriches a Pipedrive organization's data by adding a note to the organization object in Pipedrive**. It assumes there is a custom \\\"website\\\" field in your Pipedrive setup, as data will be scraped from this website to generate a note using OpenAI.\\n\\n## \u26a0\ufe0f Disclaimer\\n**These workflows use a scraping API. Before using it, ensure you comply with the regulations regarding web scraping in your country or state**.\\n\\n## Important Notes\\n- The OpenAI model used is GPT-4o, chosen for its large input token context capacity. However, it is also **the most expensive option**, you should take cost into consideration.\\n\\n- The system prompt in the OpenAI Node generates output with relevant information, but feel free to improve or **modify it according to your needs**.\\n\\n## **How It Works**\\n\\n### Node 1: `Pipedrive Trigger - An Organization is Created`\\nThis is the trigger of the workflow. When **an organization object is created in Pipedrive**,"}
{"prompt": "Create an n8n workflow for: Classify lemlist replies using OpenAI and automate reply handling Who this is for This workflow is for sales people who want to quickly and efficiently follow up with their leads What this workflow does This workflow starts every time a new reply is received in lemlist. It then classifies the response using openAI and creates the correct follow up task. The follow-up tasks currently include: Slack alerts when a lead for each new replies Tag interested leads in lemlist Unsubscription of leads when they request it The Slack alerts include: Lead email address Sender email address Reply type (positive, not interested...etc) A preview of the reply Setup To set this template up, simply follow the stickies steps in it How to customize this workflow to your needs Adjust the follow up tasks to your needs Change the Slack notification to your needs ... Lucas Perret | 8 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"2b1cc1a8b0a2fb9caab11ab2d5eb3712f9973066051b2e898cf4041a1f2a7757\",\"templateCredsSetupCompleted\":true},\"nodes\":[{\"id\":\"7786165e-5e74-4614-b065-86db19482b72\",\"name\":\"Format text with Markdown\",\"type\":\"n8n-nodes-base.markdown\",\"position\":[-1200,980],\"parameters\":{\"html\":\"={{ $json.text }}\",\"options\":{},\"destinationKey\":\"textClean\"},\"typeVersion\":1,\"continueOnFail\":true},{\"id\":\"8f73d4d6-2473-4fdf-8797-c049d6df6967\",\"name\":\"Lemlist Trigger - On new reply\",\"type\":\"n8n-nodes-base.lemlistTrigger\",\"position\":[-1600,980],\"webhookId\":\"039bb443-8d2a-4eb3-9c16-772943a46db7\",\"parameters\":{\"event\":\"emailsReplied\",\"options\":{\"isFirst\":true}},\"typeVersion\":1},{\"id\":\"1f94d672-0a70-45ad-bf96-72c4aecabcd0\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1700,680],\"parameters\":{\"width\":304.92548549441915,\"height\":504.9663351162785,\"content\":\"### Get your lemlist API key\\n\\n1. Go to your lemlist account or create one [HERE](https://app.lemlist.com/create-account)\\n\\n2. Go to Settings -> Integrations\\n\\n3. Generate your API Key and copy it\\n\\n4. On this node, click on create new credential and paste your API key\"},\"typeVersion\":1},{\"id\":\"3032b04c-76a2-4f7c-a790-ede26b102254\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-2040,680],\"parameters\":{\"width\":319.6621253622332,\""}
{"prompt": "Create an n8n workflow for: AI-Powered Children's Arabic Storytelling on Telegram Template for Kids' Story in Arabic The n8n template for creating kids' stories in Arabic offers a versatile platform for storytellers to captivate young audiences with educational and interactive tales. It allows for customization to suit various use cases and can be set up effortlessly. Check this example: https://t.me/st0ries95 Use Cases Educational Platforms: Educational platforms can automate the creation and distribution of educational stories in Arabic for children using this template. By incorporating visual and auditory elements into the storytelling process, educational platforms can enhance learning experiences and engage young learners effectively. Children's Libraries: Children's libraries can utilize this template to curate and share a diverse collection of Arabic stories with young readers. The automated generation of visual content and audio files enhances the storytelling experience, encouraging children to immerse themselves in new worlds and characters through captivating narratives. Language Learning Apps: Language learning apps focused on Arabic can integrate this template to offer culturally rich storytelling experiences for children learning the language. By translating stories into Arabic and supplementing them with visual and auditory components, these apps can facilitate language acquisition in an enjoyable and interactive manner. Configuration Guide for Nodes OpenAI Chat Model Nodes: Functionality**: Allows interaction with the OpenAI GPT-4 Turbo model. Purpose**: Enables communication with advanced chat capabilities. Create a Prompt for DALL-E Node: Customization**: Tailor prompts for generating relevant visual content. Summarization**: Define prompts for visual content generation without text. Generate an Image for the Story Node: Resource Type**: Specifies image as the resource. Prompt Setup**: Configures prompt for textless image creation within the visual content. Generate Audio for the Story Node: Resource Type**: Chooses audio as the resource. Input Definition**: Sets input text for audio file generation. Translate the Story to Arabic Node: Chunking Mode Selection**: Allows advanced chunking mode choice. Summarization Configuration**: Sets method and prompts for story translation into Arabic. Send the Story To Channel Node: Channel ID**: Specifies the channel ID for sending the story text. Text Configuration**: Sets up the text to be sent to the channel. By following these node descriptions, users can effectively configure the n8n template for kids' stories in Arabic, tailoring it to specific use cases for a seamless and engaging storytelling experience for young audiences. AlQaisi | 10 months ago | Free", "completion": "{\"meta\":{\"instanceId\":\"84ba6d895254e080ac2b4916d987aa66b000f88d4d919a6b9c76848f9b8a7616\",\"templateId\":\"2234\"},\"nodes\":[{\"id\":\"e0f68f60-f036-4103-a9fc-d6cb80b6f8a2\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1980,1100],\"parameters\":{\"model\":\"gpt-4-turbo\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"kDo5LhPmHS2WQE0b\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"23779dea-c21d-42da-b493-09394bc64436\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[2420,660],\"parameters\":{\"model\":\"gpt-4-turbo\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"kDo5LhPmHS2WQE0b\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"af59863e-12c5-414c-bf64-dd6712e3aa7b\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[1680,960],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"hours\",\"hoursInterval\":12}]}},\"typeVersion\":1.1},{\"id\":\"bc2ad02b-72c9-4132-96e8-b64487f589f7\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[2160,1140],\"parameters\":{\"options\":{},\"chunkSize\":500,\"chunkOverlap\":300},\"typeVersion\":1},{\"id\":\"cb11a8bb-bdca-43cb-a586-7f93471d58f7\",\"name\":\"OpenAI Chat"}
{"prompt": "Create an n8n workflow for: Configure your own Image Creation API Using OpenAI DALLE-3 How it works: Webhook URL that responds to Requests with an AI generated Image based on the prompt provided in the URL. Setup Steps: Ideate your prompt URL Encode The Prompt (as shown in the Template) Authenticate with your OpenAI Credentials Put together the Webhook URL with your prompt and enter into a webbrowser In this way you can expose a public url to users, employee's etc. without exposing your OpenAI API Key to them. Click here to find a blog post with additional information. Bela | 11 months ago | Free", "completion": "{\"id\":\"wDD4XugmHIvx3KMT\",\"meta\":{\"instanceId\":\"149cdf730f0c143663259ddc6124c9c26e824d8d2d059973b871074cf4bda531\"},\"name\":\"Image Generation API\",\"tags\":[],\"nodes\":[{\"id\":\"d743f947-ad45-4e59-97d4-79b98eaddedb\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[260,-20],\"webhookId\":\"970dd3c6-de83-46fd-9038-33c470571390\",\"parameters\":{\"path\":\"970dd3c6-de83-46fd-9038-33c470571390\",\"options\":{},\"responseMode\":\"responseNode\"},\"typeVersion\":1.1},{\"id\":\"832e993e-69e9-475b-8322-776d88da0440\",\"name\":\"Respond to Webhook\",\"type\":\"n8n-nodes-base.respondToWebhook\",\"position\":[1400,-20],\"parameters\":{\"options\":{},\"respondWith\":\"binary\"},\"typeVersion\":1},{\"id\":\"53044a93-375f-48f2-971d-bf765bcdb7a0\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[180,-120],\"parameters\":{\"width\":301.7420425026802,\"height\":260.80333469825376,\"content\":\"## Webhook Trigger \\n**This Node starts listening to requests to the Webhook URL**\\n\\n\"},\"typeVersion\":1},{\"id\":\"c7b3b04e-903b-4d7c-bbf1-2bc2f1b1a426\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[180,-460],\"parameters\":{\"width\":469.32758643852594,\"height\":297.34454352637044,\"content\":\"## Creating your Prompt-URL \\n**To use this Workflow you need to append your prompt to your Webhook URL in the following way**\\n\\n1. Take your Webhook URL\\n2. Ideate"}
{"prompt": "Create an n8n workflow for: Automate Screenshots with URLbox & Analyze them with AI In this automation we first make a screenshot with a screenshot API called URLbox and then send this screenshot into the OpenAI API and analyze it. You can extend this automation by the way you want to ingest the website url's & names into this workflow. Options as data source: Postgres Google Sheets Your CRM ... Setup: Replace Website & URL in Setup Node Put in your URLbox API Key Put in your OpenAI credentials Click here for a blog article with more information on the automation. Bela | 11 months ago | Free", "completion": "{\"id\":\"wDD4XugmHIvx3KMT\",\"meta\":{\"instanceId\":\"149cdf730f0c143663259ddc6124c9c26e824d8d2d059973b871074cf4bda531\"},\"name\":\"Analyze Screenshots with AI\",\"tags\":[],\"nodes\":[{\"id\":\"6d7f34b8-6203-4512-a428-7b5a18c63db6\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[240,1100],\"parameters\":{\"width\":373.2796418305297,\"height\":381.1230421279239,\"content\":\"## Setup \\n**For Testing use the Setup node to put in test name & url.**\\n\\nIf you want to use this workflow in production, you can expand it to load data from other sources like a DB or Google Sheet\"},\"typeVersion\":1},{\"id\":\"ae568c65-e8f6-45bb-9c96-a870da1fc7d6\",\"name\":\"Setup\",\"type\":\"n8n-nodes-base.set\",\"position\":[360,1320],\"parameters\":{\"values\":{\"string\":[{\"name\":\"website_name\",\"value\":\"=n8n\"},{\"name\":\"url\",\"value\":\"https://n8n.io/\"}]},\"options\":{}},\"typeVersion\":2},{\"id\":\"ca9f0357-a596-4453-b351-fdd8d47c81ad\",\"name\":\"URLbox API Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[780,1120],\"parameters\":{\"url\":\"https://api.urlbox.io/v1/render/sync\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"sendHeaders\":true,\"bodyParameters\":{\"parameters\":[{\"name\":\"url\",\"value\":\"={{ $json.url }}\"},{\"name\":\"full_page\",\"value\":true}]},\"headerParameters\":{\"parameters\":[{\"name\":\"Authorization\",\"value\":\"YOUR_API_KEY\"}]}},\"retryOnFail\":true,\"typeVersion\":4.1},{\"id\":\"3caffa3c-657a-4f74-a3cb-daf7beb67890\",\"name\":\"Sticky Note1\",\"type"}
{"prompt": "Create an n8n workflow for: Assistant for Hubspot Chat using OpenAi and Airtable This workflow will allow you to use OpenAI Assistant API together with a chatting platform. This version is configured to work with Hubspot, however, the Hubspot modules can be replaced by other platform and it will work similarly. Prerequisites: Create a Hubspot Chat (Live chat available on free plan) or Chatflow (paid hubspot only) and configure it to send all replies toward an n8n webhook (you need to create a custom app for that. I will create a separate article on how to do it, meanwhile, feel free to message me if you need support. Setup: Create a OpenAI Assistant, define its functionality and functions Update the Hubspot modules with the Hubspot API Key Update the OpenAI modules with OpenAI API Key Create an Airtable or any other database where you keep a reference between the thread id in Hubspot and Assistant API If you need help deploying this solution don't hesitate to email me or schedule a call here. Tiberiu S - Makeitfuture.com | 11 months ago | Free", "completion": "{\"id\":\"C2pB17EpXAJwOcst\",\"meta\":{\"instanceId\":\"ba379c9b99d35340c90344105e7e5d06ca0de3e88926f0384d2c23099dad1937\"},\"name\":\"OpenAI Assistant for Hubspot Chat\",\"tags\":[],\"nodes\":[{\"id\":\"7f11a684-911b-4fbc-ba1b-a8e7bce8e914\",\"name\":\"getHubspotMessage\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[280,580],\"parameters\":{\"url\":\"=https://api.hubapi.com/conversations/v3/conversations/threads/{{ $json[\\\"body\\\"][0][\\\"objectId\\\"] }}/messages/{{ $json[\\\"body\\\"][0][\\\"messageId\\\"] }}\",\"options\":{},\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"hubspotAppToken\"},\"credentials\":{\"hubspotAppToken\":{\"id\":\"56nluFhXiGjYN1EY\",\"name\":\"HubSpot App Token tinder\"},\"hubspotOAuth2Api\":{\"id\":\"y6819fYl4TsW9gl6\",\"name\":\"HubSpot account 6\"},\"hubspotDeveloperApi\":{\"id\":\"dHB9nVcnZTqf2JDX\",\"name\":\"HubSpot Developer account\"}},\"typeVersion\":4.1},{\"id\":\"687bcbb8-38c8-4d21-a46f-186e880d003c\",\"name\":\"OpenAi Create Thread\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1260,420],\"parameters\":{\"url\":\"https://api.openai.com/v1/threads\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n \\\"messages\\\": [\\n {\\n \\\"role\\\": \\\"user\\\",\\n \\\"content\\\": \\\"{{ $('getHubspotMessage').item.json[\\\"text\\\"] }}\\\"\\n }\\n ]\\n}\",\"sendBody\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"headerParameters\":{\"parameters\":[{\"name\":\"openai-beta\",\"value\":\"assistants=v1\"}]"}
{"prompt": "Create an n8n workflow for: Summarize Google Sheets form feedback via OpenAI's GPT-4 This n8n workflow was developed to collect and summarize feedback from an event that was collected via a Google Form and saved in a Google Sheets document. The workflow is triggered manually by clicking on the \"Test workflow\" button. The Google Sheets node retrieves the responses from the feedback form. The Aggregate node then combines all responses for each question into arrays and prepares the data for analysis. The OpenAI node processes the aggregated feedback data. System Prompt instructs the model to analyze the responses and generate a summary report that includes the overall sentiment regarding the event and constructive suggestions for improvement. The Markdown node converts the summary report, which is in Markdown format, into HTML. Finally, the Gmail node sends an HTML-formatted email to the specified email address. Yulia | 12 months ago | Free", "completion": "{\"id\":\"Lwvu2jjMU2irTyAY\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"Summarize Google Sheets form feedback via OpenAI's GPT-4\",\"tags\":[{\"id\":\"y9tvM3hISJKT2jeo\",\"name\":\"Ted's Tech Talks\",\"createdAt\":\"2023-08-15T22:12:34.260Z\",\"updatedAt\":\"2023-08-15T22:12:34.260Z\"}],\"nodes\":[{\"id\":\"cd80cd2f-a6e1-48eb-ba05-0f8f1a0875e5\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[680,320],\"parameters\":{},\"typeVersion\":1},{\"id\":\"9f03f1c4-c47e-4eda-bc0a-a598c21e4616\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[640,130],\"parameters\":{\"width\":369.1031874662338,\"height\":349,\"content\":\"### 1. Create a Google Sheet document\\n* This tutorial uses Google Sheet document connected to Google Forms, but a standalone Sheet document will work too\\n* Adapt initial trigger to your needs: run manually or at some time intervals\\n\\n[Link to the Google Sheets template](https://docs.google.com/spreadsheets/d/1Kcr1oF_RrfNQJczmJDpwClOSYpvSnwbeX-_pdUo91-I/edit?usp=sharing)\"},\"typeVersion\":1},{\"id\":\"1e478f81-76e7-4fc3-a147-11a92d3f9998\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1040,160],\"parameters\":{\"width\":394,\"height\":319,\"content\":\"### 2. Combine all answers into an array\\n* Since the main goal is to provide an overall summary, we need to combine all answers for each"}
{"prompt": "Create an n8n workflow for: Classify new bugs in Linear with OpenAI's GPT-4 and move them to the right team Use case When working with multiple teams, bugs must get in front of the right team as quickly as possible to be resolved. Normally this includes a manual grooming of new bugs that have arrived in your ticketing system (in our case Linear). We found this way too time-consuming. That's why we built this workflow. What this workflow does This workflow triggers every time a Linear issue is created or updated within a certain team. For us at n8n, we created one general team called Engineering where all bugs get added in the beginning. The workflow then checks if the issue meets the criteria to be auto-moved to a certain team. In our case, that means that the description is filled, that it has the bug label, and that it's in the Triage state. The workflow then classifies the bug using OpenAI's GPT-4 model before updating the team property of the Linear issue. If the AI fails to classify a team, the workflow sends an alert to Slack. Setup Add your Linear and OpenAi credentials Change the team in the Linear Trigger to match your needs Customize your teams and their areas of responsibility in the Set me up node. Please use the format Teamname. Also, make sure that the team names match the names in Linear exactly. Change the Slack channel in the Set me up node to your Slack channel of choice. How to adjust it to your needs Play around with the context that you're giving to OpenAI, to make sure the model has enough knowledge about your teams and their areas of responsibility Adjust the handling of AI failures to your needs How to enhance this workflow At n8n we use this workflow in combination with some others. E.g. we have the following things on top: We're using an automation that enables everyone to add new bugs easily with the right data via a /bug command in Slack (check out this template if that's interesting to you) This workflow was built using n8n version 1.30.0 Niklas Hatje | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"8920dc6e-b2fb-4446-8cb3-f3f6d626dcb3\",\"name\":\"Linear Trigger\",\"type\":\"n8n-nodes-base.linearTrigger\",\"position\":[420,360],\"webhookId\":\"a02faf62-684f-44bb-809f-e962c9ede70d\",\"parameters\":{\"teamId\":\"7a330c36-4b39-4bf1-922e-b4ceeb91850a\",\"resources\":[\"issue\"],\"authentication\":\"oAuth2\"},\"credentials\":{\"linearOAuth2Api\":{\"id\":\"02MqKUMdPxr9t3mX\",\"name\":\"Nik's Linear Creds\"}},\"typeVersion\":1},{\"id\":\"61214884-62f9-4a00-9517-e2d51b44d0ae\",\"name\":\"Only tickets that need to be classified\",\"type\":\"n8n-nodes-base.filter\",\"position\":[1000,360],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"bc3a756d-b2b6-407b-91c9-a1cd9da004e0\",\"operator\":{\"type\":\"string\",\"operation\":\"notContains\"},\"leftValue\":\"={{ $('Linear Trigger').item.json.data.description }}\",\"rightValue\":\"Add a description here\"},{\"id\":\"f3d8d0fc-332d-41a6-aef8-1f221bf30c0e\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $('Linear Trigger').item.json.data.state.id }}\",\"rightValue\":\"6b9a8eec-82dc-453a-878b-50f4c98d3e53\"},{\"id\":\"9cdb55b2-3ca9-43bd-84b0-ef025b59ce18\",\"operator\":{\"type\":\"number\",\""}
{"prompt": "Create an n8n workflow for: Ask a human for help when the AI doesn't know the answer This is a workflow that tries to answer user queries using the standard GPT-4 model. If it can't answer, it sends a message to Slack to ask for human help. It prompts the user to supply an email address. This workflow is used in Advanced AI examples | Ask a human in the documentation. To use this workflow: Load it into your n8n instance. Add your credentials as prompted by the notes. Configure the Slack node to use your Slack details, or swap out Slack for a different service. Deborah | 1 year ago | Free", "completion": "{\"name\":\"Ask a human\",\"nodes\":[{\"id\":\"a60c8572-56c1-4bf3-8352-a6419a475887\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[900,760],\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"b4f2e26c-903b-46b8-bd8b-110fd64de9e4\",\"name\":\"Not sure?\",\"type\":\"@n8n/n8n-nodes-langchain.toolWorkflow\",\"position\":[1120,760],\"parameters\":{\"name\":\"dont_know_tool\",\"fields\":{\"values\":[{\"name\":\"chatInput\",\"stringValue\":\"={{ $('Chat Trigger').item.json.chatInput }}\"}]},\"workflowId\":\"={{ $workflow.id}}\",\"description\":\"Use this tool if you don't know the answer to the user's question, or if you're not very confident about your answer.\"},\"typeVersion\":1},{\"id\":\"951cc691-b422-4ce6-901f-b7feb3afd1ad\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[540,1360],\"parameters\":{},\"typeVersion\":1},{\"id\":\"194ba9c0-e256-449a-8da7-ac5339123a99\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[500,1020],\"parameters\":{\"color\":7,\"width\":1118.3459011229047,\"height\":775.3931210698682,\"content\":\"### Sub-workflow: Custom tool\\nThe agent above can call this workflow. It checks if the user has supplied an email address. If they haven't it prompts them to provide one. If they have, it messages a customer support channel for help.\"},\"typeVersion\":1},{\"id\":\"38c6b363-45a7-4e72-9e40-8c0df3cc480f\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[500,460],\"parameters\":{\"color\":7,\"width\":927"}
{"prompt": "Create an n8n workflow for: Convert text to speech with OpenAI How It Works This workflow sends an HTTP request to OpenAI's Text-to-Speech (TTS) model, returning an .mp3 audio recording of the provided text. This template is meant to be adapted for your individual use case, and requires a valid OpenAI credential. Gotchas Per OpenAI's Usage Policies, you must provide a clear disclosure to end users that the TTS voice they are hearing is AI-generated and not a human voice, if you are using this workflow to provide audio output to users. Kevin Cole | 1 year ago | Free", "completion": "{\"id\":\"6Yzmlp5xF6oHo1VW\",\"meta\":{\"instanceId\":\"173f55e6572798fa42ea9c5c92623a3c3308080d3fcd2bd784d26d855b1ce820\"},\"name\":\"Text to Speech (OpenAI)\",\"tags\":[],\"nodes\":[{\"id\":\"938fedbd-e34c-40af-af2f-b9c669e1a6e9\",\"name\":\"When clicking \\\"Test workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[380,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"1d59db5d-8fe6-4292-a221-a0d0194c6e0c\",\"name\":\"Set input text and TTS voice\",\"type\":\"n8n-nodes-base.set\",\"position\":[760,380],\"parameters\":{\"mode\":\"raw\",\"options\":{},\"jsonOutput\":\"{\\n \\\"input_text\\\": \\\"The quick brown fox jumped over the lazy dog.\\\",\\n \\\"voice\\\": \\\"alloy\\\"\\n}\\n\"},\"typeVersion\":3.2},{\"id\":\"9d54de1d-59b7-4c1f-9e88-13572da5292c\",\"name\":\"Send HTTP Request to OpenAI's TTS Endpoint\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1120,380],\"parameters\":{\"url\":\"https://api.openai.com/v1/audio/speech\",\"method\":\"POST\",\"options\":{},\"sendBody\":true,\"sendHeaders\":true,\"authentication\":\"predefinedCredentialType\",\"bodyParameters\":{\"parameters\":[{\"name\":\"model\",\"value\":\"tts-1\"},{\"name\":\"input\",\"value\":\"={{ $json.input_text }}\"},{\"name\":\"voice\",\"value\":\"={{ $json.voice }}\"}]},\"headerParameters\":{\"parameters\":[{\"name\":\"Authorization\",\"value\":\"Bearer $OPENAI_API_KEY\"}]},\"nodeCredentialType\":\"openAiApi\"},\"credentials\":{\"openAiApi\":{\"id\":\"VokTSv2Eg5m5aDg7\",\"name\":\"OpenAi account\"}},\"typeVersion\":4.1},{\"id\":\"1ce72c9c-aa"}
{"prompt": "Create an n8n workflow for: Translate audio using AI Overview This workflow takes some French text, and translates it into spoken audio. It then transcribes that audio back into text, translates it into English and generates an audio file of the English text. To do so, it uses ElevenLabs (which has a free tier) and OpenAI. Setup These steps should only take a few minutes: In ElevenLabs, add a voice to your voice lab and copy its ID. Add it to the 'Set voice ID' node Get your ElevenLabs API key (click your name in the bottom-left of ElevenLabs and choose \u2018profile\u2019) In the 'Generate French audio' node, create a new header auth cred. Set the name to xi-api-key and the value to your API key In the 'credential' field of the 'Transcribe audio' node, create a new OpenAI cred with your OpenAI API key Run the workflow by clicking the orange button at the bottom of the canvas David Roberts | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"aa0c62d1-2a5e-4336-8783-a8a21cb23374\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1180,760],\"parameters\":{\"options\":{\"temperature\":0}},\"credentials\":{\"openAiApi\":{\"id\":\"VQtv7frm7eLiEDnd\",\"name\":\"OpenAi account 7\"}},\"typeVersion\":1},{\"id\":\"0c7d21e6-5bf6-4927-ad23-008b22e2ffde\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[280,560],\"parameters\":{},\"typeVersion\":1},{\"id\":\"352de912-3a36-4bf2-b013-b46e0ace38e9\",\"name\":\"Generate French Audio\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[720,560],\"parameters\":{\"url\":\"=https://api.elevenlabs.io/v1/text-to-speech/{{ $json.voice_id }}\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\\"text\\\":\\\"{{ $json.text }}\\\",\\\"model_id\\\":\\\"eleven_multilingual_v2\\\",\\\"voice_settings\\\":{\\\"stability\\\":0.5,\\\"similarity_boost\\\":0.5}}\",\"sendBody\":true,\"sendQuery\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"queryParameters\":{\"parameters\":[{\"name\":\"optimize_streaming_latency\",\"value\":\"1\"}]},\"headerParameters\":{\"parameters\":[{\"name\":\"accept\",\"value\":\"audio/mpeg\"}]}},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"OMni1VQQclVYOmeZ\",\"name\":\"ElevenLabs David\"}},\"typeVersion\":4.1},{\"id\":\"0cde2e89-0669"}
{"prompt": "Create an n8n workflow for: Qualify replies from Pipedrive persons with AI About the workflow The workflow reads every reply that is received from a cold email campaign and qualifies if the lead is interested in a meeting. If the lead is interested, a deal is made in pipedrive. You can add as many email inboxes as you need! Setup: Add credentials to the Gmail, OpenAI and Pipedrive Nodes. Add a in_campaign field in Pipedrive for persons. In Pipedrive click on your credentials at the top right, go to company settings &gt; Data fields &gt; Person and click on add custom field. Single option [TRUE/FALSE]. If you have only one email inbox, you can delete one of the Gmail nodes. If you have more than two email inboxes, you can duplicate a Gmail node as many times as you like. Just connect it to the Get email node, and you are good to go! In the Gmail inbox nodes, select Inbox under label names and uncheck Simplify. Mark de Jonge | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"0bd9e607aabfd58640f9f5a370e768a7755e93315179f5bcc6d1f8f114b3567a\"},\"nodes\":[{\"id\":\"97b36168-7fa8-4a97-a6cc-c42496918c4c\",\"name\":\"Search Person in CRM\",\"type\":\"n8n-nodes-base.pipedrive\",\"position\":[-880,400],\"parameters\":{\"term\":\"={{ $json.from.value[0].address }}\",\"limit\":1,\"resource\":\"person\",\"operation\":\"search\",\"additionalFields\":{\"includeFields\":\"\"}},\"credentials\":{\"pipedriveApi\":{\"id\":\"MdJQDtRDHnpwuVYP\",\"name\":\"Pipedrive LinkedUp\"}},\"typeVersion\":1},{\"id\":\"2a17582b-9375-4a01-87d9-a50f573b83db\",\"name\":\"In campaign?\",\"type\":\"n8n-nodes-base.if\",\"position\":[-420,400],\"parameters\":{\"conditions\":{\"string\":[{\"value1\":\"={{ $json.in_campaign }}\",\"value2\":\"True\"}]}},\"typeVersion\":1},{\"id\":\"2a8d509f-8ac2-4f45-a905-f34552833381\",\"name\":\"Get person from CRM\",\"type\":\"n8n-nodes-base.pipedrive\",\"position\":[-640,400],\"parameters\":{\"personId\":\"={{ $json.id }}\",\"resource\":\"person\",\"operation\":\"get\",\"resolveProperties\":true},\"credentials\":{\"pipedriveApi\":{\"id\":\"MdJQDtRDHnpwuVYP\",\"name\":\"Pipedrive LinkedUp\"}},\"typeVersion\":1},{\"id\":\"b9c6f3d3-1a6d-4144-8e77-3a3c6e5282d8\",\"name\":\"Is interested?\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[-180,380],\"parameters\":{\"model\":\"gpt-4\",\"prompt\":{\"messages\":[{\"content\":\"=You are the best sales development representative in the world. You send cold email messages daily to CEOs and founders of companies. You do this to persuade"}
{"prompt": "Create an n8n workflow for: OpenAI assistant with custom tools This workflow shows how you can get your OpenAI assistant to call an n8n workflow as a tool. Since you can put almost any functionality in an n8n workflow, this means you can give your assistant access to almost any data source. Note that to use this template, you need to be on n8n version 1.19.4 or later. David Roberts | 1 year ago | Free", "completion": "{\"id\":\"aVTi7K9mFjK5OjAV\",\"meta\":{\"instanceId\":\"b3a8efae31a34c2224655b66499bee098263a56d266da574e8820468780b7ddd\"},\"name\":\"OpenAI Assistant with custom n8n tools\",\"tags\":[],\"nodes\":[{\"id\":\"d15e7634-408b-43c5-a8d6-afcbc83479a9\",\"name\":\"On new manual Chat Message\",\"type\":\"@n8n/n8n-nodes-langchain.manualChatTrigger\",\"position\":[600,300],\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"5d9ad043-adbe-4970-aa4e-b81dfcb9e255\",\"name\":\"OpenAI Assistant\",\"type\":\"@n8n/n8n-nodes-langchain.openAiAssistant\",\"position\":[820,300],\"parameters\":{\"options\":{},\"assistantId\":\"asst_BWy0154vMGMdrX7MjCYaYv6a\"},\"credentials\":{\"openAiApi\":{\"id\":\"au6fQZN7it62DWlS\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"0c3aded2-886d-4c9f-8d6e-2729f12b6711\",\"name\":\"Execute Workflow Trigger\",\"type\":\"n8n-nodes-base.executeWorkflowTrigger\",\"position\":[600,960],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c77010ac-82e6-40f2-92c4-c360d276b896\",\"name\":\"Mapping data\",\"type\":\"n8n-nodes-base.code\",\"position\":[1080,820],\"parameters\":{\"jsCode\":\"return [\\n {\\n \\\"country\\\": \\\"Wakanda\\\",\\n \\\"capital\\\": \\\"Birnin Zana\\\"\\n },\\n {\\n \\\"country\\\": \\\"Narnia\\\",\\n \\\"capital\\\": \\\"Cair Paravel\\\"\\n },\\n {\\n \\\"country\\\": \\\"Gondor\\\",\\n \\\"capital\\\": \\\"Minas Tirith\\\"\\n },\\n {\\n \\\"country\\\": \\\"Oz\\\",\\n \\\"capital\\\": \\\"The"}
{"prompt": "Create an n8n workflow for: AI Customer feedback sentiment analysis LOOM Walkthrough: https://www.loom.com/share/4fbe06872cb8483993e7792018594f08 How it works Create a form for customer feedback, have OpenAI classify the sentiment of the feedback (positive/neutral/negative) and store it in Google sheets! Set up steps Connect Google sheets Connect your OpenAi account (api key + org Id) Create a customer feedback form, use an existing one or use the one below as example. All set! Here is the example google sheet being used in this workflow: https://docs.google.com/spreadsheets/d/1omWdRbiT6z6GNZ6JClu9gEsRhPQ6J0EJ2yXyFH9Zng4/edit?usp=sharing. You can download it to your account. | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"82a17fa4a0b8e81bf77e5ab999d980f392150f2a9541fde626dc5f74857b1f54\"},\"nodes\":[{\"id\":\"4ea39a4f-d8c1-438f-9738-bfbb906a3d7a\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1200,1020],\"parameters\":{\"width\":253,\"height\":342,\"content\":\"## Send customer feedback to OpenAI for sentiment analysis\"},\"typeVersion\":1},{\"id\":\"6962ea41-7d15-4932-919f-21ac94fa1269\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1960,1180],\"parameters\":{\"width\":253,\"height\":342,\"content\":\"## Add new feedback to google sheets\"},\"typeVersion\":1},{\"id\":\"4c8a8984-2d8e-4139-866b-6f3536aced07\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[800,1600],\"parameters\":{\"width\":1407,\"height\":254,\"content\":\"## Instructions\\n1. Connect Google sheets\\n2. Connect your OpenAi account (api key + org Id)\\n3. Create a customer feedback form, use an existing one or use the one below as example. \\nAll set!\\n\\n\\n- Here is the example google sheet being used in this workflow: https://docs.google.com/spreadsheets/d/1omWdRbiT6z6GNZ6JClu9gEsRhPQ6J0EJ2yXyFH9Zng4/edit?usp=sharing. You can download it to your account.\"},\"typeVersion\":1},{\"id\":\"d43a9574-626d-4817-87ba-d99bdd6f41dc\",\"name\":\"Sticky Note5\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[800,1160],\"parameters\":{\"width\":253,\"height\":342,\"content\":\"## Feedback form is submitted\"},\"typeVersion\":1},{\"id\":\"76dab"}
{"prompt": "Create an n8n workflow for: Prepare CSV files with GPT-4 This workflow generates CSV files containing a list of 10 random users with specific characteristics using OpenAI's GPT-4 model. It then splits this data into batches, converts it to CSV format, and saves it to disk for further use. The execution of the workflow begins from here when triggered manually. \"OpenAI\" Node. This uses the OpenAI API to generate random user data. The input to the OpenAI API is a fixed string, which asks for a list of 10 random users with some specific attributes. The attributes include a name and surname starting with the same letter, a subscription status, and a subscription date (if they are subscribed). There is also a short example of the JSON object structure. This technique is called one-shot prompting. \"Split In Batches\" Node. This node is used to handle the OpenAI responses one by one. \"Parse JSON\" Node. This node converts the content of the message received from the OpenAI node (which is in string format) into a JSON object. \"Make JSON Table\" Node. This node is used to convert the JSON data into a tabular format, which is easier to handle for further data processing. \"Convert to CSV\" Node. This node converts the table format data received from the \"Make JSON Table\" node into CSV format and assigns a file name. \"Save to Disk\" Node. This node is used to save the CSV generated in the previous node to disk in the \".n8n\" directory. The workflow is designed in a circular manner. So, after saving the file to disk, it goes back to the \"Split In Batches\" node to process the OpenAI output, until all batches are processed. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"6FSx5OMVxp8Ldg8A\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"Prepare CSV files with GPT-4\",\"tags\":[],\"nodes\":[{\"id\":\"5b43e57d-1fe1-4ea6-bf3d-661f7e5fc4b0\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[960,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"291466e8-1592-4080-a675-5e9f486d0d05\",\"name\":\"OpenAI\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1160,240],\"parameters\":{\"model\":\"gpt-4\",\"prompt\":{\"messages\":[{\"content\":\"=please create a list of 10 random users. Return back ONLY a JSON array. Character names of famous fiction characters. Make Names and Surnames start with the same letter. Name and Surname can be from different characters. If subscribed is false then make date_subscribed empty. If date_subscribed is not empty then make it random and no later then 2023-10-01. Make JSON in a single line, avoid line breaks. Here's an example: [{\\\"user_name\\\": \\\"Jack Jones\\\", \\\"user_email\\\":\\\"jackjo@yahoo.com\\\",\\\"subscribed\\\": true, \\\"date_subscribed\\\":\\\"2023-10-01\\\" },{\\\"user_name\\\": \\\"Martin Moor\\\", \\\"user_email\\\":\\\"mmoor@gmail.com\\\",\\\"subscribed\\\": false, \\\"date_subscribed\\\":\\\"\\\" }]\"}]},\"options\":{\"n\":3,\"maxTokens\":2500,\"temperature\":1},\"resource\":\"chat\"},\"credentials\":{\"openAiApi\":{\"id\":\"63\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"edd5bed7-a8a1-4298-b026-3b0061c5064a\",\"name\":\"Split In Batches\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[13"}
{"prompt": "Create an n8n workflow for: AI: Ask questions about any data source (using the n8n workflow retriever) This template aims to perform Q&A on data retrieved from another n8n workflow. Since that workflow can be used to retrieve any data from any service, this template can be used to ask questions about any data. It uses a manual trigger, various AI nodes, and an OpenAI Chat Model to extract and provide relevant information based on a specific query. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"mjCQV12PbF6fw8hR\",\"meta\":{\"instanceId\":\"021d3c82ba2d3bc090cbf4fc81c9312668bcc34297e022bb3438c5c88a43a5ff\"},\"name\":\"LangChain - Example - Workflow Retriever\",\"tags\":[{\"id\":\"snf16n0p2UrGP838\",\"name\":\"LangChain - Example\",\"createdAt\":\"2023-09-25T16:21:55.962Z\",\"updatedAt\":\"2023-09-25T16:21:55.962Z\"}],\"nodes\":[{\"id\":\"efdc3050-6c68-4419-9f12-f37d6fefb276\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[460,200],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e0edb9ab-c59f-4d34-983d-861bb2df4f01\",\"name\":\"Workflow Retriever\",\"type\":\"@n8n/n8n-nodes-langchain.retrieverWorkflow\",\"position\":[1120,440],\"parameters\":{\"workflowId\":\"QacfBRBnf1xOyckC\"},\"typeVersion\":1},{\"id\":\"ba47dd13-67d0-499a-b9a2-16928099efce\",\"name\":\"Retrieval QA Chain2\",\"type\":\"@n8n/n8n-nodes-langchain.chainRetrievalQa\",\"position\":[900,200],\"parameters\":{},\"typeVersion\":1},{\"id\":\"f6d16571-0573-4860-aed9-611f93b050ad\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[800,480],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"4jRB4A20cPycBqP5\",\"name\":\"OpenAI account - n8n\"}},\"typeVersion\":1},{\"id\":\"4fd00751-3db0-489b-8c7f"}
{"prompt": "Create an n8n workflow for: Force AI to use a specific output format This workflow is for anyone looking to automatically fetch, validate, and parse complex language-based queries into a structured format. Its unique capability lies in not only processing language but also fixing invalid outputs before structuring them. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"cKRViOHDPsosO7UX\",\"meta\":{\"instanceId\":\"ec7a5f4ffdb34436e59d23eaccb5015b5238de2a877e205b28572bf1ffecfe04\"},\"name\":\"[AI/LangChain] Output Parser 4\",\"tags\":[],\"nodes\":[{\"id\":\"3d669ba2-65b7-4502-92d9-645c4e51b26d\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[380,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"9a509299-746d-4a3f-b379-8a4a9a92c75a\",\"name\":\"Prompt\",\"type\":\"n8n-nodes-base.set\",\"position\":[600,240],\"parameters\":{\"values\":{\"string\":[{\"name\":\"input\",\"value\":\"Return the 5 largest states by area in the USA with their 3 largest cities and their population.\"}]},\"options\":{}},\"typeVersion\":2},{\"id\":\"e2092fe6-d803-43e9-b2df-b0fc7aa83b02\",\"name\":\"LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1060,240],\"parameters\":{},\"typeVersion\":1},{\"id\":\"711734d0-1003-4639-bdee-c160f6f976b3\",\"name\":\"Structured Output Parser\",\"type\":\"@n8n/n8n-nodes-langchain.outputParserStructured\",\"position\":[1560,900],\"parameters\":{\"jsonSchema\":\"{\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"state\\\": {\\n \\\"type\\\": \\\"string\\\"\\n },\\n \\\"cities\\\": {\\n \\\"type\\\": \\\"array\\\",\\n \\\"items\\\": {\\n \\\"type\\\": \\\"object\\\",\\n \\\"properties\\\": {\\n \\\"name\\\": \\\"string\\\",\\n \\\"population\\\": \\\"number\\\"\\n }\\n }\\n }\\n }\\n}\"},\"typeVersion\":1},{\"id\":\"f9b782f8-bb7b-4d65-be0d-d65c"}
{"prompt": "Create an n8n workflow for: AI: Summarize podcast episode and enhance using Wikipedia The workflow automates the process of creating a summarized and enriched podcast digest, which is then sent via email. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free +5", "completion": "{\"id\":\"zFxUMqgvTXGIMzvh\",\"meta\":{\"instanceId\":\"ec7a5f4ffdb34436e59d23eaccb5015b5238de2a877e205b28572bf1ffecfe04\"},\"name\":\"Podcast Digest\",\"tags\":[],\"nodes\":[{\"id\":\"48bf1045-cfc1-4b37-9cce-86634bd97480\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-420,580],\"parameters\":{},\"typeVersion\":1},{\"id\":\"75f2e528-e5fe-4508-b98f-e1f71f803e60\",\"name\":\"Podcast Episode Transcript\",\"type\":\"n8n-nodes-base.code\",\"position\":[-220,580],\"parameters\":{\"jsCode\":\"return { transcript: `So throughout the last couple episodes we\u2019ve been doing on the philosophy of mind\u2026there\u2019s been an IDEA that we\u2019ve referenced MULTIPLE TIMES\u2026 and really just glossed over it as something, that\u2019s PRACTICALLY self evident. \\n\\n\\n\\nThe idea\u2026 is that when we THINK about consciousness\u2026 we can SPLIT it into two different types\u2026there\u2019s ACCESS consciousness on the one hand\u2026 and PHENOMENAL consciousness on the other. This is what we\u2019ve been saying. \\n\\n\\n\\nWhen it comes to ACCESS consciousness\u2026that\u2019s stuff we CAN explain with neuroscience things like memories, information processing, our field of visual awareness\u2026we can CLEARLY EXPLAIN a bit about how all THAT stuff works.\\n\\n\\n\\nBut in this conversation so far, what KEEPS on being said\u2026 is that what we CAN\u2019T SEEM to explain\u2026is PHENOMENAL consciousness\u2026you know, the subjective experience, that UNDERLIES conscious thought. That it FEELS like something to be me. There\u2019s this idea\u2026that this phenomenal consciousness is something separate\u2026something fundamental, something in a category ALL IT\u2019S OWN\u2026 that needs to be explained. You can explain a lot of stuff about access consciousness\u2026but you can\u2019t explain PHENOMENAL"}
{"prompt": "Create an n8n workflow for: Custom LangChain agent written in JavaScript This workflow has multiple functionalities. It starts with a manual trigger, \"When clicking 'Execute Workflow'\", that activates two separate paths. The first path takes a preset string \"Tell me a joke\" and processes it through a custom Language Learning Model (LLM) chain node. This node interacts with an OpenAI node for query processing. The second path takes another preset string \"What year was Einstein born?\" and passes it to an \"Agent\" node. This agent further interacts with a Chat OpenAI node and a custom Wikipedia node to produce the required information. The workflow uses both built-in and custom nodes, and integrates with OpenAI for both paths. It's built for experimenting with language models, specifically in the context of conversational agents and information retrieval. Note that to use this template, you need to be on n8n version 1.19.4 or later. n8n Team | 1 year ago | Free", "completion": "{\"id\":\"q2MJWAqpKF2BCJkq\",\"meta\":{\"instanceId\":\"021d3c82ba2d3bc090cbf4fc81c9312668bcc34297e022bb3438c5c88a43a5ff\"},\"name\":\"LangChain - Example - Code Node Example\",\"tags\":[{\"id\":\"snf16n0p2UrGP838\",\"name\":\"LangChain - Example\",\"createdAt\":\"2023-09-25T16:21:55.962Z\",\"updatedAt\":\"2023-09-25T16:21:55.962Z\"}],\"nodes\":[{\"id\":\"ad1a920e-1048-4b58-9c4a-a0469a1f189d\",\"name\":\"OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.lmOpenAi\",\"position\":[900,628],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"4jRB4A20cPycBqP5\",\"name\":\"OpenAI account - n8n\"}},\"typeVersion\":1},{\"id\":\"7dd04ecd-f169-455c-9c90-140140e37542\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[800,340],\"parameters\":{\"width\":432,\"height\":237,\"content\":\"## Self-coded LLM Chain Node\"},\"typeVersion\":1},{\"id\":\"05ad7d68-5dc8-42f2-8274-fcb5bdeb68cb\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[280,428],\"parameters\":{},\"typeVersion\":1},{\"id\":\"39e2fd34-3261-44a1-aa55-96f169d55aad\",\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[620,428],\"parameters\":{\"values\":{\"string\":[{\"name\":\"input\",\"value\":\"Tell me a joke\"}]},\"options\":{}},\"typeVersion\":2},{\"id\":\"42a3184c-0c62-4e79-9220-7a93e313317e\",\"name\":\"Set1\",\""}
{"prompt": "Create an n8n workflow for: Discord AI-powered bot This workflow is a template example for making Discord GPT-powered bots. Incoming user requests are analysed and categorised via OpenAI node with the help of a hand-crafted prompt. The response message is then routed to a different Discord channel. Eduard | 1 year ago | Free", "completion": "{\"id\":\"180\",\"meta\":{\"instanceId\":\"fb924c73af8f703905bc09c9ee8076f48c17b596ed05b18c0ff86915ef8a7c4a\"},\"name\":\"Discord AI bot\",\"tags\":[],\"nodes\":[{\"id\":\"6f188270-2c08-491f-bf52-c4a152b33aa0\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[1220,780],\"parameters\":{},\"typeVersion\":1},{\"id\":\"e4839de2-fc04-40b0-b6bc-596455ad93fe\",\"name\":\"Webhook\",\"type\":\"n8n-nodes-base.webhook\",\"position\":[1220,580],\"webhookId\":\"d0cdd428-be96-4821-85bc-65342cf928d0\",\"parameters\":{\"path\":\"d0cdd428-be96-4821-85bc-65342cf928d0\",\"options\":{},\"httpMethod\":\"POST\"},\"typeVersion\":1},{\"id\":\"15dcafe1-6361-4775-ace0-e34fd2a143b4\",\"name\":\"No Operation, do nothing\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[2120,940],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0d28fe8e-da80-458b-9a75-d316019cb3ae\",\"name\":\"Analyze user request\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1420,680],\"parameters\":{\"model\":\"gpt-4\",\"prompt\":{\"messages\":[{\"role\":\"system\",\"content\":\"Act as a service desk agent and help to categorize user messages. Return back only JSON without quotations. Do not return anything else.\"},{\"content\":\"=Here is a user feedback: \\\"{{ $json.body.feedback }}\\\". Please analyse it and put into one of the categories:\\n1. \\\"success-story\\\" for user appraisal or success story. this will be processed by customer success department\\n2. \\\"urgent-issue\\\" for extreme dissatisfaction or an urgent problem. this will be escalated to the IT team. Please assess if the request is really"}
{"prompt": "Create an n8n workflow for: OpenAI examples: ChatGPT, DALLE-2, Whisper-1 \u2013 5-in-1 Primer workflow for OpenAI models: ChatGPT, DALLE-2, Whisper This workflow contains 5 examples on how to work with OpenAI API. Transcribe voice into text via Whisper model (disabled, please put your own mp3 file with voice) The old way of using OpenAI conversational model via text-davinci-003 Examples 1.x. Simple ChatGPT calls. Text completion and text edit Example 2. Provide system and user content into ChatGPT Examples 3.x. Create system / user / assistanc content via Code Node. Promtp chaining technique example Example 4. Generate code via ChatGPT Example 5. Return multiple answers. Useful for providing picking the most relevant reply IMPORTANT! Do not run the whole workflow, it's rather slow Better execute the last node of each branch or simply disconnect branches that are not needed Eduard | 1 year ago | Free", "completion": "{\"id\":\"147\",\"meta\":{\"instanceId\":\"dfdeafd1c3ed2ee08eeab8c2fa0c3f522066931ed8138ccd35dc20a1e69decd3\"},\"name\":\"OpenAI-model-examples\",\"tags\":[],\"nodes\":[{\"id\":\"ad6dc2cd-21cc-4563-86ba-f78cc4a55543\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-640,380],\"parameters\":{},\"typeVersion\":1},{\"id\":\"b370da23-ead4-4221-b7fe-a9d943f7fbb9\",\"name\":\"davinci-003-complete\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1160,60],\"parameters\":{\"prompt\":\"={{ $json.text }}\\n\\nTl;dr:\",\"options\":{\"maxTokens\":500}},\"credentials\":{\"openAiApi\":{\"id\":\"63\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"5e04f355-36c0-4540-8e65-68118cb73135\",\"name\":\"ChatGPT-ex2\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1160,740],\"parameters\":{\"prompt\":{\"messages\":[{\"role\":\"system\",\"content\":\"=You are an assistant. Always add 5 emojis to the end of your answer.\"},{\"content\":\"=Write tl;dr of the wollowing text: {{ $json.text}}\"}]},\"options\":{\"maxTokens\":500,\"temperature\":0.8},\"resource\":\"chat\"},\"credentials\":{\"openAiApi\":{\"id\":\"63\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"16a7cf80-16e3-44f9-b15c-7501417fe38f\",\"name\":\"davinci-003-edit\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[1340,60],\"parameters\":{\"input\":\"={{ $json.text }}\",\"options\":{},\"operation\":\"edit\",\"instruction\":\"translate to German\"},\"credentials\":{\"openAiApi\":{\"id"}
{"prompt": "Create an n8n workflow for: Send a ChatGPT email reply and save responses to Google Sheets This workflow sends a OpenAI GPT reply when an email is received from specific email recipients. It then saves the initial email and the GPT response to an automatically generated Google spreadsheet. Subsequent GPT responses will be added to the same spreadsheet. Additionally, when feedback is given for any of the GPT responses, it will be recorded to the spreasheet, which can then be used later to fine-tune the GPT model. Prerequisites OpenAI credentials Google credentials How it works This workflow is essentially a two-in-one workflow. It triggers off from two different nodes and have very different functionality from each trigger. The flow triggered from On email received node is as follows: Triggers off on the On email received node. Extract the email body from the email. Generate a response from the email body using the OpenAI node. Reply to the email sender using the Send reply to recipient node. A feedback link is also included in the email body which will trigger the On feedback given node. This is used to fine-tune the GPT model. Save the email body and OpenAI response to a Google Sheet. If a sheet does not exist, it will be created. The flow triggered from On feedback given node is as follows: Triggers off when a feedback link is clicked in the emailed GPT response. The feedback, either positive or negative, for that specific GPT response is then recorded to the Google Sheet. n8n Team | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"a2434c94d549548a685cca39cc4614698e94f527bcea84eefa363f1037ae14cd\"},\"nodes\":[{\"id\":\"88c0f64c-a7cd-4f35-96dd-9eee4b1d6a1a\",\"name\":\"Generate reply\",\"type\":\"n8n-nodes-base.openAi\",\"position\":[-480,2260],\"parameters\":{\"prompt\":\"=From: {{ $json.from.value }}\\nTo: {{ $json.to.value }}\\nSubject: {{ $json.subject }}\\nBody: {{ $json.reply }}\\n\\n\\nReply: \",\"options\":{\"maxTokens\":\"={{ $('Configure').first().json.replyTokenSize }}\"}},\"credentials\":{\"openAiApi\":{\"id\":\"27\",\"name\":\"[UPDATE ME]\"}},\"typeVersion\":1},{\"id\":\"7105b689-9f9c-4354-aad9-8f1abb6c0a06\",\"name\":\"On email received\",\"type\":\"n8n-nodes-base.gmailTrigger\",\"position\":[-2460,2680],\"parameters\":{\"simple\":false,\"filters\":{},\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]}},\"credentials\":{\"gmailOAuth2\":{\"id\":\"26\",\"name\":\"[UPDATE ME]\"}},\"typeVersion\":1},{\"id\":\"ea18ed9a-0158-45e1-ac1b-1993ace4ff2c\",\"name\":\"Only continue for specific emails\",\"type\":\"n8n-nodes-base.if\",\"position\":[-1360,2460],\"parameters\":{\"conditions\":{\"string\":[{\"value1\":\"={{ $('Configure').first().json.recipients.split(',') }}\",\"value2\":\"*\",\"operation\":\"contains\"},{\"value1\":\"={{ $('Configure').first().json.recipients.split(',') }}\",\"value2\":\"={{ $json.from.value[0].address }}\",\"operation\":\"contains\"}]},\"combineOperation\":\"any\"},\"typeVersion\":1},{\"id\":\"d1425dff-0fc1-4a4b-9202-418ce30d7cd9"}
{"prompt": "Create an n8n workflow for: Send specific PDF attachments from Gmail to Google Drive using OpenAI This workflow reads PDF textual content and sends the text to OpenAI. Attachments of interest will then be uploaded to a specified Google Drive folder. For example, you may wish to send invoices received from an email to an inbox folder in Google Drive for later processing. This workflow has been designed to easily change the search term to match your needs. See the workflow for more details. Prerequisites OpenAI credentials. Google credentials. How it works Triggers off on the On email received node. Iterates over the attachments in the email. Uses the OpenAI node to filter out the attachments that do not match the search term set in the Configure node. You could match on various PDF files (i.e. invoice, receipt, or contract). If the PDF attachment matches the search term, the workflow uses the Google Drive node to upload the PDF attachment to a specific Google Drive folder. n8n Team | 1 year ago | Free", "completion": "{\"meta\":{\"instanceId\":\"a2434c94d549548a685cca39cc4614698e94f527bcea84eefa363f1037ae14cd\"},\"nodes\":[{\"id\":\"deafa2e8-af41-4f11-92e0-09992f6c6970\",\"name\":\"Read PDF\",\"type\":\"n8n-nodes-base.readPDF\",\"position\":[860,1420],\"parameters\":{},\"typeVersion\":1},{\"id\":\"8e3ddbb1-83a1-4f79-9464-61d5a20f0427\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-760,1300],\"parameters\":{\"width\":444.034812880766,\"height\":599.5274151436035,\"content\":\"## Send specific PDF attachments from Gmail to Google Drive using OpenAI\\n\\n_**DISCLAIMER**: You may have varying success when using this workflow so be prepared to validate the correctness of OpenAI's results._\\n\\nThis workflow reads PDF textual content and sends the text to OpenAI. Attachments of interest will then be uploaded to a specified Google Drive folder. For example, you may wish to send invoices received from an email to an inbox folder in Google Drive for later processing. This workflow has been designed to easily change the search term to match your needs. See the workflow for more details.\\n\\n### How it works\\n1. Triggers off on the `On email received` node.\\n2. Iterates over the attachments in the email.\\n3. Uses the `OpenAI` node to filter out the attachments that do not match the search term set in the `Configure` node. You could match on various PDF files (i.e. invoice, receipt, or contract).\\n4. If the PDF attachment matches the search term, the workflow uses the `Google Drive` node to upload the PDF attachment to a specific Google Drive folder.\\n\\n\\nWorkflow written by [David Sha](https://davidsha.me).\"},\"typeVersion\":1},{\"id\":\"fb2c3697-a92f-4be1-b9a6-0326f87de70b\",\"name\":\"Configure\",\"type\":\"n8"}
{"prompt": "Create an n8n workflow for: Reddit AI digest This workflow digests mentions of n8n on Reddit that can be sent as an single email or Slack summary each week. We use OpenAI to classify if a specific Reddit post is really about n8n or not, and then the summarise it into a bullet point sentence. How it works Get posts from Reddit that might be about n8n; Filter for the most relevant posts (posted in last 7 days and more than 5 upvotes and is original content); Check if the post is actually about n8n; If it is, categorise with OpenAI. Bear in mind: Workflow only considers first 500 characters of each reddit post. So if n8n is mentioned after this amount, it won't register as being a post about n8n.io. Next steps Improve OpenAI Summary node prompt to return cleaner summaries; Extend to more platforms/sources - e.g. it would be really cool to monitor larger Slack communities in this way; Do some classification on type of user to highlight users likely to be in our ICP; Separate a list of data sources (reddit, twitter, slack, discord etc.), extract messages from there and have them go to a sub workflow for classification and summarisation. n8n Team | 2 years ago | Free", "completion": "{\"meta\":{\"instanceId\":\"cb484ba7b742928a2048bf8829668bed5b5ad9787579adea888f05980292a4a7\"},\"nodes\":[{\"id\":\"d9bae984-2ce7-4f6b-ab53-527ac9dfea3d\",\"name\":\"When clicking \\\"Execute Workflow\\\"\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[680,320],\"parameters\":{},\"typeVersion\":1},{\"id\":\"32ecf73c-b6e9-4bd6-9ecc-d82c4c50d7b5\",\"name\":\"Reddit\",\"type\":\"n8n-nodes-base.reddit\",\"position\":[880,320],\"parameters\":{\"keyword\":\"n8n\",\"location\":\"allReddit\",\"operation\":\"search\",\"additionalFields\":{\"sort\":\"new\"}},\"credentials\":{},\"typeVersion\":1},{\"id\":\"4b560620-a101-4566-b066-4ce3f44d8b0c\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[120,180],\"parameters\":{\"width\":507.1052631578949,\"height\":210.99380804953552,\"content\":\"## What this workflow does\\n\u2714\ufe0e 1) Get posts from reddit that might be about n8n\\n - Filter for the most relevant posts (posted in last 7 days and more than 5 upvotes and is original content)\\n\\n\u2714\ufe0e 2) Check if the post is actually about n8n\\n\\n\u2714\ufe0e 3) if it is, categorise with OpenAi.\\n\"},\"typeVersion\":1},{\"id\":\"f3be9af5-b4ff-4f4e-a726-fc05fab94521\",\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[1260,320],\"parameters\":{\"values\":{\"number\":[{\"name\":\"upvotes\",\"value\":\"={{ $json.ups }}\"},{\"name\":\"subredditSize\",\"value\":\"={{ $json.subreddit_subscribers }}\"}],\"string\":[{\"name\":\"selftextTrimmed\",\"value\":\"={{ $json.selftext.substring("}
{"prompt": "Create an n8n workflow for: lemlist <> GPT-3: Supercharge your sales workflows Use GPT-3 to classify email responses in lemlist. And automate: Slack alerts when a lead is interested the creation of tasks when a lead is OOO unsubscription of leads when they request it Lucas Perret | 2 years ago | Free", "completion": "{\"meta\":{\"instanceId\":\"f0a68da631efd4ed052a324b63ff90f7a844426af0398a68338f44245d1dd9e5\"},\"nodes\":[{\"id\":\"44b2e0ac-1ec9-4acd-bf00-7e280378b8df\",\"name\":\"Lemlist - Unsubscribe\",\"type\":\"n8n-nodes-base.lemlist\",\"position\":[1300,-180],\"parameters\":{\"email\":\"={{ $json[\\\"leadEmail\\\"] }}\",\"resource\":\"lead\",\"operation\":\"unsubscribe\",\"campaignId\":\"={{$json[\\\"campaignId\\\"]}}\"},\"credentials\":{\"lemlistApi\":{\"id\":\"45\",\"name\":\"Lemlist - \\\"lemlist\\\" team API key\"}},\"typeVersion\":1},{\"id\":\"75dd6db8-5e59-4521-a4be-2272e2914494\",\"name\":\"follow up task\",\"type\":\"n8n-nodes-base.hubspot\",\"position\":[1520,640],\"parameters\":{\"type\":\"task\",\"metadata\":{\"subject\":\"=OOO - Follow up with {{ $json[\\\"properties\\\"][\\\"firstname\\\"][\\\"value\\\"] }} {{ $json[\\\"properties\\\"][\\\"lastname\\\"][\\\"value\\\"] }}\"},\"resource\":\"engagement\",\"authentication\":\"oAuth2\",\"additionalFields\":{\"associations\":{\"contactIds\":\"={{ $json[\\\"vid\\\"] }}\"}}},\"credentials\":{\"hubspotOAuth2Api\":{\"id\":\"14\",\"name\":\"Hubspot account\"}},\"typeVersion\":1},{\"id\":\"0ba95d5d-fe73-4687-8e21-02b97b19924f\",\"name\":\"Switch\",\"type\":\"n8n-nodes-base.switch\",\"position\":[380,300],\"parameters\":{\"rules\":{\"rules\":[{\"value2\":\"Unsubscribe\"},{\"output\":1,\"value2\":\"Interested\"},{\"output\":2,\"value2\":\"Out of Office\"}]},\"value1\":\"={{ $json[\\\"text\\\"].trim() }}\",\"dataType\":\"string\",\"fallbackOutput\":3},\"typeVersion\":1},{\"id\":\"abdb4925-4b2a-48e0-aa3d-042e1112150a\",\"name\":\"Merge\",\""}
{"prompt": "Create an n8n workflow for: Automate testimonials in Strapi with n8n This is the workflow powering the n8n demo shown at StrapiConf 2022. The workflow searches matching Tweets every 30 minutes using the Interval node and listens to Form submissions using the Webhook node. Sentiment analysis is handled by Google using the Google Cloud Natural Language node before the result is stored in Strapi using the Strapi node. (These were originally two separate workflows that have been combined into one to simplify sharing.) Tom | 2 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"Simplify Result\",\"type\":\"n8n-nodes-base.set\",\"position\":[680,100],\"parameters\":{\"values\":{\"string\":[{\"name\":\"Content\",\"value\":\"={{$json[\\\"full_text\\\"].replace(/(?:https?|ftp):\\\\/\\\\/[\\\\n\\\\S]+/g, '')}}\"},{\"name\":\"Author\",\"value\":\"={{$json[\\\"user\\\"][\\\"name\\\"]}} (@{{$json[\\\"user\\\"][\\\"screen_name\\\"]}})\"},{\"name\":\"Created\",\"value\":\"={{new Date($json[\\\"created_at\\\"]).toISOString()}}\"},{\"name\":\"URL\",\"value\":\"=https://twitter.com/{{$json[\\\"user\\\"][\\\"screen_name\\\"]}}/status/{{$json[\\\"id_str\\\"]}}\"}]},\"options\":{},\"keepOnlySet\":true},\"typeVersion\":1},{\"name\":\"Store in Strapi\",\"type\":\"n8n-nodes-base.strapi\",\"position\":[1780,100],\"parameters\":{\"columns\":\"Content,Author,Created,URL\",\"operation\":\"create\",\"contentType\":\"posts\"},\"credentials\":{\"strapiApi\":{\"id\":\"136\",\"name\":\"Strapi Demo\"}},\"typeVersion\":1},{\"name\":\"Every 30 Minutes\",\"type\":\"n8n-nodes-base.interval\",\"position\":[240,100],\"parameters\":{\"unit\":\"minutes\",\"interval\":30},\"typeVersion\":1},{\"name\":\"Is Retweet or Old?\",\"type\":\"n8n-nodes-base.if\",\"position\":[900,100],\"parameters\":{\"conditions\":{\"string\":[{\"value1\":\"={{$json[\\\"Content\\\"]}}\",\"value2\":\"RT @\",\"operation\":\"startsWith\"}],\"dateTime\":[{\"value1\":\"={{$json[\\\"Created\\\"]}}\",\"value2\":\"={{new Date(new Date().getTime() - 30 * 60 * 1000)}}\",\"operation\":\"before\"}]},\"combineOperation\":\"any\"},\"typeVersion\":1},{\"name\":\"Search Tweets\",\"type\":\"n8n-nodes-base.twitter\",\"position\":[460,100],\"parameters\":{\"operation\":\"search\",\"searchText\":\"(strapi OR n8n.io) AND lang:en\",\"additionalFields\":{\"tweetMode\":\"extended\",\"resultType\":\"recent\"}"}
{"prompt": "Create an n8n workflow for: OpenAI-powered tweet generator This workflow uses OpenAI to generate tweets to be stored in Airtable for review. A JS snippet handles the topics to be tweeted about in the form of hashtags. Gareth Roll | 2 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[250,300],\"parameters\":{},\"typeVersion\":1},{\"name\":\"FunctionItem\",\"type\":\"n8n-nodes-base.functionItem\",\"position\":[450,300],\"parameters\":{\"functionCode\":\"// hashtag list\\nconst Hashtags = [\\n \\\"#techtwitter\\\",\\n \\\"#n8n\\\"\\n];\\n\\n// random output function\\nconst randomHashtag = Hashtags[Math.floor(Math.random() * Hashtags.length)];\\nitem.hashtag = randomHashtag;\\nreturn item;\"},\"typeVersion\":1},{\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[650,300],\"parameters\":{\"url\":\"https://api.openai.com/v1/engines/text-davinci-001/completions\",\"options\":{},\"requestMethod\":\"POST\",\"authentication\":\"headerAuth\",\"jsonParameters\":true,\"bodyParametersJson\":\"={\\n \\\"prompt\\\": \\\"Generate a tweet, with under 100 characters, about and including the hashtag {{$node[\\\"FunctionItem\\\"].json[\\\"hashtag\\\"]}}:\\\",\\n \\\"temperature\\\": 0.7,\\n \\\"max_tokens\\\": 64,\\n \\\"top_p\\\": 1,\\n \\\"frequency_penalty\\\": 0,\\n \\\"presence_penalty\\\": 0\\n}\"},\"credentials\":{\"httpHeaderAuth\":\"\"},\"typeVersion\":1},{\"name\":\"Airtable\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[1050,300],\"parameters\":{\"table\":\"main\",\"options\":{},\"operation\":\"append\",\"application\":\"appOaG8kEA8FAABOr\"},\"credentials\":{\"airtableApi\":\"\"},\"typeVersion\":1},{\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[850,300],\"parameters\":{\"values\":{\"string\":[{\"name\":\"Hashtag\",\"value\":\"={{$node[\\\"FunctionItem\\\"].json[\\\"hashtag\\\"]}}\"},{\"name\":\"Content\",\"value\":\"={{$node[\\\"HTTP Request\\\"].json[\\\"choices\\\"][0][\\\"text\\\"]}}\"}]},\"options\":"}
{"prompt": "Create an n8n workflow for: Send a random recipe once a day to Telegram This telegram bot is designed to send one random recipe a day. This specific bot has filtered out only vegan recipes, so you can choose your diet type and send only recipes for a specific diet. What credentials you need: Set up a telegram bot. Airtable for listing who has joined your bot. This is needed to send one random recipe a day. Recipe (or other) API. This one uses Spoonacular. I hope you enjoy your bot! Hilary Torn | 3 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"Cron\",\"type\":\"n8n-nodes-base.cron\",\"position\":[440,440],\"parameters\":{\"triggerTimes\":{\"item\":[{}]}},\"typeVersion\":1},{\"name\":\"Airtable2\",\"type\":\"n8n-nodes-base.airtable\",\"notes\":\"Grab our list of chats from Airtable to send a random recipe\",\"position\":[660,440],\"parameters\":{\"table\":\"Table 1\",\"operation\":\"list\",\"application\":\"your_sheet_id\",\"additionalOptions\":{}},\"credentials\":{\"airtableApi\":{\"id\":\"5\",\"name\":\"Airtable account\"}},\"notesInFlow\":true,\"typeVersion\":1},{\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[860,600],\"parameters\":{\"values\":{\"number\":[{\"name\":\"chatid\",\"value\":\"={{$node[\\\"Airtable2\\\"].json[\\\"fields\\\"][\\\"chatid\\\"]}}\"}],\"string\":[]},\"options\":{}},\"typeVersion\":1},{\"name\":\"Recipe Photo\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1240,440],\"parameters\":{\"file\":\"={{$node[\\\"Get recipes from API\\\"].json[\\\"recipes\\\"][0][\\\"image\\\"]}}\",\"chatId\":\"={{$node[\\\"Set\\\"].json[\\\"chatid\\\"]}}\",\"operation\":\"sendPhoto\",\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"1\",\"name\":\"Telegram account\"}},\"typeVersion\":1,\"continueOnFail\":true},{\"name\":\"Recipe URL\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1420,440],\"parameters\":{\"text\":\"=\\n{{$node[\\\"Get recipes from API\\\"].json[\\\"recipes\\\"][0][\\\"title\\\"]}}\\n\\n{{$node[\\\"Get recipes from API\\\"].json[\\\"recipes\\\"][0][\\\"sourceUrl\\\"]}}\",\"chatId\":\"={{$node[\\\"Set\\\"].json[\\\"chatid\\\"]}}\",\"additionalFields\":{}},\"credentials\":{\"telegramApi\":{\"id\":\"1\",\"name\":\"Telegram account\"}},\"typeVersion\":1,\"continueOnFail\":true},{\"name\":\"IF\",\"type\":\"n8n-nodes-base"}
{"prompt": "Create an n8n workflow for: Create dynamic Twitter profile banner This workflow updates your Twitter profile banner when you have a new follower. To use this workflow: Configure Header Auth in the Fetch New Followers to connect to your Twitter account. Update the URL of the template image in the Fetch BG node. Create and configure your Twitter OAuth 1.0 credentials in the last HTTP Request node. You can configure the size, and position of the avatar images in the Edit Image nodes. Check out this video to learn how to build it from scratch: How to automatically update your Twitter Profile Banner ghagrawal17 | 3 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[260,210],\"parameters\":{},\"typeVersion\":1},{\"name\":\"Fetch new followers\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[460,210],\"parameters\":{\"url\":\"https://api.twitter.com/2/users/{YOUR_USER_ID}/followers?user.fields=profile_image_url&max_results=3\",\"options\":{},\"authentication\":\"headerAuth\"},\"credentials\":{\"httpHeaderAuth\":{\"id\":\"2\",\"name\":\"Twitter Token\"}},\"typeVersion\":1},{\"name\":\"Item Lists\",\"type\":\"n8n-nodes-base.itemLists\",\"position\":[660,210],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"data\"},\"typeVersion\":1},{\"name\":\"Function\",\"type\":\"n8n-nodes-base.function\",\"position\":[1660,210],\"parameters\":{\"functionCode\":\"const binary = {};\\nfor (let i=0; i < items.length; i++) {\\n binary[`data${i}`] = items[i].binary.avatar;\\n}\\n\\nreturn [\\n {\\n json: {\\n numIcons: items.length,\\n },\\n binary,\\n }\\n];\\n\"},\"typeVersion\":1},{\"name\":\"Merge\",\"type\":\"n8n-nodes-base.merge\",\"position\":[1910,110],\"parameters\":{\"mode\":\"mergeByIndex\"},\"typeVersion\":1},{\"name\":\"Fetching images\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[860,210],\"parameters\":{\"url\":\"={{$json[\\\"profile_image_url\\\"].replace('normal','400x400')}}\",\"options\":{},\"responseFormat\":\"file\",\"dataPropertyName\":\"avatar\"},\"typeVersion\":1},{\"name\":\"Fetch bg\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1660,-40],\"parameters\":{\"url\":\"{TEMPLATE_IMAGE_URL}\",\"options\":{},\"responseFormat\":\"file\",\"dataPropertyName\":\"bg\"},\"typeVersion\":1},{\"name\":\"Resize\",\"type\":\"n8n-nodes-base.editImage\",\""}
{"prompt": "Create an n8n workflow for: Update Twitter banner using HTTP request This workflow demonstrates the use of the HTTP Request node to upload binary files for form-data-multipart type. This example workflow updates the Twitter banner. HTTP Request node: This node fetches an image from Unsplash. Replace this node with any other node to fetch the image file. HTTP Request1 node: This node uploads the Twitter Profile Banner. The Twitter API requires OAuth 1.0 authentication. Follow the Twitter documentation to learn how to configure the authentication. ghagrawal17 | 3 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[250,300],\"parameters\":{},\"typeVersion\":1},{\"name\":\"Start\",\"type\":\"n8n-nodes-base.start\",\"position\":[250,300],\"parameters\":{},\"typeVersion\":1},{\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[450,300],\"parameters\":{\"url\":\"https://unsplash.com/photos/lUDMZUWFUXE/download?ixid=MnwxMjA3fDB8MXxhbGx8Mnx8fHx8fDJ8fDE2MzczMjY4Mjc&force=true\",\"options\":{},\"responseFormat\":\"file\",\"headerParametersUi\":{\"parameter\":[]}},\"typeVersion\":1},{\"name\":\"HTTP Request1\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[650,300],\"parameters\":{\"url\":\"https://api.twitter.com/1.1/account/update_profile_banner.json\",\"options\":{},\"requestMethod\":\"POST\",\"authentication\":\"oAuth1\",\"jsonParameters\":true,\"sendBinaryData\":true,\"binaryPropertyName\":\"banner:data\"},\"credentials\":{\"oAuth1Api\":{\"id\":\"300\",\"name\":\"Unnamed credential\"}},\"typeVersion\":1}],\"connections\":{\"HTTP Request\":{\"main\":[[{\"node\":\"HTTP Request1\",\"type\":\"main\",\"index\":0}]]},\"On clicking 'execute'\":{\"main\":[[{\"node\":\"HTTP Request\",\"type\":\"main\",\"index\":0}]]}}}"}
{"prompt": "Create an n8n workflow for: Detect toxic language in Telegram messages This workflow detects toxic language (such as profanity, insults, threats) in messages sent via Telegram. This blog tutorial explains how to configure the workflow nodes step-by-step. Telegram Trigger: triggers the workflow when a new message is sent in a Telegram chat. Google Perspective: analyzes the text of the message and returns a probability value between 0 and 1 of how likely it is that the content is toxic. IF: filters messages with a toxic probability value above 0.7. Telegram: sends a message in the chat with the text \"I don't tolerate toxic language\" if the probability value is above 0.7. NoOp: takes no action if the probability value is below 0.7. Lorena | 3 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"Telegram Trigger\",\"type\":\"n8n-nodes-base.telegramTrigger\",\"position\":[600,300],\"webhookId\":\"2d0805da-143e-40c9-b327-242b1f052c31\",\"parameters\":{\"updates\":[\"message\",\"edited_message\",\"channel_post\",\"edited_channel_post\"],\"additionalFields\":{}},\"credentials\":{\"telegramApi\":\"telegram_habot\"},\"typeVersion\":1},{\"name\":\"Google Perspective\",\"type\":\"n8n-nodes-base.googlePerspective\",\"position\":[800,300],\"parameters\":{\"text\":\"={{$json[\\\"message\\\"][\\\"text\\\"]}}\",\"options\":{\"languages\":\"en\"},\"requestedAttributesUi\":{\"requestedAttributesValues\":[{\"attributeName\":\"identity_attack\"},{\"attributeName\":\"threat\"},{\"attributeName\":\"profanity\"}]}},\"credentials\":{\"googlePerspectiveOAuth2Api\":\"perspective_api\"},\"typeVersion\":1},{\"name\":\"IF\",\"type\":\"n8n-nodes-base.if\",\"position\":[1000,300],\"parameters\":{\"conditions\":{\"number\":[{\"value1\":\"={{$json[\\\"attributeScores\\\"][\\\"PROFANITY\\\"][\\\"summaryScore\\\"][\\\"value\\\"]}}\",\"value2\":0.7,\"operation\":\"larger\"}]}},\"typeVersion\":1},{\"name\":\"Telegram\",\"type\":\"n8n-nodes-base.telegram\",\"position\":[1200,150],\"parameters\":{\"text\":\"I don't tolerate toxic language!\",\"chatId\":\"={{$node[\\\"Telegram Trigger\\\"].json[\\\"message\\\"][\\\"chat\\\"][\\\"id\\\"]}}\",\"additionalFields\":{\"reply_to_message_id\":\"={{$node[\\\"Telegram Trigger\\\"].json[\\\"message\\\"][\\\"message_id\\\"]}}\"}},\"credentials\":{\"telegramApi\":\"telegram_habot\"},\"typeVersion\":1},{\"name\":\"NoOp\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[1200,400],\"parameters\":{},\"typeVersion\":1}],\"connections\":{\"IF\":{\"main\":[[{\"node\":\"Telegram\",\"type\":\"main\",\"index\":0}],[{\"node\":\"NoOp\",\"type\":\"main\",\"index\":0}]]},\"Telegram Trigger\":{\"main\":[[{\""}
{"prompt": "Create an n8n workflow for: Add positive feedback messages to a table in Notion This workflow allows you to add positive feedback messages to a table in Notion. Prerequisites Create a Typeform that contains Long Text filed question type to accepts feedback from users. Get your Typeform credentials by following the steps mentioned in the documentation. Follow the steps mentioned in the documentation to create credentials for Google Cloud Natural Language. Create a page on Notion similar to this page. Create credentials for the Notion node by following the steps in the documentation. Follow the steps mentioned in the documentation to create credentials for Slack. Follow the steps mentioned in the documentation to create credentials for Trello. Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form. Google Cloud Natural Language node: This node analyses the sentiment of the response the user has provided and gives a score. IF node: The IF node uses the score provided by the Google Cloud Natural Language node and checks if the score is positive (larger than 0). If the score is positive we get the result as True, otherwise False. Notion node: This node gets connected to the true branch of the IF node. It adds the positive feedback shared by the user in a table in Notion. Slack node: This node will share the positive feedback along with the score and username to a channel in Slack. Trello node: If the score is negative, the Trello node is executed. This node will create a card on Trello with the feedback from the user. ghagrawal17 | 3 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"Typeform Trigger\",\"type\":\"n8n-nodes-base.typeformTrigger\",\"position\":[0,400],\"webhookId\":\"ad8a87ef-d293-4e48-8d36-838d69ebce0f\",\"parameters\":{\"formId\":\"fBYjtY5e\"},\"credentials\":{\"typeformApi\":\"\"},\"typeVersion\":1},{\"name\":\"Google Cloud Natural Language\",\"type\":\"n8n-nodes-base.googleCloudNaturalLanguage\",\"position\":[200,400],\"parameters\":{\"content\":\"={{$json[\\\"Any suggestions for us? \\\"]}}\",\"options\":{}},\"credentials\":{\"googleCloudNaturalLanguageOAuth2Api\":\"\"},\"typeVersion\":1},{\"name\":\"IF\",\"type\":\"n8n-nodes-base.if\",\"position\":[400,400],\"parameters\":{\"conditions\":{\"number\":[{\"value1\":\"={{$node[\\\"Google Cloud Natural Language\\\"].json[\\\"documentSentiment\\\"][\\\"score\\\"]}}\",\"operation\":\"larger\"}]}},\"typeVersion\":1},{\"name\":\"Notion\",\"type\":\"n8n-nodes-base.notion\",\"position\":[600,300],\"parameters\":{\"resource\":\"databasePage\",\"databaseId\":\"b7d1130a-3756-4bb3-aa56-0c77bf416437\",\"propertiesUi\":{\"propertyValues\":[{\"key\":\"Name|title\",\"title\":\"={{$node[\\\"Typeform Trigger\\\"].json[\\\"Name\\\"]}}\"},{\"key\":\"Feedback|rich_text\",\"textContent\":\"={{$node[\\\"Typeform Trigger\\\"].json[\\\"Any suggestions for us? \\\"]}}\"}]}},\"credentials\":{\"notionApi\":\"\"},\"typeVersion\":1},{\"name\":\"Slack\",\"type\":\"n8n-nodes-base.slack\",\"position\":[800,300],\"parameters\":{\"channel\":\"general\",\"blocksUi\":{\"blocksValues\":[]},\"attachments\":[{\"text\":\"={{$node[\\\"Typeform Trigger\\\"].json[\\\"Any suggestions for us? \\\"]}}\",\"title\":\"={{$node[\\\"Typeform Trigger\\\"].json[\\\"Name\\\"]}} {{$node[\\\"Google Cloud Natural Language\\\"].json[\\\"documentSentiment\\\"][\\\"score\\\"]}}\"}],\"otherOptions\":{}},\"credentials\":{\"slackApi\":\""}
{"prompt": "Create an n8n workflow for: ETL pipeline for text processing This workflow allows you to collect tweets, store them in MongoDB, analyse their sentiment, insert them into a Postgres database, and post positive tweets in a Slack channel. Cron node: Schedule the workflow to run every day Twitter node: Collect tweets MongoDB node: Insert the collected tweets in MongoDB Google Cloud Natural Language node: Analyse the sentiment of the collected tweets Set node: Extract the sentiment score and magnitude Postgres node: Insert the tweets and their sentiment score and magnitude in a Posgres database IF node: Filter tweets with positive and negative sentiment scores Slack node: Post tweets with a positive sentiment score in a Slack channel NoOp node: Ignore tweets with a negative sentiment score Lorena | 3 years ago | Free", "completion": "{\"id\":\"6\",\"name\":\"ETL pipeline\",\"nodes\":[{\"name\":\"Twitter\",\"type\":\"n8n-nodes-base.twitter\",\"position\":[300,300],\"parameters\":{\"limit\":3,\"operation\":\"search\",\"searchText\":\"=#OnThisDay\",\"additionalFields\":{}},\"credentials\":{\"twitterOAuth1Api\":\"twitter_api\"},\"typeVersion\":1},{\"name\":\"Postgres\",\"type\":\"n8n-nodes-base.postgres\",\"position\":[1100,300],\"parameters\":{\"table\":\"tweets\",\"columns\":\"text, score, magnitude\",\"returnFields\":\"=*\"},\"credentials\":{\"postgres\":\"postgres\"},\"typeVersion\":1},{\"name\":\"MongoDB\",\"type\":\"n8n-nodes-base.mongoDb\",\"position\":[500,300],\"parameters\":{\"fields\":\"text\",\"options\":{},\"operation\":\"insert\",\"collection\":\"tweets\"},\"credentials\":{\"mongoDb\":\"mongodb\"},\"typeVersion\":1},{\"name\":\"Slack\",\"type\":\"n8n-nodes-base.slack\",\"position\":[1500,200],\"parameters\":{\"text\":\"=\ud83d\udc26 NEW TWEET with sentiment score {{$json[\\\"score\\\"]}} and magnitude {{$json[\\\"magnitude\\\"]}} \u2b07\ufe0f\\n{{$json[\\\"text\\\"]}}\",\"channel\":\"tweets\",\"attachments\":[],\"otherOptions\":{}},\"credentials\":{\"slackApi\":\"slack\"},\"typeVersion\":1},{\"name\":\"IF\",\"type\":\"n8n-nodes-base.if\",\"position\":[1300,300],\"parameters\":{\"conditions\":{\"number\":[{\"value1\":\"={{$json[\\\"score\\\"]}}\",\"operation\":\"larger\"}]}},\"typeVersion\":1},{\"name\":\"NoOp\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[1500,400],\"parameters\":{},\"typeVersion\":1},{\"name\":\"Google Cloud Natural Language\",\"type\":\"n8n-nodes-base.googleCloudNaturalLanguage\",\"position\":[700,300],\"parameters\":{\"content\":\"={{$node[\\\"MongoDB\\\"].json[\\\"text\\\"]}}\",\"options\":{}},\"credentials\":{\"googleCloudNaturalLanguageOAuth2Api\":\"google_nlp\"},\"typeVersion\":1},{\"name\":\"Set\",\"type\":\"n8n-n"}
{"prompt": "Create an n8n workflow for: Analyze feedback using AWS Comprehend and send it to a Mattermost channel This workflow analyzes the sentiments of the feedback provided by users and sends them to a Mattermost channel. Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form. AWS Comprehend node: This node analyses the sentiment of the response the user has provided and gives a score. IF node: The IF node uses the data provided by the AWS Comprehend node and checks if the sentiment is negative. If the sentiment is negative we get the result as true, otherwise false. Mattermost node: If the score is negative, the IF node returns true and the true branch of the IF node is executed. We connect the Mattermost node with the true branch of the IF node. Whenever the score of the sentiment analysis is negative, the node gets executed and a message is posted on a channel in Mattermost. NoOp: This node here is optional, as the absence of this node won't make a difference to the functioning of the workflow. This workflow can be used by Product Managers to analyze the feedback of the product. The workflow can also be used by HR to analyze employee feedback. You can even use this node for sentiment analysis of Tweets. To perform a sentiment analysis of Tweets, replace the Typeform Trigger node with the Twitter node. Note: You will need a Trigger node or Start node to start the workflow. Instead of posting a message on Mattermost, you can save the results in a database or a Google Sheet, or Airtable. Replace the Mattermost node with (or add after the Mattermost node) the node of your choice to add the result to your database. ghagrawal17 | 4 years ago | Free", "completion": "{\"nodes\":[{\"name\":\"Mattermost\",\"type\":\"n8n-nodes-base.mattermost\",\"position\":[810,300],\"parameters\":{\"message\":\"=You got new feedback with a score of {{$json[\\\"SentimentScore\\\"][\\\"Negative\\\"]}}. Here is what it says:{{$node[\\\"Typeform Trigger\\\"].json[\\\"What did you think about the event?\\\"]}}\",\"channelId\":\"h7cxrd1cefr13x689enzyw7xhc\",\"attachments\":[],\"otherOptions\":{}},\"credentials\":{\"mattermostApi\":\"Mattermost Credentials\"},\"typeVersion\":1},{\"name\":\"NoOp\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[800,500],\"parameters\":{},\"typeVersion\":1},{\"name\":\"IF\",\"type\":\"n8n-nodes-base.if\",\"position\":[600,400],\"parameters\":{\"conditions\":{\"number\":[],\"string\":[{\"value1\":\"={{$json[\\\"Sentiment\\\"]}}\",\"value2\":\"NEGATIVE\"}]}},\"typeVersion\":1},{\"name\":\"AWS Comprehend\",\"type\":\"n8n-nodes-base.awsComprehend\",\"position\":[400,400],\"parameters\":{\"text\":\"={{$json[\\\"What did you think about the event?\\\"]}}\",\"operation\":\"detectSentiment\"},\"credentials\":{\"aws\":\"AWS Comprehend Credentials\"},\"typeVersion\":1},{\"name\":\"Typeform Trigger\",\"type\":\"n8n-nodes-base.typeformTrigger\",\"position\":[200,400],\"webhookId\":\"ad8a87ef-d293-4e48-8d36-838d69ebce0f\",\"parameters\":{\"formId\":\"DuJHEGW5\"},\"credentials\":{\"typeformApi\":\"typeform\"},\"typeVersion\":1}],\"connections\":{\"IF\":{\"main\":[[{\"node\":\"Mattermost\",\"type\":\"main\",\"index\":0}],[{\"node\":\"NoOp\",\"type\":\"main\",\"index\":0}]]},\"AWS Comprehend\":{\"main\":[[{\"node\":\"IF\",\"type\":\"main\",\"index\":0}]]},\"Typeform Trigger\":{\"main\":[[{\"node\":\"AWS Comprehend\",\"type\":\"main\",\"index\":0}]]}}}"}
{"prompt": "Create an n8n workflow for: Analyze feedback and send a message on Mattermost This workflow analyzes the sentiments of the feedback provided by users and sends them to a Mattermost channel. Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form. Google Cloud Natural Language node: This node analyses the sentiment of the response the user has provided and gives a score. IF node: The IF node uses the score provided by the Google Cloud Natural Language node and checks if the score is negative (smaller than 0). If the score is negative we get the result as True, otherwise False. Mattermost node: If the score is negative, the IF node returns true and the true branch of the IF node is executed. We connect the Mattermost node with the true branch of the IF node. Whenever the score of the sentiment analysis is negative, the node gets executed and a message is posted on a channel in Mattermost. NoOp: This node here is optional, as the absence of this node won't make a difference to the functioning of the workflow. This workflow can be used by Product Managers to analyze the feedback of the product. The workflow can also be used by HR to analyze employee feedback. You can even use this node for sentiment analysis of Tweets. To perform a sentiment analysis of Tweets, replace the Typeform Trigger node with the Twitter node. Note:* You will need a Trigger node or Start node to start the workflow. Instead of posting a message on Mattermost, you can save the results in a database or a Google Sheet, or Airtable. Replace the Mattermost node with (or add after the Mattermost node) the node of your choice to add the result to your database. You can learn to build this workflow on the documentation page of the Google Cloud Natural Language node. ghagrawal17 | 4 years ago | Free", "completion": "{\"id\":\"133\",\"name\":\"Analyze the sentiment of feedback and send a message on Mattermost\",\"nodes\":[{\"name\":\"Typeform Trigger\",\"type\":\"n8n-nodes-base.typeformTrigger\",\"position\":[510,260],\"webhookId\":\"ad8a87ef-d293-4e48-8d36-838d69ebce0f\",\"parameters\":{\"formId\":\"\"},\"credentials\":{\"typeformApi\":\"typeform\"},\"typeVersion\":1},{\"name\":\"Google Cloud Natural Language\",\"type\":\"n8n-nodes-base.googleCloudNaturalLanguage\",\"position\":[710,260],\"parameters\":{\"content\":\"={{$node[\\\"Typeform Trigger\\\"].json[\\\"What did you think about the event?\\\"]}}\",\"options\":{}},\"credentials\":{\"googleCloudNaturalLanguageOAuth2Api\":\"cloud\"},\"typeVersion\":1},{\"name\":\"IF\",\"type\":\"n8n-nodes-base.if\",\"position\":[910,260],\"parameters\":{\"conditions\":{\"number\":[{\"value1\":\"={{$node[\\\"Google Cloud Natural Language\\\"].json[\\\"documentSentiment\\\"][\\\"score\\\"]}}\"}]}},\"typeVersion\":1},{\"name\":\"Mattermost\",\"type\":\"n8n-nodes-base.mattermost\",\"position\":[1110,160],\"parameters\":{\"message\":\"=You got a new feedback with a score of {{$node[\\\"Google Cloud Natural Language\\\"].json[\\\"documentSentiment\\\"][\\\"score\\\"]}}. Here is what it says:{{$node[\\\"Typeform Trigger\\\"].json[\\\"What did you think about the event?\\\"]}}\",\"channelId\":\"4h1bz64cyifwxnzojkzh8hxh4a\",\"attachments\":[],\"otherOptions\":{}},\"credentials\":{\"mattermostApi\":\"mattermost\"},\"typeVersion\":1},{\"name\":\"NoOp\",\"type\":\"n8n-nodes-base.noOp\",\"position\":[1110,360],\"parameters\":{},\"typeVersion\":1}],\"active\":false,\"settings\":{},\"connections\":{\"IF\":{\"main\":[[{\"node\":\"Mattermost\",\"type\":\"main\",\"index\":0}],[{\"node\":\"NoOp\",\"type\":\"main\",\"index\":0}]]},\"Typeform Trigger\":{\"main\":[[{\"node\":\"Google Cloud Natural Language\",\"type\":\"main\",\"index"}
{"prompt": "Create an n8n workflow for: Create, update, and get a profile in Humantic AI ghagrawal17 | 4 years ago | Free", "completion": "{\"id\":\"127\",\"name\":\"Create, update, and get a profile in Humantic AI\",\"nodes\":[{\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[290,300],\"parameters\":{},\"typeVersion\":1},{\"name\":\"Humantic AI\",\"type\":\"n8n-nodes-base.humanticAi\",\"position\":[490,300],\"parameters\":{\"userId\":\"https://www.linkedin.com/in/harshil1712/\"},\"credentials\":{\"humanticAiApi\":\"humantic\"},\"typeVersion\":1},{\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[690,300],\"parameters\":{\"url\":\"\",\"options\":{},\"responseFormat\":\"file\"},\"typeVersion\":1},{\"name\":\"Humantic AI1\",\"type\":\"n8n-nodes-base.humanticAi\",\"position\":[890,300],\"parameters\":{\"userId\":\"={{$node[\\\"Humantic AI\\\"].json[\\\"results\\\"][\\\"userid\\\"]}}\",\"operation\":\"update\",\"sendResume\":true},\"credentials\":{\"humanticAiApi\":\"humantic\"},\"typeVersion\":1},{\"name\":\"Humantic AI2\",\"type\":\"n8n-nodes-base.humanticAi\",\"position\":[1090,300],\"parameters\":{\"userId\":\"={{$node[\\\"Humantic AI\\\"].json[\\\"results\\\"][\\\"userid\\\"]}}\",\"options\":{\"persona\":[\"hiring\"]},\"operation\":\"get\"},\"credentials\":{\"humanticAiApi\":\"humantic\"},\"typeVersion\":1}],\"active\":false,\"settings\":{},\"connections\":{\"Humantic AI\":{\"main\":[[{\"node\":\"HTTP Request\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request\":{\"main\":[[{\"node\":\"Humantic AI1\",\"type\":\"main\",\"index\":0}]]},\"Humantic AI1\":{\"main\":[[{\"node\":\"Humantic AI2\",\"type\":\"main\",\"index\":0}]]},\"On clicking 'execute'\":{\"main\":[[{\"node\":\"Humantic AI\",\"type\":\"main\",\"index\":0}]]}}}"}
